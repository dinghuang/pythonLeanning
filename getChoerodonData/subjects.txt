Choerodon猪齿鱼 论坛提问规范：怎样让自己的问题及时得到解答文档中编写Entity不允许生成 setter 方法的缘由一键部署，拉取镜像失败关于事务实例使用自签名证书步骤出错在已有集群下新建流水线环境未在集群中找到新的命名空间等如何访问部署在虚拟机上的Choerodon非主键表的问题gitops同步回调devops-service的处理逻辑有问题集群断电重启，服务不能自动修复么系统中组织管理员、组织成员、项目所有者、项目成员、部署管理员等，拥有哪些具体的权限，这个目前在官网文档中没有维护。ansible-playbook命令执行报错阿里云部署单节点集群报错新增多个环境时cluster agent异常重启一键部署一直 waiting slaver running创建空间错误，界面显示成功，但是点击进入却找不到一键部署gitlab出错从应用模板创建应用失败Caused by: feign.FeignException: status 500 reading GitlabServiceClient#createProtectedBranches(Integer,String,String,String,Integer); content: {"failed":true,"code":"Protected branch 'master' already exists"...Ansible部署单节点集群，看日志报kublet主线程退出help，安装wiki出错一键安装choerodon的checking config-server一直过不去新的猪齿鱼，升级到0.10.0后，swagger访问404从0.10.0升级到0.11.0脚本错误namespaces "c7n-system" is forbidden: User "system:serviceaccount:kube-system:default" cannot get namespaces in the namespace "c7n-system"测试管理中测试用例的描述有问题开源企业级数字化服务平台——Choerodon猪齿鱼发布0.11版本猪齿鱼部署流水线本地启动微服务后过会儿都连接出错停止了部署manager-service出错前后端交互如何获取token从swagger-ui进入授权登录之后报错测试计划 可以拖动滚动条跨月查看测试计划“测试计划”和“测试执行” 下面的测试用例 需要增加：“优先级”字段、“优先级”筛选条件测试管理-测试计划 测试用例 可以按“模块”或“阶段”批量指派给相关测试人员执行建议增加webhook集成敏捷管理-->问题管理-->故事-->故事详情页-被阻塞 “故障”列表，如何显示“经办人”？在“测试用例-版本-导入”中看不到刚导入的用例导入的用例无法修改“执行人”choerodon-runner能不能把liquibase的源码地址发出来集成jasypt-spring-boot-starter2.0.0报错v0.10.0 一键安装成功，管理员登录后403错误template: zookeeper:1:3: executing "zookeeper" at <.WithPrefix>: can't evaluate field WithPrefix in type *install.InfraResource手动上传域名证书出错ad认证配置验证失败gitlab runner执行任务失败devopsOperationGitlabProject任务执行失败后端微服务开发项目启动不起来敏捷管理-->问题管理-->故障详情，没有“回归不通过”字段 注：这对我们 质量分析、提升质量 很重要分步部署，部署gitlab-service的参数env.open.GITLAB_PRIVATETOKEN怎么获取使用一键部署gitlab启动日志报错wiki service 部署安装失败choerodon-logging出现CrashLoopBackOff和Error状态部署流水线中的域名、网络、实例、容器、证书可否统一成“应用配置”或者“应用资源”pvc一直处于Pending状态，请问怎么处理？Excel数据初始化官方文档里面使用curl访问clusterip的方式验证服务成功启动的方式不可行希望devops持续集成服务可以自定义gitlab的ssh端口号为非默认的22helm install c7n/xwiki一直卡住不动安装nfs-provider时出现i/o timeout的原因？v0.10.0一键安装，自有NFS服务器，安装出错choerodon api报错，c7n，无法登录zookeeper卡在ContainerCreating， mount failed: exit status 32升级到v0.10版本后swagger页面找不到分步部署，部署gitlab-service时参数问题iam_service 0.10.0稳定版 初始化脚本执行报错BUG转故障操作太繁锁容器缺少更多的状态信息搭建完毕后猪齿鱼管理后台的初始登录账号密码是什么啊？一键部署的时候报错error creating error stream for port 8001 -> 9001: Timeout occured关于pv、pvc的管理Kubernetes集群部署 部署单节点出错请问日志监控节点数能否降低？c7n-charts是否有源码敏捷管理-删除冲刺时报错知识管理可否增加画词统计字数功能ansible部署不同网段K8S节点机卡住无法继续知识管理可否增加字体颜色功能？知识管理模块可否增加自动Ctrl+S的快捷键保存功能?choerodon-starter-mybatis-mapper使用最新版本jsqlparser报错请问k8s版本可以降低吗开发流水线－持续集成打开空白Gitlab CI docker-build 错误上传应用图标不成功创建空间选择空间图标，这个功能不好用操作之后，“再”次更新，没有办法截图黏贴图片了Choerodon如何离线安装？Kubernetes集群部署执行报错 FAILED - RETRYING: Master | wait for kube-scheduleralpine基础镜像DNS 解析的问题建议在环境流水线可以预设通用环境变量键值helm部署zookeeper、kafka后稳定性建议mgmt_swagger表数据插入失败问题管理/待办问题功能搜索优化AuditHelper.audit().getUser(); 不能获取登陆用户信息？一键部署Choerodon出错前端开发手册-开发新页面-运行 python 脚本报错（2）日志部署完没有日志数据为什么每次拉代码都会出现找不到包的情况？工作日志的工作日期不能选择到上个月Kubernetes集群部署RUNNING HANDLER [etcd : wait for etcd up]本地启动devops的时候界面又加载不出来了修改应用名称（大写改小写）报名称已存在k8s部署集群问题 卡在node1部署集群容器中运行命令窗口没一分钟就关闭了数据库密码如何解密，登录时是怎么做的？敏捷管理-->问题管理-->故障，无法按“标签”搜索故障，导出故障结果中也没有“标签”字段猪齿鱼微服务这块有注意到JVM的问题吗？prometheus监控平台etcd集群用户故事地图拖拉史诗没有用监控日志收集服务tls配置修改无效gitlab在安装完成后出现问题活动冲刺的看板侧边栏弹出卡片排列顺序发生变化iam服务，oracle兼容性问题zipkin-collector没有提供JAVA_OPTS关于环境流水线创建的疑问创建环境流水线日志报错实例管理一直处理中又无法删除功能需求申请V0.10.0新问题来了应用部署失败官方服务无法打开，导致字体图标无法显示kubernetes安装错误与代码待优化、改善应用创建失败自定义 SVC 名称无法在网络中查看到.使用猪齿鱼安装 chart 应用时好像有些依赖开关不启作用.实例页面的修改配置有问题LDAP无法验证成功任务更改状态失败Choerodon能否创建一个能够部署war的应用创建猪齿鱼项目发布应用的时候如何利用现有集群中的负载均衡服务helm version error参考服务器安装模式安装部署K8S无法执行kubectl命令待办事项中对应的冲刺的数量和活动冲刺中的数量不一致创建应用状态显示失败，但代码已生成部署一个nginx的dns解析异常Gitlab Runner部署问题v0.10.0安装完成后出现的问题通过一个多月安装文档的提练与实践，干了个脚本出来，有需要的私聊哈自动获取Gitlab Impersonation Token失败，请手工获取后继续部署。 http://gitlab.example.choerodon.iov0.10.0 系统能够部署成功，但是部分服务出现问题 ，请帮忙指导一下如何排查gitlab安装ssh之后，访问报错v0.10.0安装时出现情况K8S添加节点有问题部署Choerodon一直not ready仅有内置一个组织？关于猪齿鱼平台监控信息的获取猪齿鱼迁移oracle后，部分SQL报错0.10 WIKI初始化后无法访问如何把choerodon 换成自己公司的logo?kubernetes在办公内网部署失败0.10版本自主搭建DNS后验证部署失败平台集群部署etcd启动失败部署流水线-实例打开日志页面有问题获取Pod状态的逻辑是不是有问题？配置域名时的路径怎么修改access_token的过期时间choerodon.mybatis有没有计划实现动态查询admin登录后，点个人中心或者管理中的任何菜单，页面提示403无权限测试管理升级报错：{"failed":true,"code":"error.saga.notExist","message":"saga不存在"}choerodon-agent接收不到get commandDNS无法解析开源企业级数字化服务平台——Choerodon猪齿鱼发布0.10版本发现升级到0.10以后，点击页面菜单经常会卡住v0.10一键部署到步骤28出错了微服务CI报错升级XWiki配置问题V0.10安装NFS时报错0.9升级0.10失败：Error: failed to download "c7n/test-manager-service"如何跟已有的gitlab服务整合wiki service一直起不来安装xwiki失败0.9升级0.10之后容器实例无法使用0.10版本开发流水线-代码仓库跳转地址bugAPI测试中的{project_id}怎么查？Choerodon平台版本: 0.9.0 一键执行出现问题[Step 9]: create database for Choerodon报错0.9升级完0.10敏捷管理-活动冲刺界面打不开deployments.extensions "manager-service" not found0.9升级到0.10 manager-service报错升级asgard service报错添加环境流水线gitops上无法创建目录安装kubernetes失败部署流水线创建的容器无法删除启动oauth-server镜像报Connect refused一键部署好几个步骤报错猪齿鱼token获取方式侧边栏和正文的分割线有问题其它服务如何调用oauth-server的/api/client/access接口，接口所需的参数(@RequestBody String additionalInfo)需要些什么数据关于猪齿鱼，如何获取token并设置token的有效时间关于应用部署健康检查的一些问题？http://example.choerodon.io回车后一片空白，没有任何显示登陆的用户名密码多少啊？部署后，无法在猪齿鱼中发现网络，K8S控制台中正常的关于猪齿鱼平台的登陆密码相关问题无法查到gitlab的版本号关于harbor配置https的一些问题sonarqube使用你好，请问choerodon的一键部署脚本今天是否有改动？create database for Choerodon关于猪齿鱼REST API无法接受数据问题ALIYUN的服务器下载到本地的速度好慢好慢求一个choerodon一键部署脚本，谢谢！电脑重启后，好多的pod都运行不正常了，有人遇到此类情况吗？服务正常,但是有NPE 异常.sonarqube安装步骤未找到部署流水线模块创建域名时一直是处于加载状态请教下devops有两个域名地址，有啥区别？DevOps服务升级过程中，调用平滑升级接口有报错NFS部署错误每次执行ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml -K 都报错猪齿鱼的oracle的依赖是用的那个。。单台MAC下部署的可能性有没有Mac测试一波部署中容器内部服务相互调用问题猪齿鱼微服务开发框架疑问一键部署失败(MySql)本地运行服务器模板程序报错猪齿鱼微服务架构支持单点登录吗？k8s down之后重启， 部分pod未启动请教下C7NHelmRelease用来做什么？manager-service部署问题IAM-SERVICE初始数据遇到了问题登录完成后点击任意功能报404sonar使用外部postgres，提示unzip错误关于自动化测试的咨询gitlab部署使用外部mysql不成功请教下GITLAB SSH那块22端口不想改如何处理？创建应用模板时提示saga不存在从应用市场部署应用失败后台应用部署报错请教一个关于安装的问题猪齿鱼，多语言开发怎么开发？求具体文档，前后台数据格式，以及开发标准用docker打开数据库报错猪齿鱼有接口管理功能吗？使用feign去调用iam User功能模块k8s 工作节点状态始终为 NotReadywiki编辑文档，保存后格式错乱部署的时候修改了配置到了确认信息的时候又恢复了？手动刷新权限报错Choerodon猪齿鱼直播分享 | 华润置地中台转型实践分享请教一下npm start之后，localhost:9090返回500超时敏捷管理功能不能使用创建应用，应用一直在创建中部署应用时 报错create failed: Send command failed, agent session closed!todo-service 在 Swagger 中测试API时返回 403sonar部署无法连接postgres问题请问什么时候出个正式稳定版本，比如说1.0猪齿鱼框架版本和起步依赖版本的对应关系todo-service api 无法在 Swagger 中被识别一键自动安装之后，wiki管理界面报错403关于部署正式环境和测试环境的问题如何获取 Jwt_Token组织wiki功能错误微服务开发PostMapping冲突请教下与gitops有关的部署的问题写的代码出现session was not found是什么原因，有没有知道的敏捷管理如何统计每个人的周工作c7n 后端开发手册中的数据库初始化部分执行不通项目敏捷管理相关功能创建失败说下这几天系统用下来的感受创建应用步骤，一直卡在创建中。查看gitlab已经创建完成。choerodon 0.9.0布署完成后多处403初始化数据库的oracle的脚本要怎么写创建应用，生成的代码库有时候会创建2个master保护分支API管理这个界面要什么权限才可以有呢wiki部署报错通过前台创建项目，显示‘创建成功’，但是前台列表里面没有显示创建的项目想请问一下这两种配置有什么区别？gitlab进行数据迁移后，pod一直启动不了，有报错喔唷，奔溃啦考虑新增CMDB组件吗？请教client配置的一些问题敏捷管理功能建议活跃冲刺中新建看板列和状态提示重复并卡住猪齿鱼创建组织下的项目中文名相同的有问题运行中的实例，创建中的应用，如何删掉？mysql的时区和服务器时区不一致k8s v1.8.5 kube-dns 服务无法启动，报：Failed create pod sandbox创建环境时，警告信息提示位置或者信息错误关于choerodon-front通过gitlab-ci编译出现的问题node状态变成SchedulingDisablednotify-service 状态404node节点内存还很多应用频繁OOMKilled如何使用监控功能追踪kubelet日志，发现有大量错误日志gitlab 部署失败Error executing action `run` on resource 'ruby_block[directory resource: /var/opt/gitlab/git-data]'部署0.9版本的持续交付部署时mysql异常猪齿鱼平台创建项目git地址无法显示请教choerodon-starters的有些jar包公有库没有怎么处理的？新git下来的iam -server,pom文件报错开放mysql到外部网络访问0.9版本编写项目上传.CI编译报auto_devops.sh这个报错H3 cloud 私有云网络类型疑问ManagerService的启动失败liquibase脚本有oracle版本的吗我helm 可以选择版本进行安装吗自主搭建DNS后验证部署失败升级环境代理0.9.7版本失败猪齿鱼，如何把私有云仓库加到猪齿鱼仓库列表记录agent报错 repo new: git clone --mirror: fatal: Could not read from remote repository.解决方案manager-service:0.8.0.RELEASE swagger页面去掉活动冲刺的看板不能删除吗？请问如何离线安装平台Helm脚本疑问部署流水线部署报错猪齿鱼微服务框架有定时任务的功能吗？nfs挂载客户端没反应分步部署，相关服务未运行部署应用之后，状态运行中，pod数为0，在k8s里面也未执行部署部署完猪齿鱼后发现网络不稳定devops-service中一直报错如何恢复上一次部署的容器？系统部署完成前台一直转圈无法显示页面gitlab-ci在mvn-package阶段报错外围系统调用swagger接口，怎么认证？使用自定义模板生成新的应用时，生成不成功创建域名时填写的域名地址是从何而来呢oauth-server数组越界OAuth的CustomUserDetail的空指针bug手动申请证书启动iam-service与iam前端应用遇到无法跨域访问的问题gateway-helper的maven仓库版本中的错误部署SonarQube 报错敏捷管理-自定义搜索条件如何定义创建一个前端应用碰到的问题Saga分布式事务一致性解决方案使用疑惑容器日志查看建议敏捷管理-活跃冲刺功能不能拖拽，直接拖拽报错部署流水线中域名修改有buggitlab报错日志猪齿鱼的分布式事务是怎么做的开发流水线-持续集成请求响应慢应用部署失败，提示没有这张表 mgmt_servicegateway-helper的组织id的问题后端开发启动demo报错一键安装的时候Choerodon iam service安装不上应用容器报错部署实例之后没有容器，容器状态为0写的接口没有自动插入到数据表iam_permission中部署实例一直在处理中github新版代码运行故障猪齿鱼平台可以挂自己的仓库吗？部分接口不在权限列表里面choerodon-manager-service not ready一直过不去环境流水线创建了，去哪运行生成指令按创建一个nginx示例文档创建示例CI报错gitlab数据库只能用mysql吗？部署环境必须要跟Choerodon安装的K8S一样？从0到1使用Kubernetes系列——Kubernetes入门一键安装choerodon myssql创建超时分布式锁的实现平台功能试用环境kubeadm-ansible 卡在 Initialize first master通过应用管理创建的应用项目无法下载应用部署状态与K8S中的资源状态不一致devops这块spinnaker做得很优秀 可以考虑下是否可以借鉴请问EnvGroupId是指什么?请问猪齿鱼后续会加入流程控制的功能？创建应用报错问题Pipelines经常构建失败，需要重复点击构建gitlab安装问题choerodon用户没有gitlab模板权限怎么办？项目管理中(应用管理)创建应用报错v0.9安装test-manager-service有问题组织中的创建菜单报错(包括创建项目，用户等)如何访问kubernetes-dashboard web界面一键安装脚本没有(部署知识管理)kubernetes集群微服务开发框架部署大量报错一键部署安装不成功访问搭建好的Choerodon的api 参数怎么弄？zookeeper 安装按文档安装k8s集群问题devops-service 容器报错agile-service 容器报错gitlab 认证问题升级后实例部署bug部署choerodon test manager fronthelm install manager service 服务报错基础组件Chartmuseum安装之后外部域名无法访问一键部署，卡在choerodon-iam-service，等了几个小时就终止了Choerodon猪齿鱼 0.9版本发布应用部署选择应用报错403启动环境时报错，哪位大佬帮忙看看。关于DNS解析的疑问部署流水线中，网络一直处在处理中手动部署报错启动工程时，报如下错误，有人遇到没？初始化数据库的时候，报错用例的执行状态可以后台自定义（成功，失败，作废，锁定）等。猪齿鱼升级后不能部署实例了？？？IT大咖说上的第二讲 猪齿鱼持续交付视频什么时候能上传liquibase生成数据时乱码，报sql语法错误本地mysql容器3307启动成功但无法正常连接应用部署报错k8s集群中如果用猪齿鱼的微服务开发框架，需要部署哪些？如何结束分支升级实例报错K8S集群突然大量pod出现 ErrorEvent服务 是什么意思？k8s集群启动错误deployments.extensions "choerodon-manager-service" not found配置DEBUG="--debug --dry-run"有什么用呢请求一个问题，启动的时候报了一个错制作了复杂的chart用猪齿鱼发布有些问题Choerodon猪齿鱼直播培训8月14日~8月16日开始了【登录】关于HAND公司员工无法登录公司猪齿鱼问题持续集成状态是失败的网络状态一直处理中，无法对其进行操作wiki创建页面页面会报错Windows10后端开发环境mysql容器无法开启成功FAILED - RETRYING: Master | wait for kube-controller-manager猪齿鱼的文件服务要怎么使用同一个k8s集，群开发环境、测试环境、正式环境部署gitlab-ci errorgit账号和密码一键部署，卡在choerodon-iam-service这里。状态显示检查8031端口失败一键部署Choerodon指令出错nginx示例 Pods的状态为ImagePullBackOff.你好，请教一下，怎么使用choerodon-front打包整个项目CI/CD的token搞丢了iam 如何和新开发的模块整合在一起Choerodon UI问题创建一个前端应用报错创建一个新用户，登录报错一键部署0.8报错 deployments.extensions "choerodon-manager-service" not foundhelm激活环境报错helm init 错误猪齿鱼安装NFS报错，报的错看不懂在node1中启动集群报错敏捷管理创建模块后查询不到结果vagrant up 启动报错vagrant搭建的本地k8s集群dashboard创建猪齿鱼前端Demo应用一直显示创建中vagrant安装kuberntes集群，cfssl-certinfo_linux-amd64文件不存在在Windows下，通过vagrant安装的集群，一键部署【紧急】应用部署之后状态立即变为“创建失败”部署流水线，创建网络无法选择实例网络中的外部IP疑问helm install ，connect: connection refused前端开发模块报错部署实例失败环境流水线-激活环境时执行激活脚本，报错部署猪齿鱼的服务器，缓存内存过多前端开发新页面疑惑wiki管理建议菜单拿出来一键部署域名如何解析敏捷管理-关于敏捷管理问题无法导出的疑问前端开发手册中运行代码报版本有误一键部署 choerodon-manager-service not ready问题管理 不能按时间、人员筛选域名管理报错send command time out菜单图标配置一键安装 "choerodon-kafka" not found敏捷管理-燃尽图不太理解持续集成：mvn-package:pending一键安装choerodon报错接口通信的安全性求助：自己开发的服务中DetailsHelper.getUserDetails获取不到认证信息安装kubectl报错猪齿鱼集成Websocket获取HttpSessionevent-store-service服务 send_back_event_store问题菜单分配权限与角色分配权限有什么关系？自主搭建的K8S集群 有一个master节点 NotReady 了扩展go-register-server基于v8部署平台菜单配置内容为空能否在“活跃冲刺”中增加报告者的筛选敏捷管理对子任务创建分支，创建者显示错误更新域名提示域名路径已经存在时一直显示更新中状态基于v8部署平台部分服务访问异常微服务服务注册不到eureka上基于v8版本平台搭建api swagger异常新建用户登录失败swagger api 500环境流水线可以添加个环境分组或者标签部署时出现的问题0.8安装完用admin登录平台页面都找不到结束分支时，触发的develop CI流水线失败时，如何再次重启该CI流水线？云维平台首页问题-发重复请求根据示例Demo，应用部署时实例启动日志报错0.8的测试管理安装完没有创建svc0.8部署postgresql有问题很需要从excel表导入已有用例到测试管理的功能。ci执行过程中报错分支管理界面非常慢应用部署时配置信息修改有问题k8s安装配置的ansible_user用户无root权限ci时报错certificate is valid for ingress.local容器日志查看No logs测试管理-报表-projectzzy测试管理部分项目页面空白admin的菜单有缺失wiki创建空间失败在merge时，gitlab报500项目里有分层结构chart部署文件该怎么写猪齿鱼0.7版本升级0.8版本后，出得一些问题猪齿鱼0.7版本升级0.8版本，升级iam-service有问题使用feign后从授权获取用户信息出错关闭分支触发流水线时，Error response from daemon升级了0.8.0后，之前的环境就不可用了部署知识管理后端 脚本有问题关于403及权限问题的建议部署xwiki 后，登陆wiki页面报错开源企业级数字化服务平台——Choerodon猪齿鱼发布0.8版本猪齿鱼0.7版本 升级 0.8版本，升级devops service有疑问敏捷管理的经办人列表使用不方便猪齿鱼0.7版本升级到0.8版本，升级manager service 脚本有误请问猪齿鱼0.7版本如何升级到0.8版本？ci合并分支时报错，can't stat 'target/app.jar': No such file or directory前端框架中同时使用AntD跟choerodon-ui有问题吗@Permission(permissionPublic = true)多看板切换问题gitlab CI 失败 ERROR: Job failed: error executing remote command猪齿鱼平台的文件挂载要怎么配置持续交付-》应用部署 出现报错使用logback收集日志，如何把log挂载出来基于平台iam-service中的用户模块自主封装用户模块，接口认证问题查看容器日志时浏览器崩溃一键部署 choerodon-manager-service not found在哪里可以看到源代码敏捷管理问题后端：event事件服务有没有想关的demo参考敏捷管理中重新分配经办人，人员列表刷不出来连续创建多个应用时，又两个应用创建有问题请问一下，使用0.7.0版本安装的菜单是不是有问题？问题看板对于已有状态更新时类别时，提示名称重复Choerodon的疑问没有应用导入导出功能持续集成问题文档中发布版本中缺少归档的使用说明希望在界面宽度允许的情况下，将列表字段信息显示完整agent chart配置文件镜像自动构建时版本时间问题域名是否能支持HTTPS配置部署https后oauth授权可能存在的bug微服务占用内存特别大chart模板监控日志配置部署了一个微服务应用，pod一直重启部署应用的日志显示问题Swagger api 403部署了一个应用，但swagger中没有信息打开管理 下的页面都是403，求教！执行values.sh 指定的目录都是需要自己的来创建的吗？部署实例，报内存不够selectCount()和多语言同时使用产生了bug分步部署时下载的docker镜像地址和 一键部署不一致一键部署时，namespaces "choerodon-devops-dev\r" not found持续交付-》应用部署 出现bug实例替换出现问题敏捷管理的创建子任务的UI界面又BUG微服务使用问题请问一下安装Kubernetes时，work node节点关于NFS安装的问题问题管理界面提示显示错位问题猪齿鱼的SaaS环境速度太慢，导致创建两个同样的问题开发流水线-分支管理-访问报错500敏捷管理待办事项问题统计没有生效待办事项的问题是否有考虑显示所属模块micro-service UI是不是工程有问题了添加Gitlab Client报错Choerodon-ui 这个组件库是否有使用文档如何自己搭建Gitlab Runner猪齿鱼微服务架构基础服务部署的时候，如何使用域名替换iprunner问题可以提供一下chart_build的脚本吗创建实例失败前端如何配置客户端id前端模板CI单词错误前端示例疑问应用分支管理问题应用发布界面操作问题开启冲刺的开始时间可任意选择待办事项中的问题拖入到冲刺中，左上角的人员列表BUG在待办事项界面，弹出侧边界面，滑动滚动条，出现界面BUG。k8s集群，在k8s-maste节点上多个域名如何设置0.6.1版本敏捷管理初始化数据库报错敏捷管理中快速搜索描述的有问题敏捷管理中，“报告”模块的表述有几个地方有问题敏捷管理的项目设置功能，“项目Code”建议写成“项目编码”请问在哪里可以查看Choerodon的版本？Choerodon浏览器支持问题自主搭建DNS部署失败请问0.6版本如何升级到0.7版本猪齿鱼服务器时间不正确Choerodon猪齿鱼 0.7版本发布iam前端菜单栏问题预定义角色未分配权限请问 猪齿鱼平台 可以添加组织吗0.6升级到0.7 升级choerodon front脚本有误执行run start的时候报错hugo工具有开源吗开发环境搭建敏捷管理平台：部分人员在分配角色后，任务分配界面选不到该成员请教镜像库是不是有变化本地vagrant部署k8s集群初始化helm异常新建实例如何使用自定义的数据库链接线上平台环境项目部署异常容器管理界面建议启的微服务看不到容器日志后端开发测试：通过浏览器访问失败问题部署时dns相关pod处于pending状态，cpu资源不足修改应用时报错github上 Choerodon Boot这个项目 README.md中的脚本不能执行一键部署结束后，无法访问，请帮忙指导创建应用 直接处于 创建中待办事项中的完成冲刺和结束冲刺用词不一致看板的上下一列没有对齐看板的列名称没有办法修改choerodon-starter-mybatis-mapper文档小建议能否基于平台的用户模块封装建议将开启冲刺界面的“Sprint名称”修改成“冲刺名称”choerodon-starter-core readme错别字在待办事项中，一次选择多个问题，拖拽到某史诗中，有遗漏问题站长，求一个简易易懂的部署搭建环境的教程初次安装后，未见错误信息，但无法访问系统猪齿鱼的环境搭建问题使用springboot模板创建的项目git clone时报错点击菜单出现404页面应用发布界面一直加载无响应，报错403创建组织问题看板提示：多于列处理中的最大长度，无法更新啊~好无奈啊，各种报错，好慌张，好无助部署监控时pv名字错误一键部署 问题多，在部署服务的时候，总是会出错，请楼主帮忙指点下POD运行状态是RUNNING，但报错猪齿鱼平台与gitlab之间存在的一些问题本地开发demo案例连接远程服务器，zk服务连接问题欢迎大家在论坛上提交Cloopm云运维平台相关的问题和建议kubernetes集群为什么无法在猪齿云上激活请问已经有gitlab并且上面建了很多项目猪齿鱼平台能整合吗ansible的时候卡在了某一个节点harbor的漏洞扫描【搭建部署】求指教分步部署devops文档中没有配置AGENT_REPOURL在待办事项页面中的任务输入描述后切换任务遇到问题安装完成后如何启用k8s的dashboard应用版本问题请问创建应用部署环境必须要在服务器上执行脚本吗？标准的那些表，赶紧重构吧本地demo案例通过swagger访问api调用无返回结果harbor安装不成功全屏编辑描述内容没有保存成功本地环境demo案例通过MANAGER-SERVICE、swagger访问时的路径端口问题oauth-server 0.6.0.RELEASE 启动报错前端开发手册-开发新页面-运行 python 脚本报错请问迭代管理和应用部署可以关联吗？0.6.0的前端开发环境如何运行代码敏捷管理接口部分部署有问题请教微服务依赖的中间件是怎么管理的？关于自建前端应用模板的疑惑0.6版本的分布部署gitlab最后打不开创建应用，显示一直在“创建中”Swagger测试接口 返回 403活跃冲刺中的快速搜索manager/swagger-ui ERROR PAGE安装go-register-server报错请问猪齿鱼后面会提供一键部署应用集合的功能吗？前端开发手册-开发新模块-运行 npm run gulp报错请问猪齿鱼有自动生成后端代码的工具吗新创建的应用生成的git仓库地址无法clone搭建猪齿鱼0.6.0出现权限问题0.6安装的时候找不到choerodon-front-iam请教猪齿鱼提供的gitlab.ci不同stage是怎么做到共享文件的请问有简便的版本升级方案吗？前端开发手册中，配置菜单无效请问0.6版本已经上线了吗怎么关掉gitlab-runner里面的esline校验后端开发手册中，模块运行中的docker镜像下载不了后端开发，构建本地环境，相关问题点github上的choerodon/config-server配置文件有错误企业级数字化服务平台——Choerodon猪齿鱼发布0.6版本父子组件渲染时报错如何引用样式文件在新的命名空间中部署register-server，发现有绑定其他命名空间的应用在部署kafka的时候，修改了一些变量，部署不成功用eslint校验发现从文档里下载的代码有错前端开发新模块时报错猪齿鱼前端框架and版本可以升级？客户端修改保存前端 choerodon-front-iam 部署创建应用时，一直处于创建中创建环境时，env-agent运行正常，但是一直显示未激活$ sh init-local-database.sh 执行数据库初始化缺少jar包请教猪齿鱼敏捷管理和敏捷开发都体现在哪里？使用自定义模板新建项目，后台有报错基础服务无法启动创建应用后一直处于处于创建中基础服务开启后自动关闭io.choerodon.test.infra.common.utils.StringUtil下载的后端应用中没有这个类在choerodon平台创建用户，在gitlab平台没有登录首页后，组织级和项目级只能看到系统配置类菜单svc的clusterIP访问不通后端发布项目pom配置的问题在搭建的平台上创建项目，gitlab中没有生成的groups发布了一个前端应用，但API接口提示没有访问权限在使用choerodon平台创建网络后，无法访问该网络eureka-server mvn clean install 编译报错提交代码后，ci 执行失败环境流水线中，激活环境时执行激活脚本，报错k8s集群间网络通信新建的前端项目demo页打开报错使用一键部署时报错，这是为啥代码提交以后，在平台上未看到更新demo案例服务无法注册又卡到这里了。FAILED - RETRYING: Copy etcdctl binary from docker container (3 retries left).请教gitlab配置成oauth之后，本地拉取代码怎么拉呢？如何排查创建项目后gitlab上没有Group和Issue？创建项目后对应gitlab没有创建Group和Issue库请问如何创建组织这一层，还有项目怎么删除？关于猪齿鱼微服务后端框架搭建问题分步安装gitlab文档的几点疑问根据开发手册部分构建本地开发环境问题harbor的安装配置总会重mount失败的问题运营管理的菜单在哪里初始化持续交付下详情进去无权限，为什么？搭建Kubernetes集群时报错前端开发手册-开发新模块中boot目录下无generator-hap前端开发新页面过程中报错前端代码源码方式运行新建的应用如何启动管理页面怎么进入平台搭建好之后，环境流水线页面一直处于等待状态创建分支时的问题添加gitlab client时，报403更正文档中前端代码运行步骤部署Runner时有报错部署choerodon front 脚本一直报DeadlineExceeded错误持续交付相关的菜单显示不出来请求ingress.host配置什么呢？部署devops service脚本中 pvc 名称需要更正部署devops service不成功 ，提示job DeadlineExceeded创建client 时 认证失败 报invalid_scope系统配置怎么使用更正所有服务里使用到kafka地址端口！！manager和iam初始化没有反应，spring-boot服务器镜像拉不来部署config-server后 连接不上Gitlab部署 不成功choerodon-front-iam 无法启动求助：harbor不能启动Chartmuseum部署中关系配置信息的问题成功部署了一个应用，需要怎么才能访问到教程里说的集群资源不太够的样子一步一坑啊！痛不欲生啊！解决了docker又来这个了！抱歉我的不熟练，这又是什么鬼？求教安装服务怎么访问？k8s集群安装完后，有些pod状态不是running正常吗？kube-dns 一直处于 Pending状态应用部署后一直报“context deadline exceed”错误应用部署过程中报错执行ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml报错在运行脚本与平台建立链接后，环境仍未连接提供的仓库怎么打不开Kubernetes 安装后运行不稳定激活环境报错在使用ansible-playbook执行部署集群脚本时，会有报错部署k8s环境异常Kubernetes集群部署 中 kubeadm-ansible.git 这个git库找不到部署Choerodon中提供的脚本有问题请问前后端应用要按照Choerodon的标准来吗？有计划增加按组织或者项目自己定义应用模版吗？centos7.3 的三个节点的默认登录账户密码是多少？分步安装的网页打不开内容似乎不清楚，这是一个完整的句子？欢迎来到Choerodon论坛有没有已经开发好的项目关于已有服务的使用Choerodon猪齿鱼正式宣布开源猪齿鱼社区朋友：当您在Choerodon猪齿鱼论坛提问的时候请注意以下几点：1. 选择正确的“主题分类”论坛有如下分类：A.Installation managementB.Choerodon FrameworkC.User GuideD.Community ContributionsE.Road Map2. 准确地描述“主题”标题例如，主题“安装Gitlab-Runner报错”，这个主题就非常的宽泛，安装Gitlab-Runner步骤很多，每一步都有可能出问题，可以这样来描述“在Gitlab上注册Gitlab-Runner的时候连接不到Gitlab-Runner”，这个描述非常清晰的表达出是在注册Gitlab-Runner的时候有问题，并且是找不到Gitlab-Runner，而不是其他的错误。还如，主题“客户端修改保存”，也同样很宽泛，作者也在描述中注明“在租户设置中的 客户端修改会报错，导致修改不成功”，这个描述还比较准确，如果把这句话作为“主题”就更好了。3.准确地描述“主题”内容4.避免黏贴大量代码5. 相关问题只发布一次提问谢谢您的理解与配合，让我们把沟通的成本降低，把处理问题的速度加快，愿我们做的更稳定、更好！Choerodon猪齿鱼社区0.5可以升级到0.6吗，如何升级呢若按照文档中不生成setter方法
则无法通过ConvertorHelper.convert()转换对象
BTW: iam-service中的entity是有setter方法的你好，文档中建议entity 对象的属性通过构造函数添加。在convert 中需要单独实现xxxToEntity的方法。http://choerodon.io/zh/docs/development-guide/backend/demo/domain/这是一种建议的方法，在猪齿鱼中不强制对此进行限制。可以遵循自己项目的开发规范实现实体类的转换。那还是加setter方法好了, copyProperties用着还是方便Choerodon平台版本: 0.11.0遇到问题的执行步骤:
一键部署的时候很多基础组件拉镜像失败，请问目前是镜像仓库出问题了么？ 什么时候能修复？
Failed to pull image “registry.saas.hand-china.com/hitoa/c7n-slaver:latest”文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问手动  docker pull  registry.saas.hand-china.com/hitoa/c7n-slaver:latest  也失败hi, 你可以把c7n-slaver的 daemonset 删除后重试请教下怎么删除呢？kubectl delete ds c7n-slaver -n c7n-system事务实例iam/saga-instance这个菜单能不能下放到项目里面来，因为事务失败的时候，项目管理者想知道原因，但他又没有平台的权限，然后又不停尝试，浪费了些时间定位问题，最后还要抛到平台管理员这边你好，目前事务实例的所有数据都是平台运行时产生的。在后续的版本中，我们会为事务实例添加对应的通知机制，并且根据触发的源头将数据在组织，项目层展示。Choerodon平台版本: 0.11.0遇到问题的执行步骤:
一键安装choerodon后，执行后继步骤，在证书配置中，执行“ 没有公网域名时使用自签名证书”的步骤出错文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/base/harbor/#证书配置报错日志:
[root@node1 ~]# kubectl get secret \Error from server (NotFound): secrets “harbor-harbor-ingress” not found原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，请执行以下命令，提供一下返回结果执行kubectl get secret -n c7n-system返回
请再执行一下下面命令 ，提供一下返回结果执行结果如下
请问你有执行过kubectl delete之类的操作吗？好像是有，怎么重装一下harbor？用的一键安装不用重装，按下面命令来一次就好解决了，谢谢。Choerodon平台版本: 0.11.0问题描述:
问题一：在组织层进行集群的创建流水线环境，并完成了此流水线环境的关联（状态为运行中）。但此后基于这个集群新建新的流水线环境(如env-001)，都没有任何反应，在k8s集群中么有任何的命名空间，服务，和Pod找到与(env-001相关的)，helm list 里面也没有。
问题二：此时我先忽略问题一，新部署一个应用(app-001)，环境选择刚刚新建的env-001，此时实例中我新部署的应用一直处于部署中请问这是什么情况？问题一：在组织层进行集群的创建流水线环境，并完成了此流水线环境的关联（状态为运行中）。但此后基于这个集群新建新的流水线环境(如env-001)，都没有任何反应，在k8s集群中么有任何的命名空间，服务，和Pod找到与(env-001相关的)，helm list 里面也没有。在内部pass平台集群客户端最开始的版本有个bug，已经修复了，有可能是这个原因，把集群客户端pod删除下，重新拉了镜像启动了就好了。你好！谢谢建议，问题已经解决。
重启POD之后(kubectl delete pod <> -n <>)，命名空间就可以出来了(如env-001)Choerodon平台版本: 0.6.0遇到问题的执行步骤:
如何访问部署在虚拟机上的Choerodon文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/choerodon/环境信息(如:节点信息):
单节点集群报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问Hi ，请查看此处dns怎么重新设置虚拟机部署，dns服务器可以部署到主机上吗？还是要部署到虚拟机上？你没有域名吗，没有域名的话 dns部署在服务器上比较方便，您的主机直接修改 hosts文件即可有域名的话，虚拟机上怎么来解析域名， kube-dns服务怎么配置（帖子被作者删除，如无标记将在 24 小时后自动删除）Choerodon平台版本：0.6.4运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：我们这边对非主键的处理是什么呢，猪齿鱼对那些没有主键的表有强校验，不继承BaseMapper又扫不进去通用mapper的集合。
目前的通用mapper确实是强依赖主键的，因为插入的时候要根据设置的主键策略（自增或者序列或者uuid等）去维护主键，然后在插入后回传主键。而且更新方法都是根据主键更新的。
判断插入成功是 insert方法会返回int的插入数量，如果是0 就是没插入成功。
无主键的表目前没有做测试，因为目前我们的都是有主键的表，暂时也没有考虑。
可能需要你自己写一些Provider
扫描mapper是要把自定义一个mapper，然后继承Marker这个标记接口
（帖子被作者删除，如无标记将在 24 小时后自动删除）谢谢Choerodon平台版本: 0.10.0问题描述我们同一个组织下，有三个项目分别为A,B,C;项目A,B下面有一个应用的名字都叫appS然后，将项目A,B下打包的appS镜像都发布到应用市场；最后，C项目下从应用市场随便选择一个叫appS的镜像去部署；同步就会出现问题，因为代码中找到多个应用时，默认取的都是第一个。取的第一个后，后面去查询应用版本是否存在，就报appversion.not.exist.in.database;实际上，我遇到的这个情况，应该取第二个应用。不知道我能不能解释明白这个问题？实际上如果存在多个同名应用，可能都有这个问题下面是我临时加的一段修复逻辑，有不当之处请指正
您好， 当不同项目下的同一code的应用都发布到应用市场时，选择一个部署，会出现你说的问题，默认取了第一个。然后你的修改方案适合从choerodon平台前端点击部署，如果通过直接从gitops环境库新增release文件部署时，此时实例也会不存在，也会导致version not exist的问题。 我们这边已记录bug,  将在下个版本中修改获取应用的方法。 感谢您的贡献！Choerodon平台版本: 0.11.0遇到问题的执行步骤:
4节点choerodon集群，晚上集群断电，早上发现choerodon各个服务的pod一直不断crash，重启。
请问目前平台服务在集群断电重启后不能自动修复么？   对于断电重启有什么好的解决办法呢？文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，猪齿鱼服务间是有依赖关系的，请按安装顺序依次启动就好。意思是断电重启后，只需按照安装步骤里面将服务重新启停下就行了么？嗯  是的哈
但是发现这些基础组件也有没起来的， 基础组件也需要启停么？不需要哈Choerodon平台版本：0.11.0运行环境：SaaS问题描述：系统中组织管理员、组织成员、项目所有者、项目成员、部署管理员等，拥有哪些具体的权限，这个目前在官网文档中没有维护。Choerodon平台版本: 0.11.0遇到问题的执行步骤:
执行 命令 ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml到RUNNING HANDLER [master : Master | wait for kube-controller-manager]无法继续已经用命令 ansible-playbook -i inventory/hosts reset.yml 进行重置，但重新执行部署还是会卡在这里你好，请问此服务器有没有自己手动安装过docker呢？没有，昨天因为出现waiting slaver running…后，没有解决，所以将整系统重装了，然后再按新安装的步骤进行操作。
步骤和之前安装一样执行，但今天却出现这个情况。Choerodon平台版本: 0.6.0遇到问题的执行步骤:
执行 ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml 报错：
文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请提供一下你编写的inventory/hosts文件内容，谢谢你好，hosts name不能有大写字母的哈，请修改ecs的host name见图
registry.choerodon.com.cn/choerodon-c7ncd/choerodon-cluster-agent:0.11.0好的。指的是短时间新增多个环境吗，重启只能能正常使用吗。更正一下，观察了阵子什么都不操做的情况下也会重启，现在重启记录是6次，早上截图当时还是4次，创建环境时看到panic日志也可能是碰巧，两个集群部署的agent都有这种情况Choerodon平台版本: 0.11.0遇到问题的执行步骤:一键部署后，一直打印2018/11/27 18:05:17 [INFO] waiting slaver running…
2018/11/27 18:05:23 [INFO] waiting slaver running…
2018/11/27 18:05:29 [INFO] waiting slaver running…
2018/11/27 18:05:35 [INFO] waiting slaver running…
2018/11/27 18:05:41 [INFO] waiting slaver running…
2018/11/27 18:05:47 [INFO] waiting slaver running…
2018/11/27 18:05:53 [INFO] waiting slaver running…
2018/11/27 18:05:59 [INFO] waiting slaver running…
2018/11/27 18:06:05 [INFO] waiting slaver running…
2018/11/27 18:06:11 [INFO] waiting slaver running…
2018/11/27 18:06:17 [INFO] waiting slaver running…
2018/11/27 18:06:23 [INFO] waiting slaver running…
2018/11/27 18:06:29 [INFO] waiting slaver running…
2018/11/27 18:06:35 [INFO] waiting slaver running…
2018/11/27 18:06:41 [INFO] waiting slaver running…
2018/11/27 18:06:47 [INFO] waiting slaver running…
2018/11/27 18:06:53 [INFO] waiting slaver running…
2018/11/27 18:06:59 [INFO] waiting slaver running…
2018/11/27 18:07:05 [INFO] waiting slaver running…
2018/11/27 18:07:11 [INFO] waiting slaver running…
2018/11/27 18:07:17 [INFO] waiting slaver running…
2018/11/27 18:07:23 [INFO] waiting slaver running…
2018/11/27 18:07:29 [INFO] waiting slaver running…
2018/11/27 18:07:35 [INFO] waiting slaver running…
2018/11/27 18:07:41 [INFO] waiting slaver running…
2018/11/27 18:07:47 [INFO] waiting slaver running…
2018/11/27 18:07:53 [INFO] waiting slaver running…
2018/11/27 18:07:59 [INFO] waiting slaver running…
2018/11/27 18:08:05 [INFO] waiting slaver running…
2018/11/27 18:08:11 [INFO] waiting slaver running…
2018/11/27 18:08:17 [INFO] waiting slaver running…
2018/11/27 18:08:23 [INFO] waiting slaver running…服务器是否连接到了网络？网络畅顺，安装其他步骤都没问题看下c7n-system 下有没有pendding的pod[root@node1 c7n-0.1.0]# kubectl get po -n c7n-system
NAME                       READY     STATUS              RESTARTS   AGE
c7n-slaver-hpq2j           0/1       ContainerCreating   0          15h
dnsmasq-5f5777c999-rlbgs   1/1       Running             0          15h
[root@node1 c7n-0.1.0]# kubectl get po -n kube-system
NAME                                        READY     STATUS    RESTARTS   AGE
default-http-backend-6dd4d5b7c9-9scps       1/1       Running   0          15h
heapster-746d67c7b9-w27b2                   1/1       Running   0          15h
kube-apiserver-node1                        1/1       Running   0          15h
kube-controller-manager-node1               1/1       Running   0          15h
kube-dns-79d99555df-dpxht                   3/3       Running   0          15h
kube-flannel-t296t                          1/1       Running   0          15h
kube-lego-6f45757db7-d526z                  1/1       Running   0          15h
kube-proxy-gm9g5                            1/1       Running   0          15h
kube-scheduler-node1                        1/1       Running   0          15h
kubernetes-dashboard-dc8fcdbc5-l52bw        1/1       Running   0          15h
nfs-provisioner-d68984794-ggsdt             1/1       Running   0          15h
nginx-ingress-controller-5d77d4945d-9lzcs   1/1       Running   0          15h
tiller-deploy-6bf79495c8-qnvxg              1/1       Running   0          15h
[root@node1 c7n-0.1.0]# ./c7n install -c config.yml --no-timeout --debug
2018/11/28 09:08:58 [INFO] getting resource /version.yml
2018/11/28 09:08:59 [INFO] getting resource /0.11/install.yml
2018/11/28 09:08:59 [INFO] namespace c7n-system already exists
2018/11/28 09:08:59 [DEBUG] ignore create pv because specify storage class and no other persistence config
2018/11/28 09:09:00 [INFO] waiting slaver running…
2018/11/28 09:09:06 [INFO] waiting slaver running…pod一直在ContainerCreating你好，请执行下面语句，提供一下返回结果kubectl describe ds c7n-slaver -n c7n-system执行后的结果
我将系统重装再试试好了请再执行下下面语句，提供一下返回结果kubectl describe pvc -n c7n-system slaverkubectl describe pvc -n c7n-system slaver执行结果
Choerodon平台版本：0.11.0运行环境：SaaS问题描述：已经创建成功空间，但是点击进入却显示，提示没有。这个空间应该是被删除了，现在这个还不好同步，因为在xwiki那边被删除的，Choerodon这边不好判断。
不过在XWiki那边是有删除记录的。好的。Choerodon平台版本: 0.11.0遇到问题的执行步骤:
安装到Gitlab的时候，gitlab pod一直挂掉重启， 通过kubectl get pods -n c7n-system -w命令查看，
然后查看pod的日志kubectl logs -f  -n c7n-system gitlab-5c5bb76595-pktmf
文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/choerodon/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请问是否使用的这里文档搭建的nfs动态存储？http://choerodon.io/zh/docs/installation-configuration/steps/nfs/我们使用的 没有NFS服务服务器 步骤你的主机是什么操作系统？centos7.5你好，请参考一下这里进行操作，看看是否可行Hi, I'm trying to install Gitlab CE on a vanilla Ubuntu Server 16.04 and got the following error; then I tried to fall back on CentOS7 and am having the...Choerodon平台版本: 0.10遇到问题的执行步骤:
创建应用环境信息(如:节点信息):
k8s 1.10报错日志:
2018-11-28 09:49:30.284  WARN [devops-service,] 1 — [saga-consumer-3] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor invoke method error, transaction rollback, msg SagaTaskInstanceDTO{id=102, sagaInstanceId=57, taskCode=‘devopsOperationGitlabProject’, sagaCode=‘devops-create-gitlab-project’, instanceLock=‘172.20.154.137:8060’, status=‘RUNNING’, seq=1, input=’{“userId”:1,“path”:“frontdemo1”,“groupId”:14,“type”:“application”,“organizationId”:1,“appId”:12}’}, cause
io.choerodon.core.exception.CommonException: error.branch.create
at io.choerodon.devops.infra.persistence.impl.GitlabRepositoryImpl.createProtectBranch(GitlabRepositoryImpl.java:145)
at io.choerodon.devops.app.service.impl.ApplicationServiceImpl.operationApplication(ApplicationServiceImpl.java:325)
at io.choerodon.devops.api.eventhandler.DevopsSagaHandler.createApp(DevopsSagaHandler.java:97)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.invoke(SagaMonitor.java:194)
at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.run(SagaMonitor.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: feign.FeignException: status 500 reading GitlabServiceClient#createProtectedBranches(Integer,String,String,String,Integer); content:
{“failed”:true,“code”:“Protected branch ‘master’ already exists”,“message”:“Protected branch ‘master’ already exists”}
at feign.FeignException.errorStatus(FeignException.java:62)
at feign.codec.ErrorDecoder$Default.decode(ErrorDecoder.java:91)
at feign.SynchronousMethodHandler.executeAndDecode(SynchronousMethodHandler.java:138)
at feign.SynchronousMethodHandler.invoke(SynchronousMethodHandler.java:76)
at feign.ReflectiveFeign$FeignInvocationHandler.invoke(ReflectiveFeign.java:103)
at com.sun.proxy.$Proxy214.createProtectedBranches(Unknown Source)
at io.choerodon.devops.infra.persistence.impl.GitlabRepositoryImpl.createProtectBranch(GitlabRepositoryImpl.java:142)
… 11 more０.10版本的bug, 请参考这个topic   应用创建失败Choerodon平台版本：0.6.0运行环境：本地主机自主搭建(win10+vagrant+virtualbox)问题描述：1、使用ansible安装部署集群完成报错：
fatal: [node1 -> node1]: FAILED! => {“changed”: true, “cmd”: [“kubectl”, “taint”, “nodes”, “node1”, “node-role.kubernetes.io/master-”], “delta”: “0:00:00.129858”, “end”: “2018-11-27 11:42:58.997671”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-11-27 11:42:58.867813”, “stderr”: “The connection to the server localhost:8080 was refused - did you specify the right host or port?”, “stderr_lines”: [“The connection to the server localhost:8080 was refused - did you specify the right host or port?”], “stdout”: “”, “stdout_lines”: []}
2、运行kubectl命令报错kubectl get nodes：
The connection to the server localhost:8080 was refused - did you specify the right host or port?
3、查看报错日志journalctl -n 100 -f -u kubelet：
Nov 27 11:53:21 node1 systemd[1]: kubelet.service holdoff time over, scheduling restart.
Nov 27 11:53:21 node1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 27 11:53:21 node1 systemd[1]: Starting kubelet: The Kubernetes Node Agent…
Nov 27 11:53:21 node1 kubelet[14739]: I1127 11:53:21.790030   14739 feature_gate.go:156] feature gates: map[]
Nov 27 11:53:21 node1 kubelet[14739]: I1127 11:53:21.790100   14739 controller.go:114] kubelet config controller: starting controller
Nov 27 11:53:21 node1 kubelet[14739]: I1127 11:53:21.790105   14739 controller.go:118] kubelet config controller: validating combination of defaults and flags
Nov 27 11:53:21 node1 kubelet[14739]: error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory
Nov 27 11:53:21 node1 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Nov 27 11:53:21 node1 systemd[1]: Unit kubelet.service entered failed state.
Nov 27 11:53:21 node1 systemd[1]: kubelet.service failed.1、下载ansible到本地路径
2、修改vagrfile文件循环一次(一个节点)，修改inventory/hosts如下
[all]
node1 ansible_host=192.168.56.11 ip=192.168.56.11 ansible_user=root ansible_ssh_pass=vagrant ansible_become=true3、运行vagrant up完成后，配置相应信息(关闭Selinux，关闭Swap，关闭firewall，关闭iptable)
4、运行集群部署命令ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml，报错：
fatal: [node1]: FAILED! => {“msg”: “Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this.  Please add this host’s fingerprint to your known_hosts file to manage this host.”}
5、修改配置文件
vi /etc/ansible/ansible.cfg
host_key_checking = False    #不检测host key
6、再次运行部署，报错TASK [etcd : Generate Ca certs] ****************************************************************************************************************************************************************
fatal: [node1 -> node1]: FAILED! => {“changed”: true, “cmd”: [“kubectl”, “taint”, “nodes”, “node1”, “node-role.kubernetes.io/master-”], “delta”: “0:00:00.129858”, “end”: “2018-11-27 11:42:58.997671”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-11-27 11:42:58.867813”, “stderr”: “The connection to the server localhost:8080 was refused - did you specify the right host or port?”, “stderr_lines”: [“The connection to the server localhost:8080 was refused - did you specify the right host or port?”], “stdout”: “”, “stdout_lines”: []}NO MORE HOSTS LEFT *****************************************************************************************************************************************************************************
to retry, use: --limit @/vagrant/cluster.retryPLAY RECAP *************************************************************************************************************************************************************************************
node1                      : ok=44   changed=30   unreachable=0    failed=1请执行下面命令查看日志你好，日志如下
建议你先reset集群后重试。reset集群时，如有network相关报错请忽略。你好，以下是reset的报错，重新部署依然存在以上一样的错误
这样吧   建议你按下面命令进行操作：谢谢， 安装以上操作可以了Choerodon平台版本: 0.10.0遇到问题的执行步骤:
先按照一键部署的流程进行安装，wiki安装失败。 再按照分步部署文档中知识管理部署模块的内容部署wiki，执行创建数据库helm install c7n/mysql-client 
–set env.MYSQL_HOST=c7n-mysql.c7n-system.svc 
–set env.MYSQL_PORT=3306 
–set env.MYSQL_USER=root 
–set env.MYSQL_PASS=password 
–set env.SQL_SCRIPT="
CREATE USER IF NOT EXISTS ‘choerodon’@’%’ IDENTIFIED BY ‘password’;
CREATE DATABASE IF NOT EXISTS wiki_service DEFAULT CHARACTER SET utf8;
CREATE DATABASE IF NOT EXISTS xwiki DEFAULT CHARACTER SET utf8;
GRANT ALL PRIVILEGES ON wiki_service.* TO choerodon@’%’;
GRANT ALL PRIVILEGES ON xwiki.* TO choerodon@’%’;
FLUSH PRIVILEGES;" 
–version 0.1.0 
–name create-c7nwiki-db 
–namespace c7n-system
命令后，通过kubectl get pod --all-namespaces -w命令，出现下面的错误：
但是mysql里面已经创建wiki_service、xwiki两个数据库。文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/choerodon-wiki/环境信息(如:节点信息):
5节点，单节点4核4线程，内存16GB，磁盘空间160GB.报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问一键安装失败 有报什么错吗一键失败时，当时看了日志好像是拉取xwiki:10.4-mysql-tomcat-0.11.0镜像失败了，是从 汉得的仓库拉取的。 然后按照分步部署里面 部署知识管理模块按照，第一步创建数据库就失败了。镜像拉取失败的话 直接重试一键安装程序就可以了 ， 不用到分布安装里面的好的，谢谢，我再试下重新一键安装，不需要停止服务吧？不需要的遇到同样问题，一键安装，到xwiki就无法成功安装了
又重新执行了一次一键安装，同样是到xwiki那里就不断重试，到最后xwiki install failed
2018/11/27 11:39:07 [INFO] job xwiki-init-config haven’t finished yet. please wait patiently
2018/11/27 11:39:07 [DEBUG] still install xwiki
2018/11/27 11:39:17 [INFO] job xwiki-init-config haven’t finished yet. please wait patiently
2018/11/27 11:39:17 [DEBUG] still install xwiki
2018/11/27 11:39:19 [Error] rpc error: code = Unknown desc = Job failed: DeadlineExceeded
2018/11/27 11:39:19 [Error] install failed您再试一下，我这边更新了一下镜像库和超时Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:
安装过程一直提示500
我的环境是2台8H32G的腾讯云服务器，两台公网和内网IP对应：
111.231.204.22    172.27.0.14
132.232.125.136  172.27.0.17原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问看下 config-server的日志日志也是这个错呢
到config-service容器中 执行 curl localhost:8011/health 看看你好 是eureka没有启动起来么？ 你好，是进入config-service对应的pod中执行curl localhost:8011/health命令  不是在本机执行。如果不知道怎么进pod中执行命令，请参考下面文档http://hardocs.com/d/kubernetes/150-kubectl_exec.html你好，是这样执行么？你好，0.10.0中已经移除了swagger-ui。有关api测试可以通过前端管理-> API管理 -> API测试 进行猪齿鱼平台权限不够。。看不到API管理。。。
难道以后本地开发服务，只能自己搭前端测了吗。。。0.10.0新增了平台开发者的角色。可以给自己分配该角色。本地开发推荐postman。这样不用单独运行manager-service，而且更加灵活小哥哥这个是因为什么呢，从swagger的授权按钮点击跳转的
还有这里的登录入口在哪，怎么登录呢只启动了这四个服务：
可以看一下oauth-server 和api-gateway 是否已经启动成功。小哥哥现在是直接访问各自的接口可以访问，但是 http://localhost:8080/oauth/oauth/login 访问不了，
还有这个iam的controller，http://localhost:8030/v1/languages/list或者http://localhost:8030/v1/organizations/1/org_level可以访问，但是http://localhost:8080/iam/v1/languages/list,api-gateway调用iam不通，这是路由问题吗
api-gateway 调用依赖的是 zuul 的路由列表，你启动 api-gateway 和 gateway-helper 时，有没有启用 config-server。如果启用了，需要manager_service.MGMG_ROUTE 这张表有没有对应的路由数据。如果没有启用，需要查看 api-gateway 的 application.yml 文件有没有配置zuul.route。我用的是api-gateway的0.11.0版本的，直接下载下来的没有改动，本地没有启动config-server，然后这张表里面只有一个例子的路由记录。
看一下注册中心有没有服务的清单，如果有的话，试下localhost:8081/health 和 localhost:8081/env登录的默认地址应该是 http://localhost:8080/oauth/loginhttp://localhost:8080/oauth/login登录了用admin admin然后跳转到了swagger，这个页面上面说已经没了
然后我用postman测试需要带jwt_token，jwt_token怎么获得呢
我看我点登录的时候api-gateway没有打印出和jwt_token有关的东西
@sssnow通过 api-gateway 的请求需要用 access_token，可以从登录时的请求中获取。通过直接访问请求接口，需要使用 jwt_token, 可以在 https://jwt.io 生成我私信你一个默认的jwt_token。acess_token是从http://127.0.0.1:8080/oauth/login登录吗，从这里用admin登录进去，跳转到下面页面了，然后在cookie中没有看到access_token，我可以在哪里打断点看到这个access_token吗
这个接口的response header 会返回一个带access_token的跳转地址没看到
或者通过接口 调用的形式获取。啊啊啊啊，这样子滴吗，
啊哈，acces_token有了，这个没好：
Choerodon平台版本: 0.11.0遇到问题的执行步骤: 升级manager-service文档地址: http://choerodon.io/zh/docs/installation-configuration/update/0.10-to-0.11/环境信息(如:节点信息):报错日志:helm upgrade manager-service c7n/manager-service 
–set env.open.JAVA_OPTS="-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap" 
-f <(helm get values manager-service) 
–version 0.11.0正确的版本号是 0.11.1原因分析:脚本编写错误，版本号不对疑问:提出您对于遇到和解决该问题时的疑问asgard-service 服务版本 0.11.1helm upgrade gateway-helper c7n/gateway-helper 
–set env.open.JAVA_OPTS="-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap" 
–set env.open.SPRING_CACHE_MULTI_L1_ENABLED=true 
–set env.open.SPRING_CACHE_MULTI_L2_ENABLED=false 
–set env.open.SPRING_REDIS_HOST=redis 
–set env.open.SPRING_REDIS_PORT=6379 
–set env.open.SPRING_REDIS_DATABASE=4 
-f <(helm get values gateway-helper) \   —not  notify-service
–version 0.11.1helm upgrade devops-service c7n/devops-service 
–set env.open.JAVA_OPTS="-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap" 
–set preJob.preConfig.mysql.host=c7n-mysql.c7n-system.svc 
–set preJob.preConfig.mysql.port=3306 
–set preJob.preConfig.datasource.url=“jdbc:mysql://c7n-mysql.c7n-system.svc:3306/manager-service?useUnicode=true&characterEncoding=utf-8&useSSL=false” \    //此处manager-service 应该是 _ manager_service  //很难找到这个错误
–set preJob.preConfig.datasource.username=choerodon 
–set preJob.preConfig.datasource.password=password 
–set preJob.preInitDB.mysql.host=c7n-mysql.c7n-system.svc \
–set preJob.preInitDB.mysql.port=3306 \
–set preJob.preInitDB.datasource.url=“jdbc:mysql://c7n-mysql.c7n-system.svc:3306/devops_service?useUnicode=true&characterEncoding=utf-8&useSSL=false” 
–set preJob.preInitDB.mysql.username=choerodon \  – preJob.preInitDB. datasource.username
–set preJob.preInitDB.mysql.password=password \  – preJob.preInitDB. datasource.password
–set env.open.SPRING_DATASOURCE_URL=“jdbc:mysql://c7n-mysql.c7n-system.svc:3306/devops_service?useUnicode=true&characterEncoding=utf-8&useSSL=false” 
–set env.open.SPRING_DATASOURCE_USERNAME=choerodon 
–set env.open.SPRING_DATASOURCE_PASSWORD=password 
–set env.open.EUREKA_CLIENT_SERVICEURL_DEFAULTZONE=“http://register-server.c7n-system:8000/eureka/” 
–set env.open.SPRING_REDIS_HOST=c7n-redis.c7n-system.svc 
–set env.open.SPRING_REDIS_DATABASE=3 
–set env.open.CHOERODON_EVENT_CONSUMER_KAFKA_BOOTSTRAP_SERVERS=“kafka-0.kafka-headless.c7n-system.svc.cluster.local:9092,kafka-1.kafka-headless.c7n-system.svc.cluster.local:9092,kafka-2.kafka-headless.c7n-system.svc.cluster.local:9092” 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=“kafka-0.kafka-headless.c7n-system.svc.cluster.local:9092,kafka-1.kafka-headless.c7n-system.svc.cluster.local:9092,kafka-2.kafka-headless.c7n-system.svc.cluster.local:9092” 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES=“zookeeper-0.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.c7n-system.svc.cluster.local:2181” 
–set env.open.SPRING_CLOUD_CONFIG_ENABLED=true 
–set env.open.SPRING_CLOUD_CONFIG_URI=“http://config-server.c7n-system:8010/” 
–set env.open.SERVICES_HARBOR_BASEURL=“https://registry.example.choerodon.io” 
–set env.open.SERVICES_HARBOR_USERNAME=admin 
–set env.open.SERVICES_HARBOR_PASSWORD=“Harbor12345” 
–set env.open.SERVICES_HELM_URL=“http://chart.example.choerodon.io” 
–set env.open.SERVICES_GITLAB_URL=“http://gitlab.example.choerodon.io” 
–set env.open.SERVICES_GITLAB_SSHURL=“gitlab.example.choerodon.io” 
–set env.open.SERVICES_GITLAB_PASSWORD=password 
–set env.open.SERVICES_GITLAB_PROJECTLIMIT=100 
–set env.open.SERVICES_GATEWAY_URL=http://api.example.choerodon.io 
–set env.open.SECURITY_IGNORED="/ci,/webhook,/v2/api-docs,/agent/,/ws/,/webhook/**" 
–set env.open.AGENT_VERSION=“0.11.0” 
–set env.open.AGENT_REPOURL=“https://openchart.choerodon.com.cn/choerodon/c7n/” 
–set env.open.AGENT_SERVICEURL=“ws://devops.example.choerodon.io/agent/” 
–set env.open.TEMPLATE_VERSION_MICROSERVICE=“0.11.0” 
–set env.open.TEMPLATE_VERSION_MICROSERVICEFRONT=“0.11.0” 
–set env.open.TEMPLATE_VERSION_JAVALIB=“0.11.0” 
–set ingress.enable=true 
–set ingress.host=devops.example.choerodon.io 
–set service.enable=true 
–set persistence.enabled=true 
–set persistence.existingClaim=“chartmuseum-pvc” 
–version 0.11.0helm upgrade agile-service c7n/agile-service 
–set env.open.JAVA_OPTS="-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap" \  此处丢掉了\ 符号
-f <(helm get values agile-service) 
–set env.open.SPRING_DATASOURCE_URL=“jdbc:mysql://c7n-mysql.c7n-system.svc:3306/agile_service?useUnicode=true&characterEncoding=utf-8&useSSL=false&allowMultiQueries=true” 
–version 0.11.1感谢您的反馈，我们会尽快修复文档中的问题每一次升级都像是在挑战不可能完成的任务:joy:开个玩笑我们正在尝试实现一键升级， 如果顺利的话 下个版本开始升级会变得简单些V大，升级0.11后，部署集群agent之前，手动数据库清理了一些演示项目的agent数据和helm delete 了相关的agent(因为之前agent部署的实在太多了)，集群agent部署后，在这些清理过的项目中创建环境，虽然环境显示运行中，但在gitlab项目关联的gitops群组里头并没有生成与环境编码相对应的项目，同时在集群的agent上一直提示git clone仓库不存在的提示，这种情况应该如何修复
这些没用的就可以从数据库里面删掉了,删掉再重启集群agent就不会了Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问
起步就遇到很多问题啊，这是啥原因啊请检查部署步骤检查是否有遗漏http://choerodon.io/zh/docs/installation-configuration/steps/helm/每个步骤都执行了的您好 需要检查的是helm的安装命令我重装系统无数次了，还是这个问题，肯定是你们脚本有问题啊 请问你的k8s集群是哪个版本？是这个么请执行你好，执行后：帮忙看下啊你确认你是执行的下面命令吗？Choerodon平台版本：0.11.0运行环境：SaaS问题描述：我的理解 ，测试用例不应该被描述成问题。好的，这里的描述用语我们再斟酌修改一下。Choerodon猪齿鱼是一个开源企业服务平台，是基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理的开源平台，同时提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，致力帮助企业聚焦于业务，加速数字化转型。2018年11月23日，Choerodon猪齿鱼发布0.11版本，本次更新内容包含新增了访问快捷方式、敏捷工作日历、站内通知、开发控制台、集群管理等诸多功能，以及对Agent、环境权限等进行了优化，欢迎各位更新体验。发布版本：0.11发布时间：2018年11月23日功能范围：知识管理、敏捷管理、持续交付、测试管理以及微服务开发框架下面就为大家带来详细的版本更新介绍。新增在Wiki中创建空间的功能。新增文档收藏功能。新增分享按钮，可以直接复制页面短链接。新增空间弹出框的空间搜索功能。Wiki新增配置邮件服务器，能够使用邮件通知和邮件分享功能。新增Wiki的logo和favicon可以根据Choerodon平台的设置同步设置的功能。新增Choerodon平台分配平台管理员时，Wiki同步分配系统管理员的功能。新增了Choerodon平台的组织和项目首页的Wiki空间组件。新增集群管理模块，支持对Kubernetes集群的创建、编辑以及权限分配。新增环境的权限分配功能，支持为各个环境配置特定的操作人员。新增删除环境的功能，支持在环境停用区对环境进行删除操作。新增开发流水线代码仓库中查看代码质量的入口。Dashboard页面新增快速查看分支情况、代码提交情况、应用构建情况与部署情况的模块，并提供了快速跳转至相应模块的入口。实例部分新增deployments层,且支持一个chart文件中存在多个deployments。新增实例重新部署的功能。新增报表中部署失败的错误信息。测试用例新增使用模板Excel导入功能。测试用例新增导出功能。测试执行详情新增翻页功能。新增客户端角色分配，平台管理员、组织管理员和项目管理员可以为客户端分配操作权限。新增消息接收设置，用户可以设置接收消息的类别。新增仪表盘启停用功能，平台管理员可以启停用自己的仪表盘。新增组织层、项目层任务调度。LDAP同步添加超时强制停止。修改Wiki页面复制或移动成功之后直接跳转到目标页。修改Wiki页面删除成功之后跳转到其父页面。Wiki管理菜单现在提到了组织和项目的顶层，并增加了Wiki空间菜单。修改了搜索的弹出框和搜索页面的样式。修改了404、403的页面样式。修改了所有更新、热门、最近工作、最近访问、所有空间、最近空间为异步加载，提高页面加载性能。修改了用户信息页样式。页面的编辑等功能按钮移动到了靠左的位置。问题管理新增字段展示、字段搜索、字段排序，支持自定义筛选。活跃冲刺中的问题拖到其他位置，问题及其子任务全部还原到状态机初始状态。活跃冲刺界面展示和问题详情表单页面优化。产品全局图标优化。待办事项史诗计数详情优化。问题链接列表显示经办人信息。迭代速度图未开启的冲刺不统计。故事地图中移除问题添加验证。优化待办事项创建问题请求。0.10版本中，当在同一个集群上初始化多个环境时，需要向集群安装多个环境Agent应用，升级时需要针对每一个Agent在同一个集群执行升级脚本，维护成本较高；升级成集群客户端之后，在同一个集群创建环境时将不再需要执行环境客户端安装脚本,一键即可以创建环境、停用、删除环境。针对环境维度的操作不需要再去集群中执行相应脚本。重新整理优化开发流水线结构，统一以应用为中心进行操作。重新整理优化部署流水线结构，统一以环境为中心进行操作。优化了应用市场导出文件的命名，支持自定义命名。优化统一了平台各个空界面。优化了删除操作提示框，明确指出了删除对象名称。完善了平台指导文案，加强初级用户的理解。优化了删除实例后，关联网络列表中的目标对象内容。优化了实例升级失败或新建失败后，列表中版本的显示问题。优化了环境总览界面顶部创建操作按钮的显示。优化了部署总览界面快速部署的图标显示。测试执行导出改为异步修改，增加进度条。配合敏捷服务修改部分接口。测试用例文件夹复制和移动现在可进行批量操作。测试阶段文件夹查看增加版本显示。创建阶段有默认时间。测试步骤可拖动滚动。测试执行页面隐藏空循环。测试摘要按版本显示从新到旧排序。测试用例倒序排列。将测试执行和测试计划侧边展开状态保存。创建测试步骤不弹出新建页，在表格中插入新行进行编辑。测试步骤复制图标改为按钮。测试计划、测试执行表格的样式调整。测试计划中克隆测试阶段可以跨循环、版本。仪表盘配置优化为可在界面上控制哪些角色可见。邮件模板创建时优化为可添加网络图片，并且支持HTML编码。API 测试修改为内部接口不能在页面进行测试。修复Wiki文章的内容块区域互相遮挡的问题。修复Wiki创建页面在没有填写标题的情况下也能创建成功的问题。修复Wiki编辑器添加的issue宏，url中没有项目名的问题。修复Wiki的通知信息，用户没办法删除的问题。修复创建页面的模板描述太长的问题。修复创建页面树状浏览器选择出现不应该出现的页面的问题。修复创建页面树状浏览器选择没办法选择到组织的问题。修复问题管理中工作日志时间登记后页面数据没有更新的问题。修复待办事项版本、史诗排序错误。修复活跃冲刺及迭代工作台剩余时间计算错误。修复活跃冲刺中同列多个状态拖动白屏的问题。修复问题转换为子任务状态颜色不正确的问题。修复发布版本跳转未解决问题列表筛选错误的问题。修复链接地址中未做转码处理导致请求重复的问题。修复燃尽图报告点击子任务进入的是父任务详情的问题。修复史诗和版本燃耗图中链接到问题管理，返回页面404的问题修复发布版本时统计未完成数量不正确的问题。修复编辑应用名称时，未分辨输入字母的大小写的问题。修复创建网络时，选择实例与选择应用的逻辑问题。修复了偶现替换实例失败的问题。修复了部署超时后无法操作的问题。修复了创建域名时未校验环境的问题。修复了创建应用失败后不能处理的问题。修复测试用例文件夹复制拖动的不滚动的问题。修复测试计划页面滚动底部的问题。修复表格编辑保存时闪现旧值的问题。修复了修改测试用例后不跳到第一页的问题。修复API测试加载缓慢的问题。修复新导入的LADP用户报错的问题。修复IE兼容性问题。移除了项目中部署管理员角色，并将其所有权限分配给项目所有者。移除了部署流水线实例管理中的部署实例与单应用视图。移除了停止实例后的升级实例与重新部署的选项。感谢以下这些朋友在社区论坛中提出反馈和意见，在此次版本更新中作出突出贡献。@happyyangyuan
希望devops持续集成服务可以自定义gitlab的ssh端口号为非默认的22@Bruce
关于环境流水线创建的疑问
创建应用状态显示失败，但代码已生成
创建环境流水线日志报错@frank
应用创建失败@bojiangzhou
实例管理一直处理中又无法删除@mxjstone
0.10版本开发流水线-代码仓库跳转地址bug@quzhongquan
敏捷管理-->问题管理-->故事-->故事详情页-被阻塞 “故障”列表，如何显示“经办人”？
敏捷管理-->问题管理-->故障，无法按“标签”搜索故障，导出故障结果中也没有“标签”字段Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请问为什么现在看这里是权限分配了没有复制指令了呢，复制指令的按钮没了，我是本地虚拟机启动的，用之前连接用的helm命令执行也不管用，一直都是未连接你好，0.11 版本的，需要在组织层激活集群http://choerodon.io/zh/docs/installation-configuration/update/0.10-to-0.11/目前环境逻辑做了修改，之前环境关联k8s会在每个ns下新建一个agent，该方式会造成较多的消耗，并且不支持k8s集群平台功能的支持。0.11.0版本之后我们在组织层引入了集群的概念，每个集群对应一个k8s平台，需要在k8s激活一个集群，环境直接关联该集群即可，这种方式可以减少资源消耗，同时我们会在后续的版本拓展k8s集群管理的功能。Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：使用docker-compose启动了zk和kafka，然后拉源代码在本地启动各个服务，刚开始都注册成功，运行没几分钟一个个都断掉了报错：
啊哈，这个帖子不能删的吗，没问题了想撤回没有问题的话，我将帖子关闭了docker-compose 可以参照下面这个，我做了修改http://choerodon.io/zh/docs/development-guide/backend/intergration/run/Choerodon平台版本: 0.10.0遇到问题的执行步骤:
部署 manager service后 pod起不来
文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/choerodon/环境信息(如:节点信息):报错日志:
原因分析:疑问:能否给一个更加完整的日志之前分配的硬件资源不满足，准备重新部署试下前端如何调用后端服务，怎么获取token，token的权限有哪些？postman 怎么获取access_token?你可以先用浏览器访问页面并登录，然后打开浏览器的开发人员工具，在Application选项卡中的Cookies里能看到access_token。
完全没有前端怎么获取jwt_token呢，后端就启动了四个服务可以用postman测试获取token吗
Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：docker-compose遇到问题时的前置条件：iam, manager数据库以用0.7.0中的脚本初始化问题描述：docker-compse运行的开发环境， 脚本如下原因分析：oauth-server的这个环境变量choerodon.default.redirect.url=http://localhost:8080配置问题？
文档里面没有配置， 我看加了也不行？疑问：http://choerodon.io/zh/docs/development-guide/backend/intergration/run/
这里提供的compose文件有问题oauth-server需要redisdocker-compose启动redis,mysql,kafka,zk嗯， 感谢您的回复，
web_server_redirect_uri这个字段的值要如何设置？
我设置了全路径 http://localhost:8080/manager/swagger-ui.html 不行？改成这样可以了
http://localhost:8080/manager/webjars/springfox-swagger-ui/o2c.html您好，我这里点击这个之后跳转报错，本地服务启动了enruka，api，gateway，ouath，manage，是还有什么服务没有启动吗：
Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：测试管理-测试计划 可以拖动滚动条跨月查看测试计划执行的操作：
1、测试管理-测试计划，制定测试计划
2、查看测试计划报错信息：
步骤2、无法跨月查看测试计划
建议：
步骤2、可以拖动滚动条跨月查看测试计划您好，谢谢您的反馈，我们会在后续迭代中设计此功能Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：“测试计划”和“测试执行” 下面的测试用例 需要增加：“优先级”字段、“优先级”筛选条件执行的操作：
1、登录猪齿鱼
2、测试管理-测试计划-测试版本-测试循环-测试阶段，查看阶段下面的测试用例
报错信息
步骤2、3，列表没有“优先级”字段，筛选条件没有“优先级”建议：步骤2、3，列表增加“优先级”字段，筛选条件增加“优先级”您好，谢谢您的反馈，这个问题我们会在后续迭代中修改。Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：测试管理-测试计划   测试用例 可以按“模块”或“阶段”批量指派给对应测试人员执行执行的操作：
1、登录猪齿鱼
2、测试管理-测试计划，批量分配测试用例给 相关测试人员报错信息(请尽量使用代码块或系统截图的形式展现)：
步骤2：无法批量指派给 相关测试人员
建议：步骤2：测试用例 可以按“模块”或“阶段”批量指派给对应测试人员执行您好，谢谢您的反馈，我们会在后续迭代中计划加入此功能。重要的操作,比如任务指派，提测，应用部署等等，触发消息发到第三方hook，举个例子钉钉项目群自定义的机器人上，让项目相关人员第一时间了解到平台项目上的动态，这会比查看站内信方便很多你好，感谢你的建议，相关的需求我们已经在规划中了Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：敏捷管理–>问题管理–>故事–>故事详情页-被阻塞  “故障”列表，如何显示“经办人”？执行的操作：
1、登录猪齿鱼平台
2、敏捷管理–>问题管理–>故事–>故事详情页
3、故事详情页-被阻塞  “故障”列表，查看各故障对应的“经办人”实际结果：步骤3    没有显示“经办人”，必须进每个故障才能查看到故障对应的经办人。期望结果：步骤3    显示各故障对应的“经办人”报错信息(请尽量使用代码块或系统截图的形式展现)：建议：期望结果：步骤3    显示各故障对应的“经办人”你好，感谢你的详细说明，这个建议我们会在下个版本中优化并发布。这个功能对我们来说很重要，多谢了你好，这个问题处理的怎么样了？你好，在0.11.0版本中已经在关联的问题列表显示经办人了。现在可以看到了Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：导入的用例在“测试用例-版本-导入”中看不到刚导入的用例执行的操作：
1、导入用例
2、在“测试用例-版本-导入”查看刚导入的用例
报错信息(请尽量使用代码块或系统截图的形式展现)：建议：步骤2可以看到刚导入的用例您好，我们会尽快修复这个问题。预计今天修好，发布后会在论坛通知您，谢谢。您好，新的 test-manager-service 0.11.1 版本中修复了这个问题，感谢您的反馈现在可以了，已经在试用了Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：导入的用例无法修改“执行人”执行的操作：
1、用例导入
2、修改“执行人”
报错信息：
步骤2 找不到修改“执行人”的地方，“执行人”字段不可编辑建议：步骤2 可以修改用例“执行人”，以便将用例分配给他人执行您好，您这个地方想修改的应该是 “已指定至” 这个字段里的指派人OKChoerodon平台版本：0.6.0运行环境：汉得内部猪齿鱼环境问题描述：你好,我们 homs项目 走ci/cd 流程的时候经常由于gitlab-runner的限制导致经常性的超时.可以不可以单独给我们配置一个gitlab-runner? 目前还是使用的choerodon-runner可以不可以单独给我们配置一个gitlab-runner?由于我们资源有限，无法为您单独配置runner,您可以选择在自己的集群或主机上搭建一个runner，如何搭建runner参考如下
https://docs.gitlab.com/ee/ci/runners/
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/gitlab-runner/想研究下liquibase的那个工具包的一些深度用法…反编译代码有点太反人类了你好，我们的代码都是放在github上的，你可以在这个地址找到，如果有好的建议的话也可以给我们提pull requestThis is the toolkit developed by Choerodon and provides some basic dependencies for use in the development process. - choerodon/choerodon-startersChoerodon平台版本：0.6.0choerodon-starter-mybatis-mapper版本：0.6.4.RELEASE运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：集成jasypt-spring-boot-starter2.0.0报错，去掉依赖就OK
你好，可以参考下


github.com/abel533/MyBatis-Spring-Boot





Issue: Bean property 'mapperHelper' is not writable or has an invalid setter method


	opened by longfeizheng
	on 2016-09-14


	closed by longfeizheng
	on 2018-01-22


NotWritablePropertyException: Invalid property 'mapperHelper' of bean class [org.mybatis.spring.mapper.MapperFactoryBean]: Bean property 'mapperHelper' is not writable or has an invalid setter method
gradle 搭建提示这个错误，哪里出问题啦？...







请确认下jasypt-spring-boot-starter和mybatis-spring-boot-starter 之间有没有冲突多谢，我看看jasypt已解决。看我们猪齿鱼这边要不要做调整
感谢建议，我们这边会做一个测试，如果没问题的话会进行修复Choerodon平台版本: 0.10.0遇到问题的执行步骤:1、一键安装成功
2、管理员账号登录，报403错误；
3、所有菜单都是403错误；
4、检查iam_permission表是空的，无任何数据疑问:1、我如何初始化这些数据？
2、如果不能初始化数据，是不是需要从新安装？你好，可以参考下面几篇帖子



admin登录后，点个人中心或者管理中的任何菜单，页面提示403无权限 Choerodon Framework


    iam_permission表为空是什么原因呢，zookeeper和kafka的配置一切正常
  






持续交付下详情进去无权限，为什么？ Choerodon Framework


[image]







部分接口不在权限列表里面 Choerodon Framework


    Choerodon平台版本：0.7.0 


运行环境(如localhost或k8s)：自主搭建 


问题描述： 


新部署了一个应用，有些接口没有出现来权限表里面，但swagger里面可以查询到。使用manager-service里面的手动刷新权限的接口也没有效果
  

我检查了一下：
1、zookeeper 和 kafka 都正常；
2、kafka里没有任何消息
我该怎么处理？先去go-register-server看下是否正确连接到kafka，并发送服务启动事件到kafka里go-register-server 日志如下：
1121 18:54:50.447147 1 client_config.go:529] Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work.
I1121 18:54:50.449434 1 controller.go:59] Setting up event handlers
I1121 18:54:50.449491 1 apps.go:32] Register eureka app APIs
I1121 18:54:50.449772 1 controller.go:92] Starting Pod controller
I1121 18:54:50.449787 1 controller.go:95] Waiting for informer caches to sync
I1121 18:54:50.450934 1 server.go:45] Started server
I1121 18:54:50.550049 1 controller.go:102] Starting workers
I1121 18:54:50.550087 1 controller.go:111] Started workers
I1121 18:56:02.138490 1 apps.go:127] Receive registry from CONFIG-SERVER
I1121 18:57:19.288389 1 apps.go:127] Receive registry from MANAGER-SERVICE
I1121 19:13:26.571356 1 apps.go:127] Receive registry from MANAGER-SERVICE
I1121 19:17:00.583274 1 apps.go:127] Receive registry from MANAGER-SERVICE这个镜像版本是否有问题，没有发送kafka消息
Choerodon平台版本: 0.1.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):
2台8H32G的腾讯云服务器报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问
执行./c7n install -c config.yml --no-timeout  就报错您是什么时候下载的./c7n这个执行文件的, 建议您重新在下载 再试一下现在又报这个错了麻烦看下这是啥问题啊？这是两个不同c7n造成数据问题，你可以执行下面命令修复一下数据Choerodon平台版本：0.10运行环境：自主搭建问题描述：

!的提示,难道手动上传外部证书也跟 cert-manager有关系吗,
failed: running kubectl: error: unable to recognize “STDIN”: no matches for certmanager.k8s.io/, Kind=Certificate刚刚又折腾了下这个外部证书导入功能,因为官方文档从没提及过cert manager,只说要有kube-lego组件,但kube-lego是不支持 certmanager.k8s.io 这个CRD的,而且社区已经放弃了这个工具,然后用下面指令补装了cert manager阿里的源,安装的版本是0.2.2(似乎有点旧)
然后分别创建certmanager.k8s.io Issuer和ClusterIssuer资源最后重新上传证书,gitops项目上,多了一个xxx.yaml文件:查看c7n agent的日志,发现Certificate资源成功部署:但接下来有错误前端界面同样感叹号提示

cert-manager 的日志非常多下面这个的信息:看信息是certificates controller找不到issuer,choerodon在gitops上创建的yaml文件中确实没有 issuerRef这个spec,难道是因为这个原因?Hi， 您可以看下这个https://blog.vinkdong.com/choerodon证书管理终端cert-manager部署和测试/你好,解决了,NB
这么重要的信息建议放到官方部署文档里去,让用户少走弯路Choerodon平台版本：0.10.0运行环境：自主搭建问题描述：
ad服务器是肯定没问题的，应为在我们公司深度使用，已经接入了许多系统。
配置如下图所示：
执行的操作：
点击“保存并测试”按钮报错信息：
iam-service的报错日志：你好，0.10.0版本确实有这个ad服务器连不上的问题，我们计划在0.11版本修复这个问题好的，谢谢你。Choerodon平台版本: 0.10.0遇到问题的执行步骤:
gitlab runner执行到push镜像到私有harbor内失败报错报错日志:部署runner时你是否指定了 harbor的 admin密码呢必须已经指定了harbor的账号密码了呢！你本地测试下直接push是否有问题CI文件中有docker login 语句吗？试了本地可以直接push的。
.gitlab-ci.yml文件内是没有docker login语句的，感觉也不应该要求显示login吧？.gitlab-ci.yml文件如下：请参照最新模板进行CI文件编写哈，需要添加docker login命令谢谢，加了docker login就可以了：
- docker login -u ${DOCKER_USER} -p ${DOCKER_PWD} ${DOCKER_REGISTRY}我是参考这份文档才踩的坑：
创建一个nginx示例建议把相关文档更新一下啦。感谢反馈Choerodon平台版本: 0.10.0操作步骤
在“应用管理”下新建应用，提示应用处于“创建中”，过一段时间之后，应用状态为“失败”报错日志:
查看事务内的日志显示如下
io.choerodon.core.exception.CommonException: error.git.clone
at io.choerodon.devops.infra.common.util.GitUtil.clone(GitUtil.java:223)
at io.choerodon.devops.app.service.impl.ApplicationServiceImpl.operationApplication(ApplicationServiceImpl.java:304)
at io.choerodon.devops.api.eventhandler.DevopsSagaHandler.createApp(DevopsSagaHandler.java:97)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.invoke(SagaMonitor.java:194)
at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.run(SagaMonitor.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)原因分析:
单独登录gitlab，可以看到gitlab上是已经创建完成了一个空的project的，但是不知道为何猪齿鱼平台会认为创建gitlab project失败。您好，创建应用时，选择的是什么模板？你好，我选择的是系统自带的“ MicroServiceFront”模板。我好似知道原因了，是我们公司内网限制访问到github，所以git克隆模板代码失败了。这个我排查了好久，可以考虑往日志里面增加一下更明确的提示。对的，因为模板放在github上面的，所以需要外网访问，后面会考虑加入日志提示可以使用“空模板”，这样就不会报错了。好的，谢谢了Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：阿里云，
遇到问题时的前置条件：问题描述：在这一台机器上安装了zookeeper和kafka，然后启动各个模块直接通过java启动jar包，但是永远都只能启动4个，启动4个了在启动1个服务之前启动的就会停掉，或者有时候kafka就直接进程没了，麻烦小哥哥告知下，是不是内存2GB不行要启动的微服务有这些：
你好，每个微服务的内存至少需要768MB，建议1G ~ 2G 之间。如果服务器内存只有2G 的话，是没法启动所有的服务的。好的，声音好听的小哥哥Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：敏捷管理–>问题管理–>故障详情，没有“回归不通过”字段
执行的操作：
1、登录猪齿鱼
2、敏捷管理–>问题管理–>故障详情
3、在 故障详情页，查看故障 各字段**实际结果：**步骤3  没有“回归不通过”字段**期望结果：**步骤3  有“回归不通过”字段      注：这对我们 质量分析、提升质量 很重要建议：期望结果：步骤3  有“回归不通过”字段      注：这对我们 质量分析、提升质量 很重要你好，这个描述属于支持用户可配置字段。目前的版本确实不支持，我们会将你的需求列入到后续的开发计划中，当前是否可以通过创建一个标签来满足。
目前敏捷中一个issue是可以有多个标签的。同时需要在 “敏捷管理-问题管理”按照“回归不通过”进行筛选，通过“标签”的话目前还没办法进行筛选嗯，明白了，可配置字段这个功能后面的版本会新增，还请等待。
感谢你的反馈！请问这个问题现在怎么样了？你好，我们现在还没有提供创建自定义字段的功能，您看是否可以先通过标签进行管理，创建一个“回归不通过”的标签。我们已在问题列表增加了对标签的搜索功能。Choerodon平台版本: 0.10.0遇到问题的执行步骤:
如题，不知道在哪里获取gitlab private token文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/choerodon-devops/您好 可以参考这个




分步部署，部署gitlab-service时参数问题 Installation management


    Choerodon平台版本: 0.10.0 


遇到问题的执行步骤: 
在安装gitlab-service时，参数 env.open.GITLAB_PRIVATETOKEN=“GEuRhgb6kG9y3prFosSb” 问题。 
请问token是怎么获取的？ 还是按照文档的内容直接设置？ 


文档地址: http://choerodon.io/zh/docs/installation-co…


好的，谢谢，找到了报错
Error executing action create on resource ‘storage_directory[/var/opt/gitlab/.ssh]’There was an error running gitlab-ctl reconfigure:
storage_directory[/var/opt/gitlab/.ssh] (gitlab::gitlab-shell line 38) had an error: Mixlib::ShellOut::ShellCommandFailed: ruby_block[directory resource: /var/opt/gitlab/.ssh] (/opt/gitlab/embedded/cookbooks/cache/cookbooks/package/resources/storage_directory.rb line 33) had an error: Mixlib::ShellOut::ShellCommandFailed: Failed asserting that ownership of “/var/opt/gitlab/.ssh” was git:git能否提供一下您nfs的服务器信息centos6.9/etc/exports文件内容
/data/nfs_data          10...*(rw,sync,no_root_squash,no_subtree_check,no_root_squash)您好 操作系统需要在 7.2以上， 6.x的nfs权限存在异常状况好的，谢谢，我更换系统后再试试，后续再反馈给你们。Choerodon平台版本: 0.10遇到问题的执行步骤:helm install c7n/wiki-service 
–set env.open.JAVA_OPTS="-Xms256m -Xmx512m" 
–set preJob.preConfig.mysql.host=c7n-mysql.c7n-system.svc 
–set preJob.preConfig.mysql.port=3306 
–set preJob.preConfig.mysql.database=manager_service 
–set preJob.preConfig.mysql.username=choerodon 
–set preJob.preConfig.mysql.password=password 
–set preJob.preConfig.datasource.url=“jdbc:mysql://c7n-mysql.c7n-system.svc:3306/manager_service?useUnicode=true&characterEncoding=utf-8&useSSL=false” 
–set preJob.preConfig.datasource.username=choerodon 
–set preJob.preConfig.datasource.password=password 
–set preJob.preInitDB.mysql.host=c7n-mysql.c7n-system.svc 
–set preJob.preInitDB.mysql.port=3306 
–set preJob.preInitDB.mysql.database=wiki_service 
–set preJob.preInitDB.mysql.username=choerodon 
–set preJob.preInitDB.mysql.password=password 
–set preJob.preInitDB.datasource.url=“jdbc:mysql://c7n-mysql.c7n-system.svc:3306/wiki_service?useUnicode=true&characterEncoding=utf-8&useSSL=false” 
–set preJob.preInitDB.datasource.username=choerodon 
–set preJob.preInitDB.datasource.password=password 
–set env.open.SPRING_DATASOURCE_URL=“jdbc:mysql://c7n-mysql.c7n-system.svc:3306/wiki_service?useUnicode=true&characterEncoding=utf-8&useSSL=false” 
–set env.open.SPRING_DATASOURCE_USERNAME=choerodon 
–set env.open.SPRING_DATASOURCE_PASSWORD=password 
–set env.open.EUREKA_CLIENT_SERVICEURL_DEFAULTZONE=“http://register-server.c7n-system:8000/eureka/” 
–set env.open.EUREKA_DEFAULT_ZONE=http://register-server.c7n-system:8000/eureka/ 
–set env.open.CHOERODON_EVENT_CONSUMER_KAFKA_BOOTSTRAP_SERVERS="kafka-0.kafka-headless.c7n-system.svc.cluster.local:9092 " 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=“kafka-0.kafka-headless.c7n-system.svc.cluster.local:9092” 
–set env.open.SPRING_KAFKA_BOOTSTRAP_SERVERS=“kafka-0.kafka-headless.c7n-system.svc.cluster.local:9092” 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES=“zookeeper-0.zookeeper-headless.c7n-system.svc.cluster.local:2181” 
–set env.open.SPRING_KAFKA_PRODUCER_VALUE_SERIALIZER=org.apache.kafka.common.serialization.ByteArraySerializer 
–set env.open.SPRING_CLOUD_CONFIG_ENABLED=true 
–set env.open.SPRING_CLOUD_CONFIG_URI=http://config-server.c7n-system:8010/ 
–set env.open.WIKI_CLIENT=xwiki 
–set env.open.WIKI_URL=http://wiki.amd.test.com 
–set env.open.WIKI_TOKEN=Choerodon 
–set env.open.WIKI_DEFAULT_GROUP=XWikiAllGroup 
–name wiki-service 
–version 0.10.1 
–namespace c7n-system文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/choerodon-wiki/环境信息(如:节点信息):NAME                                         READY     STATUS    RESTARTS   AGE
agile-service-59fb69df57-kqcmf               1/1       Running   0          1h
api-gateway-6d94f5b566-tkrss                 1/1       Running   0          2h
asgard-service-84db777df6-qn92g              1/1       Running   0          2h
c7n-mysql-64746b49cf-7dr89                   1/1       Running   0          3h
c7n-redis-5cf967b4b7-hmh2s                   1/1       Running   0          3h
chartmuseum-chartmuseum-864cb9cc5d-4thfl     1/1       Running   0          3h
config-server-799b5b6d68-h8wmk               1/1       Running   0          2h
devops-service-695d5d6bfc-qqxc8              1/1       Running   0          1h
file-service-5d767d8f84-c66b5                1/1       Running   0          1h
gateway-helper-5dd7567694-lwnjc              1/1       Running   0          2h
gitlab-58c9df65d5-w924g                      1/1       Running   0          1h
gitlab-mysql-7f9df57f66-lfghs                1/1       Running   0          2h
gitlab-redis-6c6cd6c7f-f8dhj                 1/1       Running   0          2h
gitlab-service-86d6b888db-g8q5j              1/1       Running   0          1h
harbor-harbor-adminserver-58db79cd46-fhj89   1/1       Running   1          3h
harbor-harbor-database-0                     1/1       Running   0          3h
harbor-harbor-jobservice-8458b65b4d-nlhg7    1/1       Running   1          3h
harbor-harbor-registry-5c8774b645-jxhwg      1/1       Running   0          3h
harbor-harbor-ui-59df9446f8-d7xd8            1/1       Running   2          3h
harbor-redis-master-0                        1/1       Running   0          3h
iam-service-6db4f67659-8r5mr                 1/1       Running   0          2h
kafka-0                                      1/1       Running   0          3h
manager-service-6874c4ffb7-fb7dm             1/1       Running   0          2h
minio-c4c85b847-mwp9p                        1/1       Running   0          3h
nfs-provisioner-6657665d4b-hlzb6             1/1       Running   0          3h
notify-service-7486454694-bvbx2              1/1       Running   0          2h
oauth-server-6d7dc4c6-qwbqk                  1/1       Running   0          1h
register-server-7c64c89d67-2gfr7             1/1       Running   0          2h
test-manager-service-d948bbff8-vvddz         1/1       Running   0          42m
wiki-service-6495c8bbc5-x458d                0/1       Pending   0          15m
xwiki-549fb8d884-sxtrh                       1/1       Running   0          36m
zookeeper-0                                  1/1       Running   0          3h2018-11-16 11:31:11.069  INFO [-,] 7 — [           main] i.c.config.ConfigToolApplication         : Starting ConfigToolApplication v0.6.3.RELEASE on wiki-service-init-config-b5qm2 with PID 7 (/var/choerodon/choerodon-tool-config.jar started by root in /)
2018-11-16 11:31:11.075  INFO [-,] 7 — [           main] i.c.config.ConfigToolApplication         : No active profile set, falling back to default profiles: default
2018-11-16 11:31:11.164  INFO [-,] 7 — [           main] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@621be5d1: startup date [Fri Nov 16 11:31:11 CST 2018]; root of context hierarchy
2018-11-16 11:31:12.092  WARN [-,] 7 — [           main] o.m.s.mapper.ClassPathMapperScanner      : No MyBatis mapper was found in ‘[io.choerodon.config]’ package. Please check your configuration.
2018-11-16 11:31:12.844  WARN [-,] 7 — [           main] o.s.c.a.ConfigurationClassPostProcessor  : Cannot enhance @Configuration bean definition ‘io.choerodon.mybatis.MybatisMapperAutoConfiguration’ since its singleton instance has been created too early. The typical cause is a non-static @Bean method with a BeanDefinitionRegistryPostProcessor return type: Consider declaring such methods as ‘static’.
2018-11-16 11:31:13.365  INFO [-,] 7 — [           main] trationDelegate$BeanPostProcessorChecker : Bean ‘org.springframework.cloud.sleuth.instrument.async.AsyncDefaultAutoConfiguration$DefaultAsyncConfigurerSupport’ of type [org.springframework.cloud.sleuth.instrument.async.AsyncDefaultAutoConfiguration$DefaultAsyncConfigurerSupport$$EnhancerBySpringCGLIB$$cc344d84] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-11-16 11:31:13.458  INFO [-,] 7 — [           main] trationDelegate$BeanPostProcessorChecker : Bean ‘org.springframework.cloud.sleuth.annotation.SleuthAnnotationAutoConfiguration’ of type [org.springframework.cloud.sleuth.annotation.SleuthAnnotationAutoConfiguration$$EnhancerBySpringCGLIB$$9157f5f3] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-11-16 11:31:13.679  INFO [-,] 7 — [           main] trationDelegate$BeanPostProcessorChecker : Bean ‘sleuthAdvisorConfig’ of type [org.springframework.cloud.sleuth.annotation.SleuthAdvisorConfig] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-11-16 11:31:13.689  INFO [-,] 7 — [           main] trationDelegate$BeanPostProcessorChecker : Bean ‘org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration’ of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$8ef2afc1] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-11-16 11:31:13.754  INFO [-,] 7 — [           main] trationDelegate$BeanPostProcessorChecker : Bean ‘org.springframework.cloud.sleuth.instrument.async.AsyncDefaultAutoConfiguration’ of type [org.springframework.cloud.sleuth.instrument.async.AsyncDefaultAutoConfiguration$$EnhancerBySpringCGLIB$$76d65be0] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-11-16 11:31:13.778  INFO [-,] 7 — [           main] trationDelegate$BeanPostProcessorChecker : Bean ‘org.springframework.cloud.sleuth.instrument.web.client.TraceWebClientAutoConfiguration$TraceOAuthConfiguration’ of type [org.springframework.cloud.sleuth.instrument.web.client.TraceWebClientAutoConfiguration$TraceOAuthConfiguration$$EnhancerBySpringCGLIB$$d3ef4220] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
2018-11-16 11:31:17.980  INFO [-,] 7 — [           main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService
2018-11-16 11:31:18.321  INFO [-,] 7 — [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
2018-11-16 11:31:18.456  INFO [-,] 7 — [           main] io.choerodon.config.utils.FileUtil       : jar拆解
2018-11-16 11:31:18.663  INFO [-,] 7 — [           main] io.choerodon.config.utils.FileUtil       : jar拆解完成
2018-11-16 11:31:18.664  INFO [-,] 7 — [           main] io.choerodon.config.ConfigToolExecute    : 根据指定文件进行配置初始化: wiki-service
2018-11-16 11:31:18.683  INFO [-,] 7 — [           main] io.choerodon.config.utils.FileUtil       : 文件路径获取:temp/BOOT-INF/classes/application.yml
2018-11-16 11:31:18.798  WARN [-,] 7 — [           main] io.choerodon.mybatis.helper.AuditHelper  : principal not instanceof CustomUserDetails audit user is 0L
2018-11-16 11:31:18.806  INFO [-,] 7 — [       Thread-5] s.c.a.AnnotationConfigApplicationContext : Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@621be5d1: startup date [Fri Nov 16 11:31:11 CST 2018]; root of context hierarchy
2018-11-16 11:31:18.808  INFO [-,] 7 — [       Thread-5] o.s.j.e.a.AnnotationMBeanExporter        : Unregistering JMX-exposed beans on shutdown提出您分析问题的过程,以便我们能更准确的找到问题所在
这里本来是怀疑官方给的wiki-service安装命令的问题，保留了官方的参数，同时按照test-manager-service的配置方法，给了preJob.preConfig.datasource 和 preJob.preInitDb.datasource定义，还是同样的问题。
问题是出在wiki-service-init-config这个Job上的，从而导致后面的初始化失败。
有没有可能发布的包不对，请校验，谢谢重新看了chart里的pre-config-config.yaml，
java -Dspring.datasource.url=“jdbc:mysql://{{ .Values.preJob.preConfig.mysql.host }}:{{ .Values.preJob.preConfig.mysql.port }}/{{ .Values.preJob.preConfig.mysql.database }}?useUnicode=true&characterEncoding=utf-8&useSSL=false” -Dspring.datasource.username={{ .Values.preJob.preConfig.mysql.username }} -Dspring.datasource.password={{ .Values.preJob.preConfig.mysql.password }} -Dservice.name={{ .Chart.Name }} -Dservice.version={{ .Chart.Version }} -Dconfig.file={{ .Values.preJob.preConfig.configFile }} -Dconfig.jar=/{{ .Chart.Name }}.jar -jar /var/choerodon/choerodon-tool-config.jar;首先确定的是官方给的命令基本没啥问题，但是少了preJob.preConfig.configFile的申明，不知道是不是这个造成的提出您对于遇到和解决该问题时的疑问您好，我们的Chart Values 里面是指定配置文件的，就不需要声明了。你这个初始化配置的过程是怎么看出来失败的呢，后面启动wiki-service的时候没办法拉到配置吗？按照官方文档搭建集群后，fluent-bit-choerodon-logging和choerodon-logging两类pod一直处于CrashLoopBackOff和Error状态，请问可以怎么处理？可以先看一下 choerodon-logging 的日志panic: Get https://10.96.0.1:443/api/v1/namespaces/logging/configmaps/fluent-bit-conf-choerodon-logging: dial tcp 10.96.0.1:443: i/o timeout连接不上您的 k8s api请问 你是使用我们文档中的方法搭建的 k8s吗其他三个节点 Error syncing pod我们是用我们原有的k8s集群搭建日志监控，版本都是可以的，请问api访问可以怎么处理？目前是 choerodon-logging无法连接到您的 k8s api server建议检查一下 网络组件是否存在问题 或者 网络组件是否对应用隔离策略helm install c7n/kibana \请问这条命令我怎么看到kibana的前端页面？自定义的域名访问不了Choerodon平台版本：0.10.0运行环境：SaaS问题描述：部署流水线中的域名、网络、实例、容器、证书，可否加上一层菜单，统一成“应用配置”或者“应用资源”。按照文档上的方式安装pvc后，pvc一直处于Pending状态，消息为waiting for a volume to be created, either by external provisioner “choerodon.io/nfs-client-provisioner” or manually created by system administrator，请问可以怎么处理？请直接执行下面命令  提供一下nfs-client-provisioner pod的日志已经可以了，确认nfs-provisioner的pod已经Running了，但是pvc报
failed to provision volume with StorageClass “nfs-provisioner”你这两个都执行了的吗？有的，我没有nfs服务，执行完后已经能在pod看到nfs-provisioner处于Running状态这些日志是从nfs-provisioner这台服务器打印的你有NFS服务器吗？没有的，所以我用第一个，nfs-provisioner处于Running状态这个你有没有执行？有的，已经起来了，但是pvc启动报了异常
我没有nfs服务，跑的是第一个，第二个没跑跑的第一个就不可能报出这个错误来哈这个是之前用错了，已经没有了，现在是下面那个日志问题这个目录在指定节点手动创建了吗？我把这个目录改成我自己的磁盘目录了，目录手动建的helm install c7n/nfs-provisioner \persistence.nodeName 可以是master在get node时的名称吗你把这个pending的 pvc删除掉重新创建试一试    现在看nfs-provisioner是没有问题的Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
进行数据初始化的时候报错，但数据初始化进去了。
报错信息(请尽量使用代码块的形式展现)：初始化的这个jar有git地址没，想看下具体是哪里报错了。你好，能提供下liquibase 工具包的版本吗？源码库可以在github上找到



GitHub



choerodon/choerodon-starters
This is the toolkit developed by Choerodon and provides some basic dependencies for use in the development process. - choerodon/choerodon-starters





This is the toolkit developed by Choerodon and provides some basic dependencies for use in the development process. - choerodon/choerodon-startersjar 包可以在中央仓库找到https://oss.sonatype.org/content/groups/public/io/choerodon/choerodon-tool-liquibase/好的，非常感谢。类似如下curl命令
curl $(kubectl get svc choerodon-front -o jsonpath="{.spec.clusterIP}" -n c7n-system)这条命令就是访问k8s集群的clusterip嘛，但是我的pc端根本访问不了k8s集群的内部网络呀！分布安装命令都是在master节点执行哦请问一下在master节点也是本地本地网络，该如何做到可以直接与k8s集群的网络通信的？如果您使用我们脚本安装k8s是互通的呢原来是这样子啊，我是使用rancher部署的k8s集群，难怪。Choerodon平台版本: 0.10.0遇到问题的执行步骤:
官方文档给出的前提条件：至少一个可以通过公网访问22端口的服务器（该22端口不能被其他服务占用，安装Gitlab使用SSH协议会用到它）
但是我们的服务器一般22端口是给服务器本机的sshd用的，而且我用的k8s的ingress只支持7层的80和443端口，不支持4层自定义端口号。文档地址:
http://choerodon.io/zh/docs/installation-configuration/pre-install/ssh使用tcp协议 你需要吧gitlab所属域名指定的服务器sshd端口修改为其它端口 在将22端口绑定gitlab好的，谢谢我们会在0.11版本支持非22端口,欢迎继续关注好的，加油Choerodon平台版本: 0.10.0遇到问题的执行步骤:
执行helm install c7n/xwiki … 命令文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/choerodon-wiki/报错日志:
无报错原因分析:
xwiki-init-config的job显示是创建并运行状态，但是不资道为什么helm install命令没有继续往下执行怀疑是不是因为我第一次helm install c7n/xwiki 失败，然后残留了一些东西，二次安装虽然没有提示报错，但是会受到影响一直卡住？您好 initjob完成之后 helm才会继续执行 请耐心等待job完成我试着手动把那个xwiki-init-config job删掉，然后helm install命令才能够继续往下执行了。不是啊，是卡住20多分钟了还是不行啊！job执行时间取决于您的硬件配置 你可以查看该pod的日子您不能直接删除这个job，未正确初始化的数据可能导致使用过程中出现问题是我配置错误，导致job卡住。但是pod日志上为有任何提示，所以浪费了我不少时间。能否告知一下 你配置了什么导致job卡住。我们优化一下日志我按照官网文档上的说明安装时，在安装nfs-provider发现所有的helm读取命令比如list、install等都会报
dial tcp 10.96.0.1:443: i/o timeout，请问可以怎么解决？有按照文档安装 helm吗有的，拷贝命令装的请执行helm version命令   是否出现卡死情况，若有请执行下面命令重启tiller非常感谢Choerodon平台版本: 0.10.0遇到问题的执行步骤: 一键安装文档地址: http://choerodon.io/zh/docs/installation-configuration/steps/install/choerodon/环境信息(如:节点信息):报错日志:原因分析:如果您有自己的nfs服务器并且支持v4版本，可以注释storageClassName并配置您的的nfs,某些NFS服务器可能存在程序无法获取到正确权限的问题
请问：1、我有单独的NFS服务器，config.yml 应该怎么配置?请问是否有做这一步操作http://choerodon.io/zh/docs/installation-configuration/steps/nfs/执行了helm install c7n/nfs-client-provisioner 
–set rbac.create=true 
–set persistence.enabled=true 
–set storageClass.name=nfs-provisioner 
–set persistence.nfsServer=nfs.xxx.cn 
–set persistence.nfsPath=/data 
–version 0.1.0 
–name nfs-client-provisioner 
–namespace devops执行正常那就只需要把这写好就是，注释掉的地方就注释掉不用再把nfs服务器信息配置上。1、已经修改config.yml，只配置了 storageClassName: nfs-provisioner；
2、启动了slaver，每个node都创建pod；
出现错误：挂载超时
Unable to mount volumes for pod “c7n-slaver-frvwt_devops(e9665095-e8d5-11e8-bc22-001a4a160154)”: timeout expired waiting for volumes to attach/mount for pod “devops”/“c7n-slaver-frvwt”. list of unattached/unmounted volumes=[data]建议你使用helm命令删除 除了nfs-client-provisioner的  其他releases
然后删除c7n-system命名空间下的所有cm后再进行安装具体命令如下昨天安装完成后，一切正常，都可以访问的遇到问题的执行步骤:
昨天安装完成后，使用正常，今天发现GIT报错，报错内容如下
==> /var/log/gitlab/nginx/gitlab_error.log <==
2018/11/13 09:30:48 [crit] 835#0: *13085 connect() to unix:/var/opt/gitlab/gitlab-workhorse/socket failed (22: Invalid argument) while connecting to upstream, client: 10.233P/1.1", upstream: “http://unix:/var/opt/gitlab/gitlab-workhorse/socket:/help”, host: “10.233.67.9:80”文档地址:环境信息(如:节点信息):报错日志:登录API报如下错误This application has no explicit mapping for /error, so you are seeing this as a fallback.Tue Nov 13 09:32:20 CST 2018There was an unexpected error (type=Internal Server Error, status=500).Request processing failed; nested exception is org.springframework.transaction.TransactionSystemException: Could not commit JDBC transaction; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: Communications link failure during commit(). Transaction resolution unknown.原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请查看Mysql的pod是否正常只有GIT有问题帮忙再看一下啊，感谢感谢！！请问你是通过什么方式进行安装的呢？我用的是一键安装的方式，安装完成后访问一切正常，第二天就出现gitlab无法访问，C7N无法打开的情况，我看了几个重启过的POD的日志，麻烦您帮助看看
[root@node1 ~]# kubectl logs harbor-harbor-adminserver-6469fd5789-k6qm2 -n c7n-system
2018-11-12T07:41:52Z [INFO] initializing system configurations…
2018-11-12T07:41:53Z [INFO] Upgrading schema for pgsql …
2018-11-12T07:41:55Z [INFO] Registering database: type-PostgreSQL host-harbor-harbor-database port-5432 databse-registry sslmode-“disable”
2018-11-12T07:41:55Z [INFO] Register database completed
2018-11-12T07:41:55Z [INFO] the path of json configuration storage: /etc/adminserver/config/config.json
2018-11-12T07:41:55Z [INFO] the path of key used by key provider: /etc/adminserver/key
2018-11-12T07:41:56Z [INFO] system initialization completed
10.233.66.0 - - [12/Nov/2018:07:42:05 +0000] “GET /api/configurations HTTP/1.1” 200 1717
10.233.66.0 - - [12/Nov/2018:07:42:06 +0000] “GET /api/configurations HTTP/1.1” 200 1717[root@node1 ~]# kubectl logs harbor-harbor-jobservice-55d4489867-vdxb6 -n c7n-system
2018-11-12T07:41:14Z [ERROR] [service_logger.go:63]: Job context initialization error: Get http://harbor-harbor-adminserver/api/configurations: dial tcp 10.233.18.151:80: i/o timeout
2018-11-12T07:41:14Z [INFO] Retry in 9 seconds
2018-11-12T07:41:53Z [ERROR] [service_logger.go:63]: Job context initialization error: Get http://harbor-harbor-adminserver/api/configurations: dial tcp 10.233.18.151:80: i/o timeout
2018-11-12T07:41:53Z [INFO] Retry in 13 seconds
2018-11-12T07:42:06Z [INFO] Registering database: type-PostgreSQL host-harbor-harbor-database port-5432 databse-registry sslmode-“disable”
2018-11-12T07:42:06Z [INFO] Register database completed
2018-11-12T07:42:06Z [INFO] Server is started at :8080 with http
2018-11-12T07:42:06Z [INFO] Logger sweeper is started
2018-11-12T07:42:06Z [INFO] OP commands sweeper is started
2018-11-12T07:42:06Z [INFO] Redis job stats manager is started
2018-11-12T07:42:06Z [INFO] Start to clear the job outdated log files
2018-11-12T07:42:06Z [INFO] 0 job outdated log files cleared
2018-11-12T07:42:06Z [INFO] Message server is started
2018-11-12T07:42:06Z [INFO] Subscribe redis channel {harbor_job_service_namespace}:period:policies:notifications
2018-11-12T07:42:06Z [INFO] Load 0 periodic job policies
2018-11-12T07:42:06Z [INFO] Periodic enqueuer is started
2018-11-12T07:42:06Z [INFO] Redis scheduler is started
2018-11-12T07:42:06Z [INFO] Redis worker pool is started
2018-11-12T13:12:16Z [INFO] Message server is stopped
2018-11-12T13:12:16Z [ERROR] [service_logger.go:63]: Message server exits with error: error occurred when receiving from pub/sub channel of message server: read tcp 10.233.66.8:58416->10.233.13.89:6379: use of closed network connection
2018-11-12T13:12:23Z [INFO] Restart message server (1 times)
ERROR: worker.fetch - read tcp 10.233.66.8:58412->10.233.13.89:6379: i/o timeout
ERROR: worker.fetch - read tcp 10.233.66.8:58418->10.233.13.89:6379: i/o timeout
ERROR: requeuer.process - read tcp 10.233.66.8:58414->10.233.13.89:6379: i/o timeout
ERROR: requeuer.process - read tcp 10.233.66.8:58420->10.233.13.89:6379: i/o timeout
2018-11-12T13:12:50Z [INFO] Message server is started
2018-11-12T13:12:50Z [INFO] Subscribe redis channel {harbor_job_service_namespace}:period:policies:notifications[root@node1 ~]# kubectl logs harbor-harbor-ui-59df9446f8-lnqwz -n c7n-system
2018-11-12T07:42:06Z [DEBUG] [init.go:37]: topic StartReplication is subscribed
2018-11-12T07:42:06Z [DEBUG] [init.go:37]: topic OnPush is subscribed
2018-11-12T07:42:06Z [DEBUG] [init.go:37]: topic OnDeletion is subscribed
2018-11-12T07:42:06Z [DEBUG] [authenticator.go:126]: Registered authencation helper for auth mode: db_auth
2018-11-12T07:42:06Z [DEBUG] [authenticator.go:126]: Registered authencation helper for auth mode: ldap_auth
2018-11-12T07:42:06Z [DEBUG] [authenticator.go:126]: Registered authencation helper for auth mode: uaa_auth
2018-11-12T07:42:06Z [INFO] Config path: /etc/ui/app.conf
2018-11-12T07:42:06Z [INFO] initializing configurations…
2018-11-12T07:42:06Z [INFO] key path: /etc/ui/key
2018-11-12T07:42:06Z [INFO] initializing client for adminserver http://harbor-harbor-adminserver …
2018-11-12T07:42:06Z [INFO] initializing the project manager based on local database…
2018-11-12T07:42:06Z [INFO] configurations initialization completed
2018-11-12T07:42:06Z [INFO] Registering database: type-PostgreSQL host-harbor-harbor-database port-5432 databse-registry sslmode-“disable”
2018-11-12T07:42:06Z [INFO] Register database completed
2018-11-12T07:42:06Z [INFO] User id: 1 updated its encypted password successfully.
2018-11-12T07:42:06Z [INFO] Policy scheduler start at 2018-11-12T07:42:06Z
2018-11-12T07:42:06Z [ERROR] [main.go:162]: Failed to parse SYNC_REGISTRY: strconv.ParseBool: parsing “”: invalid syntax
2018-11-12T07:42:06Z [INFO] Because SYNC_REGISTRY set false , no need to sync registry
2018-11-12T07:42:06Z [INFO] Init proxy
2018/11/12 07:42:06 [I] [asm_amd64.s:2337] http server Running on http://:8080root@node1 ~]# kubectl logs mysql-7df6dc764d-7r5gd -n c7n-system
2018-11-13T01:32:23.096091Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2018-11-13T01:32:23.105751Z 0 [Note] mysqld (mysqld 5.7.22) starting as process 1 …
2018-11-13T01:32:23.124153Z 0 [Note] InnoDB: PUNCH HOLE support available
2018-11-13T01:32:23.124185Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2018-11-13T01:32:23.124189Z 0 [Note] InnoDB: Uses event mutexes
2018-11-13T01:32:23.124193Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
2018-11-13T01:32:23.124196Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.3
2018-11-13T01:32:23.124199Z 0 [Note] InnoDB: Using Linux native AIO
2018-11-13T01:32:23.124783Z 0 [Note] InnoDB: Number of pools: 1
2018-11-13T01:32:23.124934Z 0 [Note] InnoDB: Using CPU crc32 instructions
2018-11-13T01:32:23.127243Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
2018-11-13T01:32:23.138957Z 0 [Note] InnoDB: Completed initialization of buffer pool
2018-11-13T01:32:23.142222Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
2018-11-13T01:32:23.270212Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
2018-11-13T01:32:23.286670Z 0 [Note] InnoDB: Log scan progressed past the checkpoint lsn 39140914
2018-11-13T01:32:23.286700Z 0 [Note] InnoDB: Doing recovery: scanned up to log sequence number 39140923
2018-11-13T01:32:23.286708Z 0 [Note] InnoDB: Database was not shutdown normally!
2018-11-13T01:32:23.286713Z 0 [Note] InnoDB: Starting crash recovery.
2018-11-13T01:32:25.238504Z 0 [Note] InnoDB: Removed temporary tablespace data file: “ibtmp1”
2018-11-13T01:32:25.238550Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2018-11-13T01:32:25.241543Z 0 [Note] InnoDB: Setting file ‘./ibtmp1’ size to 12 MB. Physically writing the file full; Please wait …
2018-11-13T01:32:25.501702Z 0 [Note] InnoDB: File ‘./ibtmp1’ size is now 12 MB.
2018-11-13T01:32:25.509043Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
2018-11-13T01:32:25.509073Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
2018-11-13T01:32:25.511051Z 0 [Note] InnoDB: Waiting for purge to start
2018-11-13T01:32:25.561276Z 0 [Note] InnoDB: 5.7.22 started; log sequence number 39140923
2018-11-13T01:32:25.562969Z 0 [Note] Plugin ‘FEDERATED’ is disabled.
2018-11-13T01:32:25.563147Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
2018-11-13T01:32:25.719502Z 0 [Note] InnoDB: Buffer pool(s) load completed at 181113  9:32:25
2018-11-13T01:32:25.731535Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
2018-11-13T01:32:25.739050Z 0 [Warning] CA certificate ca.pem is self signed.
2018-11-13T01:32:25.742048Z 0 [Note] Server hostname (bind-address): ‘*’; port: 3306
2018-11-13T01:32:25.744214Z 0 [Note] IPv6 is not available.
2018-11-13T01:32:25.744247Z 0 [Note]   - ‘0.0.0.0’ resolves to ‘0.0.0.0’;
2018-11-13T01:32:25.744288Z 0 [Note] Server socket created on IP: ‘0.0.0.0’.
2018-11-13T01:32:25.752126Z 0 [Warning] Insecure configuration for --pid-file: Location ‘/var/run/mysqld’ in the path is accessible to all OS users. Consider choosing a different directory.
2018-11-13T01:32:25.820137Z 0 [Warning] ‘user’ entry ‘root@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:25.820214Z 0 [Warning] ‘user’ entry ‘mysql.session@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:25.820231Z 0 [Warning] ‘user’ entry ‘mysql.sys@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:25.821162Z 0 [Warning] ‘db’ entry ‘performance_schema mysql.session@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:25.821186Z 0 [Warning] ‘db’ entry ‘sys mysql.sys@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:25.821721Z 0 [Warning] ‘proxies_priv’ entry ‘@ root@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:25.896832Z 0 [Warning] ‘tables_priv’ entry ‘user mysql.session@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:25.896876Z 0 [Warning] ‘tables_priv’ entry ‘sys_config mysql.sys@localhost’ ignored in --skip-name-resolve mode.
2018-11-13T01:32:26.305038Z 0 [Note] Event Scheduler: Loaded 0 events
2018-11-13T01:32:26.305504Z 0 [Note] mysqld: ready for connections.
Version: ‘5.7.22’  socket: ‘/var/run/mysqld/mysqld.sock’  port: 3306  MySQL Community Server (GPL)
2018-11-13T02:31:08.467293Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 7784ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)
2018-11-13T02:47:59.885743Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 5900ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)
2018-11-13T03:04:18.262290Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 8714ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)2018-11-13T09:32:26.480109Z 4 [Note] Aborted connection 4 to db: ‘asgard_service’ user: ‘choerodon’ host: ‘10.233.64.0’ (Got timeout reading communication packets)
2018-11-13T09:51:16.150190Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 6394ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)
2018-11-13T10:02:07.500194Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 4548ms. The settings might not be optimal. (flushed=0 and evicted=0, during the time.)
2018-11-13T10:04:34.802214Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 9627ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)
2018-11-13T10:04:42.872862Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 7070ms. The settings might not be optimal. (flushed=0 and evicted=0, during the time.)
2018-11-13T09:32:26.480109Z 4 [Note] Aborted connection 4 to db: ‘asgard_service’ user: ‘choerodon’ host: ‘10.233.64.0’ (Got timeout reading communication packets)
2018-11-13T13:48:51.461836Z 7 [Note] Aborted connection 7 to db: ‘asgard_service’ user: ‘choerodon’ host: ‘10.233.64.0’ (Got timeout reading communication packets)
2018-11-14T07:59:53.047309Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 14053ms. The settings might not be optimal. (flushed=0 and evicted=0, during the time.)
2018-11-14T08:00:01.254120Z 8104 [Note] Aborted connection 8104 to db: ‘xwiki’ user: ‘choerodon’ host: ‘10.233.67.1’ (Got timeout reading communication packets)
2018-11-14T08:28:24.142173Z 16 [Note] Aborted connection 16 to db: ‘manager_service’ user: ‘choerodon’ host: ‘10.233.65.0’ (Got timeout reading communication packets)
2018-11-14T20:28:33.776146Z 3 [Note] Aborted connection 3 to db: ‘asgard_service’ user: ‘choerodon’ host: ‘10.233.64.0’ (Got timeout reading communication packets)
2018-11-14T21:00:54.498867Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 4919ms. The settings might not be optimal. (flushed=0 and evicted=0, during the time.)
2018-11-14T21:01:26.176594Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 6675ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)
2018-11-14T21:41:52.995537Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 4092ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)
2018-11-14T21:42:03.390162Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 6394ms. The settings might not be optimal. (flushed=3 and evicted=0, during the time.)
2018-11-14T21:49:11.933336Z 12193 [Note] Aborted connection 12193 to db: ‘manager_service’ user: ‘choerodon’ host: ‘10.233.65.0’ (Got timeout reading communication packets)root@node1 ~]# kubectl logs nfs-provisioner-6767bdd874-r4q2p -n c7n-system
I1112 13:11:19.846455       1 main.go:63] Provisioner choerodon.io/nfs-provisioner specified
I1112 13:11:19.846544       1 main.go:87] Setting up NFS server!
I1112 13:11:20.025408       1 server.go:144] starting RLIMIT_NOFILE rlimit.Cur 65536, rlimit.Max 65536
I1112 13:11:20.025451       1 server.go:155] ending RLIMIT_NOFILE rlimit.Cur 1048576, rlimit.Max 1048576
I1112 13:11:20.026074       1 server.go:129] Running NFS server!
I1112 13:11:25.079928       1 leaderelection.go:185] attempting to acquire leader lease  c7n-system/choerodon.io-nfs-provisioner…
I1112 13:11:42.495277       1 leaderelection.go:194] successfully acquired lease c7n-system/choerodon.io-nfs-provisioner
I1112 13:11:42.495388       1 controller.go:631] Starting provisioner controller choerodon.io/nfs-provisioner_nfs-provisioner-6767bdd874-r4q2p_7529a3ee-e67c-11e8-9e64-0a580ae94005!
I1112 13:11:42.495381       1 event.go:221] Event(v1.ObjectReference{Kind:“Endpoints”, Namespace:“c7n-system”, Name:“choerodon.io-nfs-provisioner”, UID:“53db89a8-e62d-11e8-b387-02cdc66c8a33”, APIVersion:“v1”, ResourceVersion:“81949”, FieldPath:""}): type: ‘Normal’ reason: ‘LeaderElection’ nfs-provisioner-6767bdd874-r4q2p_7529a3ee-e67c-11e8-9e64-0a580ae94005 became leader
I1112 13:11:42.595683       1 controller.go:680] Started provisioner controller choerodon.io/nfs-provisioner_nfs-provisioner-6767bdd874-r4q2p_7529a3ee-e67c-11e8-9e64-0a580ae94005!
E1112 16:12:16.789670       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out, possibly due to previous leader failure
E1113 03:40:13.275179       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1113 06:15:57.463731       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out, possibly due to previous leader failure
E1113 11:21:44.356621       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out, possibly due to previous leader failure
E1114 02:43:41.292293       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1114 02:53:28.952884       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out, possibly due to previous leader failure
E1114 03:00:56.624165       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1114 03:40:06.685927       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1114 04:46:36.640681       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1114 08:42:20.748921       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1114 11:22:13.879954       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out, possibly due to previous leader failure
E1114 19:50:45.632462       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1114 23:00:52.873317       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out, possibly due to previous leader failure
[root@node1 ~]# kubectl logs register-server-74dcd77fd9-45h2t -n c7n-system
W1115 00:34:19.426514       1 client_config.go:529] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I1115 00:34:19.428530       1 controller.go:60] Setting up event handlers
I1115 00:34:19.428594       1 apps.go:32] Register eureka app APIs
I1115 00:34:19.430600       1 server.go:45] Started server
I1115 00:34:19.432702       1 controller.go:93] Starting Pod controller
I1115 00:34:19.432720       1 controller.go:96] Waiting for informer caches to sync
I1115 00:34:19.440670       1 leaderelection.go:174] attempting to acquire leader lease…
I1115 00:34:19.532864       1 controller.go:106] Starting workers
I1115 00:34:19.532915       1 controller.go:115] Started workers
I1115 00:34:19.533154       1 repository.go:62] Delete instance by key c7n-system/register-server-74dcd77fd9-45h2t not exist
E1115 00:34:37.675265       1 event.go:260] Could not construct reference to: ‘&v1.Endpoints{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:“register-server”, GenerateName:"", Namespace:“c7n-system”, SelfLink:"/api/v1/namespaces/c7n-system/endpoints/register-server", UID:“7e8f5ee6-e64c-11e8-9598-02d0f28c71e8”, ResourceVersion:“526139”, Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63677604485, loc:(*time.Location)(0x1baeb80)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{“choerodon.io/release":"register-server”}, Annotations:map[string]string{“control-plane.alpha.kubernetes.io/leader":"{“holderIdentity”:“register-server-74dcd77fd9-45h2t1542213259440538957”,“leaseDurationSeconds”:15,“acquireTime”:“2018-11-14T16:34:37Z”,“renewTime”:“2018-11-14T16:34:37Z”,“leaderTransitions”:8}”}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Subsets:[]v1.EndpointSubset(nil)}’ due to: ‘no kind is registered for the type v1.Endpoints’. Will not report event: ‘Normal’ ‘LeaderElection’ ‘register-server-74dcd77fd9-45h2t1542213259440538957 became leader’
I1115 00:34:37.675613       1 leaderelection.go:184] successfully acquired lease c7n-system/register-server
E1115 03:22:25.299276       1 leaderelection.go:258] Failed to update lock: etcdserver: request timed out, possibly due to previous leader failure
E1115 03:50:45.446918       1 leaderelection.go:258] Failed to update lock: etcdserver: request timed out
E1115 07:00:52.263093       1 leaderelection.go:258] Failed to update lock: etcdserver: request timed out请提供一下节点的配置（CPU 内存 磁盘情况）及数量信息==> /var/log/gitlab/unicorn/unicorn_stderr.log <==
I, [2018-11-15T21:21:56.523655 #8418]  INFO – : listening on addr=127.0.0.1:8080 fd=20
E, [2018-11-15T21:21:56.524374 #8418] ERROR – : adding listener failed addr=/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket (in use)
E, [2018-11-15T21:21:56.524429 #8418] ERROR – : retrying in 0.5 seconds (4 tries left)
E, [2018-11-15T21:21:57.025384 #8418] ERROR – : adding listener failed addr=/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket (in use)
E, [2018-11-15T21:21:57.025537 #8418] ERROR – : retrying in 0.5 seconds (3 tries left)
E, [2018-11-15T21:21:57.526404 #8418] ERROR – : adding listener failed addr=/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket (in use)
E, [2018-11-15T21:21:57.526532 #8418] ERROR – : retrying in 0.5 seconds (2 tries left)
E, [2018-11-15T21:21:58.028005 #8418] ERROR – : adding listener failed addr=/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket (in use)
E, [2018-11-15T21:21:58.028233 #8418] ERROR – : retrying in 0.5 seconds (1 tries left)
E, [2018-11-15T21:21:58.529136 #8418] ERROR – : adding listener failed addr=/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket (in use)
E, [2018-11-15T21:21:58.529280 #8418] ERROR – : retrying in 0.5 seconds (0 tries left)
E, [2018-11-15T21:21:59.030515 #8418] ERROR – : adding listener failed addr=/var/opt/gitlab/gitlab-rails/sockets/gitlab.socket (in use)
Errno::EADDRINUSE: Address already in use - connect(2) for /var/opt/gitlab/gitlab-rails/sockets/gitlab.socket
/opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/socket_helper.rb:122:in initialize' /opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/socket_helper.rb:122:innew’
/opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/socket_helper.rb:122:in bind_listen' /opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/http_server.rb:231:inlisten’
/opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/http_server.rb:808:in block in bind_new_listeners!' /opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/http_server.rb:808:ineach’
/opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/http_server.rb:808:in bind_new_listeners!' /opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/lib/unicorn/http_server.rb:130:instart’
/opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/gems/unicorn-5.1.0/bin/unicorn:126:in <top (required)>' /opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/bin/unicorn:23:inload’
/opt/gitlab/embedded/service/gitlab-rails/vendor/bundle/ruby/2.4.0/bin/unicorn:23:in `<top (required)>’Choerodon平台版本: 0.10.0遇到问题的执行步骤:ZooKeeper安装后，因为磁盘空间不足，master节点宕过一次。恢复后，zookeeper一个pod就一直卡在ContainerCreating文档地址:环境信息(如:节点信息):
master01   Ready     master    5d        v1.8.5
master02   Ready     master    5d        v1.8.5
master03   Ready     master    5d        v1.8.5
worker01   Ready         5d        v1.8.5
worker04   Ready         5d        v1.8.5报错日志:
Type     Reason       Age                  From               MessageWarning  FailedMount  47m (x1204 over 2d)  kubelet, worker01  Unable to mount volumes for pod “zookeeper-0_c7n-system(ef8ea10f-e6e7-11e8-b079-00505688fc77)”: timeout expired waiting for volumes to attach/mount for pod “c7n
-system”/“zookeeper-0”. list of unattached/unmounted volumes=[zookeeper]  Warning  FailedMount  32m (x508 over 1d)   kubelet, worker01  (combined from similar events): MountVolume.SetUp failed for volume “pvc-4afee96b-e3da-11e8-8499-00505688fc77” : mount failed: exit status 32
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/ef8ea10f-e6e7-11e8-b079-00505688fc77/volumes/kubernetes.io~nfs/pvc-4afee96b-e3da-11e8-8499-00505688fc77 --scope – mount -t nfs -o vers=4
.1 10.233.13.16:/export/pvc-4afee96b-e3da-11e8-8499-00505688fc77 /var/lib/kubelet/pods/ef8ea10f-e6e7-11e8-b079-00505688fc77/volumes/kubernetes.io~nfs/pvc-4afee96b-e3da-11e8-8499-00505688fc77Output: Running scope as unit run-63310.scope.
mount.nfs: Connection timed out
Warning  FailedSync  22m (x1273 over 2d)  kubelet, worker01  Error syncing pod原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你对磁盘进行了什么操作吗没有，把eviction-hard nodefs.available 调低了麻烦 看下 nfs-provisioner 的日志I1112 08:37:27.480310       1 main.go:63] Provisioner choerodon.io/nfs-provisioner specified
I1112 08:37:27.480386       1 main.go:87] Setting up NFS server!
I1112 08:37:27.582087       1 server.go:144] starting RLIMIT_NOFILE rlimit.Cur 65536, rlimit.Max 65536
I1112 08:37:27.582106       1 server.go:155] ending RLIMIT_NOFILE rlimit.Cur 1048576, rlimit.Max 1048576
I1112 08:37:27.593118       1 server.go:129] Running NFS server!
I1112 08:37:32.603135       1 leaderelection.go:185] attempting to acquire leader lease  c7n-system/choerodon.io-nfs-provisioner…
E1112 08:38:04.991459       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:38:23.644565       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:38:40.807130       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:38:58.188010       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:39:15.425725       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:39:33.153305       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:39:51.395279       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:40:10.353147       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:40:27.872515       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
E1112 08:40:45.791352       1 leaderelection.go:268] Failed to update lock: etcdserver: request timed out
I1112 08:40:53.019790       1 leaderelection.go:194] successfully acquired lease c7n-system/choerodon.io-nfs-provisioner
I1112 08:40:53.020335       1 controller.go:631] Starting provisioner controller choerodon.io/nfs-provisioner_nfs-provisioner-7984dc5957-xwvs5_32a4ab50-e656-11e8-8935-0a580ae9441d!
I1112 08:40:53.020345       1 event.go:221] Event(v1.ObjectReference{Kind:“Endpoints”, Namespace:“c7n-system”, Name:“choerodon.io-nfs-provisioner”, UID:“bd49dad2-e3d6-11e8-8499-00505688fc77”, APIVersion:“v1”, ResourceVersion
:“441227”, FieldPath:""}): type: ‘Normal’ reason: ‘LeaderElection’ nfs-provisioner-7984dc5957-xwvs5_32a4ab50-e656-11e8-8935-0a580ae9441d became leaderI1112 08:40:53.120503       1 controller.go:680] Started provisioner controller choerodon.io/nfs-provisioner_nfs-provisioner-7984dc5957-xwvs5_32a4ab50-e656-11e8-8935-0a580ae9441d!hi 你尝试把自己该pod删除再看下 如果问题仍然存在 可以私信我们远程查看一下这个问题Choerodon平台版本: 0.10.0遇到问题的执行步骤: 从0.9.0升级到0.10.0文档地址:环境信息(如:节点信息):配置的内部DNS服务器，已经解析相关域名。错误信息：Not Found 404原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问hi  0.10版本开始已经没有swagger界面了多谢Choerodon平台版本: 0.10.0遇到问题的执行步骤:
在安装gitlab-service时，参数 env.open.GITLAB_PRIVATETOKEN=“GEuRhgb6kG9y3prFosSb” 问题。请问token是怎么获取的？ 还是按照文档的内容直接设置？文档地址: http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/choerodon-devops/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:针对env.open.GITLAB_PRIVATETOKEN 参数设置的值有疑问，是需要通过其他途径获取？还是按照文档设置的进行设置。用admin用户登录gitlab,然后点击管理然后点击admin   找到admin用户
创建impression token
然后拿到token
多谢。目前我在重新部署0.10.0 使用一键安装Choerodon平台版本: 0.6.0遇到问题的执行步骤:
执行 sh init-local-database.sh 时文档地址:
https://github.com/choerodon/iam-service环境信息(如:节点信息):
本地报错日志:
groovy.lang.MissingPropertyException: No such property: helper for class: org.liquibase.groovy.delegate.ChangeSetDelegate
at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:53) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.PogoGetPropertySite.getProperty(PogoGetPropertySite.java:52) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callGroovyObjectGetProperty(AbstractCallSite.java:307) ~[groovy-2.4.10.jar!/:2.4.10]
at db.Script1$_run_closure1$_closure2.doCall(Script1.groovy:5) ~[na:na]
at db.Script1$_run_closure1$_closure2.doCall(Script1.groovy) ~[na:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1027) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:42) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117) ~[groovy-2.4.10.jar!/:2.4.10]
at org.liquibase.groovy.delegate.DatabaseChangeLogDelegate.changeSet(DatabaseChangeLogDelegate.groovy:115) ~[liquibase-groovy-dsl-1.2.2.jar!/:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:384) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1027) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:69) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:174) ~[groovy-2.4.10.jar!/:2.4.10]
at db.Script1$_run_closure1.doCall(Script1.groovy:4) ~[na:na]
at db.Script1$_run_closure1.doCall(Script1.groovy) ~[na:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:294) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1027) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:42) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117) ~[groovy-2.4.10.jar!/:2.4.10]
at liquibase.parser.ext.GroovyLiquibaseChangeLogParser.processDatabaseChangeLogRootElement(GroovyLiquibaseChangeLogParser.groovy:136) ~[liquibase-groovy-dsl-1.2.2.jar!/:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:325) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:384) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1027) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:69) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:182) ~[groovy-2.4.10.jar!/:2.4.10]
at liquibase.parser.ext.GroovyLiquibaseChangeLogParser$_getChangeLogMethodMissing_closure3.doCall(GroovyLiquibaseChangeLogParser.groovy:93) ~[liquibase-groovy-dsl-1.2.2.jar!/:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:93) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.metaclass.ClosureMetaMethod.invoke(ClosureMetaMethod.java:84) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokeMissingMethod(MetaClassImpl.java:944) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1267) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1220) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.ExpandoMetaClass.invokeMethod(ExpandoMetaClass.java:1125) ~[groovy-2.4.10.jar!/:2.4.10]
at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1027) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:69) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:52) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:154) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:174) ~[groovy-2.4.10.jar!/:2.4.10]
at db.Script1.run(Script1.groovy:3) ~[na:na]
at db.Script1$run.call(Unknown Source) ~[na:na]
at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:48) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:113) ~[groovy-2.4.10.jar!/:2.4.10]
at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:117) ~[groovy-2.4.10.jar!/:2.4.10]
at liquibase.parser.ext.GroovyLiquibaseChangeLogParser.parse(GroovyLiquibaseChangeLogParser.groovy:64) ~[liquibase-groovy-dsl-1.2.2.jar!/:na]
at liquibase.Liquibase.getDatabaseChangeLog(Liquibase.java:229) ~[liquibase-core-3.5.3.jar!/:na]
at liquibase.Liquibase.update(Liquibase.java:202) ~[liquibase-core-3.5.3.jar!/:na]
at liquibase.Liquibase.update(Liquibase.java:192) ~[liquibase-core-3.5.3.jar!/:na]
at liquibase.Liquibase.update(Liquibase.java:188) ~[liquibase-core-3.5.3.jar!/:na]
at io.choerodon.liquibase.LiquibaseExecutor.load(LiquibaseExecutor.java:236) ~[classes!/:0.5.2.RELEASE]
at io.choerodon.liquibase.LiquibaseExecutor.simpleExec(LiquibaseExecutor.java:181) ~[classes!/:0.5.2.RELEASE]
at io.choerodon.liquibase.LiquibaseExecutor.run(LiquibaseExecutor.java:77) ~[classes!/:0.5.2.RELEASE]
at org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:776) [spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:760) [spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.afterRefresh(SpringApplication.java:747) [spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) [spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at io.choerodon.liquibase.LiquibaseTools.main(LiquibaseTools.java:16) [classes!/:0.5.2.RELEASE]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_181]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_181]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_181]
at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) [choerodon-tool-liquibase.jar:0.5.2.RELEASE]
at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) [choerodon-tool-liquibase.jar:0.5.2.RELEASE]
at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) [choerodon-tool-liquibase.jar:0.5.2.RELEASE]
at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) [choerodon-tool-liquibase.jar:0.5.2.RELEASE]原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问线上部署的话，使用0.5.6的dbtool镜像>>image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/dbtool:0.5.6
本地初始化数据库测试的话，使用0.6.3以上版本的jar包 >>choerodon-tool-liquibase-0.6.3.RELEASE.jarthen
curl http://nexus.choerodon.com.cn/repository/choerodon-release/io/choerodon/choerodon-tool-liquibase/0.5.2.RELEASE/choerodon-tool-liquibase-0.5.2.RELEASE.jar -o target/choerodon-tool-liquibase.jar
fi这个脚本哪些地方需要改的？我把0.5.2替换成0.6.3，执行还是报错什么数据库mysql，这是修改后的执行脚本
 mkdir -p target
if [ ! -f target/choerodon-tool-liquibase.jar ]
then
 curl http://nexus.choerodon.com.cn/repository/choerodon-release/io/choerodon/choerodon-tool-liquibase/0.6.3.RELEASE/choerodon-tool-liquibase-0.6.3.RELEASE.jar -o target/choerodon-tool-liquibase.jar
fi
java -Dspring.datasource.url=“jdbc:mysql://47.106.119.52/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false” 
-Dspring.datasource.username=username 
-Dspring.datasource.password=password 
-Ddata.drop=false -Ddata.init=true 
-Ddata.dir=src/main/resources 
-jar target/choerodon-tool-liquibase.jar执行还是报错不应该啊，换了jar包还是报一样的错误吗？先把targer里面的包删掉在跑脚本，不然不会重新下载的肯定删掉了的，报错信息是Error: Invalid or corrupt jarfile target/choerodon-tool-liquibase.jarok，已经可以了
我自己通过浏览器下载，然后重命名的方式放到target目录下，再执行脚本 执行成功提个建议：
下次如果需要换版本的话，先把readme里的改了好，感谢建议
贴一个可以的脚本用例执行过程中，发现BUG后，需要跳转页面到新建故障问题管理页面，然后回头再关联故障，这交互有点断层，而且关联的时候本来只要关联故障类型的问题，可列表里把所有类型的问题都成为待选建议bug确认后直接生成新的故障您好，感谢您对我们平台的支持以及提出宝贵的意见，我们会进行参考优化。谢谢虽然有prometheus做监控，但还是希望在平台上能直观的看到pod的CPU内存接口流量这些信息后续的版本会在环境总览的实例信息中展示更多的内容，主要是kubectl descript相关的内容，一些额外的监控数据我们会在后续的版本中尽力支持，但是该监控数据目前是需要对应k8s环境中有prometheus的组件，局限性很大，我们也在努力想解决方案，该块功能后续肯定会支持，谢谢反馈Choerodon平台版本：0.10.0运行环境：自主分步搭建问题描述：打开猪齿鱼管理后台，跳转到登录界面，请问初始登录账号密码是多少？我在安装步骤里面没找到。是我配置错了redirect url，好像是跳转到了官方的example页面了2018/11/13 15:38:54 [INFO] getting resource /version.yml
2018/11/13 15:38:54 [INFO] getting resource /0.10/install.yml
2018/11/13 15:38:54 [INFO] namespace c7n-system already exists
2018/11/13 15:38:55 [INFO] waiting slaver running…
2018/11/13 15:38:56 [INFO] waiting slaver running…
E1113 15:38:56.985633   54513 portforward.go:331] an error occurred forwarding 8000 -> 9000: error forwarding port 9000 to pod 20496b0c09cfdee667c4fc13956116e991ddf4d609de41571b1e802966b8d1fd, uid : unable to do port forwarding: socat not found.
2018/11/13 15:38:57 [INFO] clean history jobs…
2018/11/13 15:38:57 [INFO] start install gitlab-mysql
2018/11/13 15:38:57 [INFO] no user config resource for gitlab-mysql
2018/11/13 15:38:57 [INFO] using exist release gitlab-mysql
2018/11/13 15:38:57 [INFO] start install mysql
2018/11/13 15:38:57 [INFO] using exist release mysql
2018/11/13 15:38:57 [INFO] start install redis
2018/11/13 15:38:57 [INFO] no user config resource for redis
2018/11/13 15:38:57 [INFO] using exist release redis
2018/11/13 15:38:57 [INFO] start install zookeeper
2018/11/13 15:38:57 [INFO] no user config resource for zookeeper
2018/11/13 15:38:57 [INFO] using exist release zookeeper
2018/11/13 15:38:57 [INFO] start install gitlab
2018/11/13 15:38:58 [INFO] Checking gitlab-mysql is running
E1113 15:38:58.090469   54513 portforward.go:331] an error occurred forwarding 8001 -> 9001: error forwarding port 9001 to pod 20496b0c09cfdee667c4fc13956116e991ddf4d609de41571b1e802966b8d1fd, uid : unable to do port forwarding: socat not found.
E1113 15:38:59.091963   54513 portforward.go:331] an error occurred forwarding 8001 -> 9001: error forwarding port 9001 to pod 20496b0c09cfdee667c4fc13956116e991ddf4d609de41571b1e802966b8d1fd, uid : unable to do port forwarding: socat not found.
E1113 15:39:00.792958   54513 portforward.go:331] an error occurred forwarding 8001 -> 9001: error forwarding port 9001 to pod 20496b0c09cfdee667c4fc13956116e991ddf4d609de41571b1e802966b8d1fd, uid : unable to do port forwarding: socat not found.E1113 15:39:33.383598   54513 portforward.go:271] error creating error stream for port 8001 -> 9001: Timeout occured
E1113 15:39:53.385537   54513 portforward.go:293] error creating forwarding stream for port 8001 -> 9001: Timeout occured
E1113 15:40:13.384708   54513 portforward.go:293] error creating forwarding stream for port 8001 -> 9001: Timeout occured
E1113 15:40:33.384348   54513 portforward.go:271] error creating error stream for port 8001 -> 9001: Timeout occured
E1113 15:40:53.384997   54513 portforward.go:293] error creating forwarding stream for port 8001 -> 9001: Timeout occured
E1113 15:41:23.140925   54513 portforward.go:293] error creating forwarding stream for port 8001 -> 9001: Timeout occured
E1113 15:42:02.990660   54513 portforward.go:271] error creating error stream for port 8001 -> 9001: Timeout occured
E1113 15:43:00.590815   54513 portforward.go:293] error creating forwarding stream for port 8001 -> 9001: Timeout occured
E1113 15:44:54.720169   54513 portforward.go:293] error creating forwarding stream for port 8001 -> 9001: Timeout occured您好，在您的节点上安装 socat 试一下sudo yum install socatTHX节点上没有按照socat。又有一个新问题，一直卡在这里，很久没有提示了，这个是什么原因呢。[root@k8s-master-02 c7n-0.1.0]# ./c7n install -c config.yml --no-timeout
2018/11/13 16:24:36 [INFO] getting resource /version.yml
2018/11/13 16:24:36 [INFO] getting resource /0.10/install.yml
2018/11/13 16:24:36 [INFO] namespace c7n-system already exists
2018/11/13 16:24:37 [INFO] waiting slaver running…
2018/11/13 16:24:38 [INFO] waiting slaver running…
2018/11/13 16:24:38 [INFO] clean history jobs…
2018/11/13 16:24:38 [INFO] start install gitlab-mysql
2018/11/13 16:24:38 [INFO] no user config resource for gitlab-mysql
2018/11/13 16:24:38 [INFO] using exist release gitlab-mysql
2018/11/13 16:24:38 [INFO] start install mysql
2018/11/13 16:24:38 [INFO] using exist release mysql
2018/11/13 16:24:38 [INFO] start install redis
2018/11/13 16:24:38 [INFO] no user config resource for redis
2018/11/13 16:24:38 [INFO] installing redis
2018/11/13 16:24:40 [INFO] installed redis
2018/11/13 16:24:40 [INFO] start install zookeeper
2018/11/13 16:24:40 [INFO] no user config resource for zookeeper
2018/11/13 16:24:40 [INFO] installing zookeeper
2018/11/13 16:24:40 [INFO] installed zookeeper
2018/11/13 16:24:41 [INFO] start install gitlab
2018/11/13 16:24:42 [INFO] created pvc [gitlab]
2018/11/13 16:24:43 [INFO] Checking gitlab-mysql is running你可以添加 --debug 看下 debug日志Choerodon平台版本：0.10.0运行环境：自主搭建问题描述：
随着使用猪齿鱼平台的深入，发现很多应用都有持久化保存数据的需要。但由于目前没有相应的pv、pvc的管理策略，只能由运维人员手动在服务器创建pv、pvc，当数量多了就很不容易管理。因此想咨询一下猪齿鱼开发团队后期是否有相应的计划加入pv、pvc的管理。
这里我大胆提出一些功能展望，请各位大佬指点。
1、在创建环境时，可以配置 动态存储卷，并且设置存储大小
2、可以手动创建pvc，可以管理pvc中的文件，支持文件的上传下载
3、部署应用时可以选择该环境下的pvc如果pv、pvc仅仅只是nfs的类型其实是可以的，但是很多pv是挂在host上的，这种方式的管理很偏硬件，所以实际实现起来很不好维护，后面有规划在实例中显示相关pv、pvc的绑定情况，主要内容类似kubectl describe的信息，但是具体pv、pvc的创建、更新等管理的功能我们暂时没有做规划，但是这方面的诉求目前是有一些的，之后我们会再讨论看有没有什么好的关联处理方式，感谢您的建议，我们会认真考虑相关的诉求Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):报错日志:
安装文档中的 私有云安装示例部署单节点，执行显示成功，没有失败，但是执行这个命令报错：
kubectl get po -n kube-system刚开始执行完kubectl get po -n kube-system后可以看到相关的一些信息：
过一会儿再来执行就上面的那个错误了请考虑是否是内存不足导致程序退出啊哈，这个配置是不是肯定不行的，其他什么东西也没装
您好，这个配置无法满足k8s的最低需求啊哈，谢谢谢谢小哥哥我想问下之前的这个分享还有视频回放可以看吗
分享可以在这个网址回看
http://www.itdks.com/liveevent/detail/16310啊哈，谢谢日志监控要求的节点数是否至少需要3个？取决于elasticsearch节点的数量，为了高可用需求需要使用至少三个ES节点，一般情况下每个es节点会分布到不同的主机上，如果你的主机不够，一台主机可能会同时运行两个或三个es节点，这样会产生很大的性能问题，并且导致cpu、内存使用率大幅上升和其他意外情况。请问日志监控安装成功后可以如何验证？可以怎么访问控制台？您访问您安装时配置的域名即可希望可以开源出来c7n helm charts的源码。
由于外部的镜像库比较慢，或者被我司内网网络隔离了，所以我这边是希望修改helm的定义，指定到我们私有镜像库地址。您好，chart由ci构建而成，你可以通过 helm fetch 命令获取chart包好的，谢谢Choerodon平台版本：0.10.0问题描述：
在敏捷管理-待办事项中同时创建两个冲刺，并且无问题，在删除冲刺时会报错导致无法删除。详细报错截图如下：
你好，这个问题是我们上个版本的一个bug，新版本已经修复过了。如果您要删除冲刺，临时的解决办法是在冲刺中创建一个issue。或者等待我们0.11 版本发布。Choerodon平台版本：0.10.0运行环境：自主搭建问题描述：画词统计字数以较常用的功能，在石墨、Word的文档编辑工具中都有。这样可以方便编辑人员统计字数。嗯，这个我们会考虑下。Choerodon平台版本: 0.6.0遇到问题的执行步骤:
ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml执行了这个命令后，卡在了下面的这个节点这。。。。。TASK [node : Join to cluster if needed] *******************************************************************************************************************************************
省略此处信息，下方描述即可知
因为新添加的节点机和master主机为非同一网段，master和节点1机器均为192.168.11.*网段，新节点机为192.168.16.*网段，ip均正常互通，ip也同样均唯一，添加该16网段机器作为节点机，即不能成功，求救，谢谢！hi，添加节点不是这条命令。请查看下文档中关于如何添加节点操作流程抱歉问题描述不清楚，补充说明如下：
1、之前是重置ansible的执行语句，在节点处停止，但是去除非同一网段ip后即可正常完成部署，否则非然
2、1步骤重置部署成功后，再次修改hosts，添加非同一网段ip机器，又一次卡在添加节点机未知，如下
TASK [node : Join to cluster if needed] ********************************************************************************************
Monday 12 November 2018  08:52:28 +0800 (0:00:01.433)       0:01:21.593 *******
3、兄台，操作机器为青浦机房虚拟机，区别仅为相应节点机不在同一网段，成功的为与master同在11网段ip的机器，16网段即卡停，故还需要在此求问帮助，谢谢请重启16网段虚拟机后重试，谢谢。TASK [node : Join to cluster if needed] ********************************************************************************************
Monday 12 November 2018  09:26:55 +0800 (0:00:01.709)       0:01:11.626 *******^F5fatal: [k8s-slave-node-108-3]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “192.168.16.108”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [k8s-slave-node-109-4]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “192.168.16.109”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [k8s-slave-node-110-5]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “192.168.16.110”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [k8s-slave-node-111-6]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “192.168.16.111”. Make sure this host can be reached over ssh”, “unreachable”: true}NO MORE HOSTS LEFT *****************************************************************************************************************
to retry, use: --limit @/root/kubeadm-ansible/scale.retryPLAY RECAP *************************************************************************************************************************
k8s-master-node-184-1      : ok=39   changed=1    unreachable=0    failed=0
k8s-slave-node-108-3       : ok=41   changed=9    unreachable=1    failed=0
k8s-slave-node-109-4       : ok=40   changed=6    unreachable=1    failed=0
k8s-slave-node-110-5       : ok=40   changed=6    unreachable=1    failed=0
k8s-slave-node-111-6       : ok=40   changed=6    unreachable=1    failed=0
k8s-slave-node-168-2       : ok=41   changed=1    unreachable=0    failed=0reboot时即停止，然后再重新执行，再次卡住再次卡住最后实录
TASK [node : Create kubeadm client config] *****************************************************************************************
Monday 12 November 2018  09:41:01 +0800 (0:00:00.664)       0:01:09.753 *******
ok: [k8s-slave-node-108-3]
ok: [k8s-slave-node-168-2]
ok: [k8s-slave-node-110-5]
ok: [k8s-slave-node-109-4]
ok: [k8s-slave-node-111-6]TASK [node : Join to cluster if needed] ********************************************************************************************
Monday 12 November 2018  09:41:02 +0800 (0:00:01.389)       0:01:11.142 *******https://192.168.11.168/#!/node?namespace=default请反馈一下在卡住状态下查看16网段节点kubelet日志Nov 12 13:37:58 k8s-slave-node-108-3 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Nov 12 13:37:58 k8s-slave-node-108-3 systemd[1]: Unit kubelet.service entered failed state.
Nov 12 13:37:58 k8s-slave-node-108-3 systemd[1]: kubelet.service failed.
Nov 12 13:38:09 k8s-slave-node-108-3 systemd[1]: kubelet.service holdoff time over, scheduling restart.
Nov 12 13:38:09 k8s-slave-node-108-3 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 12 13:38:09 k8s-slave-node-108-3 systemd[1]: Starting kubelet: The Kubernetes Node Agent…
Nov 12 13:38:09 k8s-slave-node-108-3 kubelet[15497]: I1112 13:38:09.168891   15497 feature_gate.go:156] feature gates: map[]
Nov 12 13:38:09 k8s-slave-node-108-3 kubelet[15497]: I1112 13:38:09.169024   15497 controller.go:114] kubelet config controller: starting controller
Nov 12 13:38:09 k8s-slave-node-108-3 kubelet[15497]: I1112 13:38:09.169035   15497 controller.go:118] kubelet config controller: validating combination of defaults and flags
Nov 12 13:38:09 k8s-slave-node-108-3 kubelet[15497]: error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory
Nov 12 13:38:09 k8s-slave-node-108-3 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Nov 12 13:38:09 k8s-slave-node-108-3 systemd[1]: Unit kubelet.service entered failed state.
Nov 12 13:38:09 k8s-slave-node-108-3 systemd[1]: kubelet.service failed./etc/kubernetes/pki/ca.crt请将master节点上/etc/kubernetes/pki/ca.crt拷贝至16网段机子同一位置拷贝后仍然如前
16网段机器日志仍如下
Nov 12 14:16:13 k8s-slave-node-108-3 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Nov 12 14:16:13 k8s-slave-node-108-3 systemd[1]: Unit kubelet.service entered failed state.
Nov 12 14:16:13 k8s-slave-node-108-3 systemd[1]: kubelet.service failed.
Nov 12 14:16:23 k8s-slave-node-108-3 systemd[1]: kubelet.service holdoff time over, scheduling restart.
Nov 12 14:16:23 k8s-slave-node-108-3 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Nov 12 14:16:23 k8s-slave-node-108-3 systemd[1]: Starting kubelet: The Kubernetes Node Agent…
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: I1112 14:16:23.658107   18462 feature_gate.go:156] feature gates: map[]
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: I1112 14:16:23.658215   18462 controller.go:114] kubelet config controller: starting controller
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: I1112 14:16:23.658221   18462 controller.go:118] kubelet config controller: validating combination of defaults and flags
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: I1112 14:16:23.663944   18462 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: I1112 14:16:23.664002   18462 client.go:95] Start docker client with request timeout=2m0s
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: E1112 14:16:23.664369   18462 kube_docker_client.go:91] failed to retrieve docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: W1112 14:16:23.664391   18462 kube_docker_client.go:92] Using empty version for docker client, this may sometimes cause compatibility issue.
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: W1112 14:16:23.664604   18462 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: I1112 14:16:23.668741   18462 feature_gate.go:156] feature gates: map[]
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: W1112 14:16:23.668881   18462 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Nov 12 14:16:23 k8s-slave-node-108-3 kubelet[18462]: error: failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory
Nov 12 14:16:23 k8s-slave-node-108-3 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Nov 12 14:16:23 k8s-slave-node-108-3 systemd[1]: Unit kubelet.service entered failed state.
Nov 12 14:16:23 k8s-slave-node-108-3 systemd[1]: kubelet.service failed.机器账号信息私信你了经排查是k8s-slave-node-108-3节点安装了其他版本的docker，导致脚本在安装docker时失败，现已为你重新安装docker喔噢，原来如此，多谢多谢，感谢感谢Choerodon平台版本：0.10.0运行环境：SaaS问题描述：文档编辑中采用不同的颜色高亮文本字体是一个比较常用的功能，现在的版本中还没有功能，以后是否可以增加？嗯嗯，多谢提醒。这个功能正在规划中。 Choerodon平台版本：0.10.0运行环境：自主搭建问题描述：知识管理模块可否增加自动Ctrl+S的快捷键保存功能?其他的很多平台都有类似这样的功能，编辑文档习惯性的会Ctrl+S保存文档。嗯嗯，好的。我们后面会规划这个功能的。Choerodon平台版本：0.10.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：如：在启动服务过程中，启动报错
Error starting ApplicationContext. To display the auto-configuration report re-run your application with ‘debug’ enabled.
2018-11-12 16:09:46.387 ERROR [hzero-platform,] 38532 — [           main] o.s.boot.SpringApplication               : Application startup failedorg.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘dialect’ defined in class path resource [io/choerodon/mybatis/MybatisMapperAutoConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [io.choerodon.mybatis.pagehelper.Dialect]: Factory method ‘dialect’ threw exception; nested exception is java.lang.NoSuchMethodError: net.sf.jsqlparser.statement.select.Top.setRowCount(J)V
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:599) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1173) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1067) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:513) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:761) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:866) ~[spring-context-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:542) ~[spring-context-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122) ~[spring-boot-1.5.3.RELEASE.jar:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:737) [spring-boot-1.5.3.RELEASE.jar:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:370) [spring-boot-1.5.3.RELEASE.jar:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:314) [spring-boot-1.5.3.RELEASE.jar:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:1162) [spring-boot-1.5.3.RELEASE.jar:1.5.3.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:1151) [spring-boot-1.5.3.RELEASE.jar:1.5.3.RELEASE]
at org.hzero.platform.Application.main(Application.java:31) [classes/:na]
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [io.choerodon.mybatis.pagehelper.Dialect]: Factory method ‘dialect’ threw exception; nested exception is java.lang.NoSuchMethodError: net.sf.jsqlparser.statement.select.Top.setRowCount(J)V
at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:189) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:588) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
… 18 common frames omitted
Caused by: java.lang.NoSuchMethodError: net.sf.jsqlparser.statement.select.Top.setRowCount(J)V
at io.choerodon.mybatis.pagehelper.parser.SqlServerParser.(SqlServerParser.java:78) ~[choerodon-starter-mybatis-mapper-0.6.4.RELEASE.jar:0.6.4.RELEASE]
at io.choerodon.mybatis.pagehelper.dialect.SqlServerDialect.(SqlServerDialect.java:48) ~[choerodon-starter-mybatis-mapper-0.6.4.RELEASE.jar:0.6.4.RELEASE]
at io.choerodon.mybatis.MybatisMapperAutoConfiguration.dialect(MybatisMapperAutoConfiguration.java:93) ~[hzero-starter-mybatis-mapper-0.3.0-20181108.074242-46.jar:0.3.0-SNAPSHOT]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_162]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_162]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_162]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_162]
at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:162) ~[spring-beans-4.3.8.RELEASE.jar:4.3.8.RELEASE]
… 19 common frames omitted原因分析：服务内升级了jsqlparser版本，用于解析sql而pageHelper不支持对应版本。建议：更新pageHelper版本，支持最新的jsqlparser版本pageHelperPageHelper文档上只支持到0.9.5版本的jsqlpaser，你用的啥版本。还有sqlserver数据库因为没有做过测试，可能会有一些问题，我们后续讨论是否兼容SqlServer数据库那兼容时间点可以发下？我们的排期优先级可能不太高，日期尚不明确。但是我们是开源项目，也欢迎你加入进来，可以到github上提merge requet 目前原来系统k8s版本比所要求的版本低，请问k8s版本可以降低吗
http://choerodon.io/zh/docs/installation-configuration/install-list/请问原来是什么版本？目前CentOS7.2默认安装的版本为Docker1.15，比官网要求的版本低想知道现在最高可以支持到Docker哪个版本你需要升级的centos，建议升级到 centos7.4 目前， 不建议在7.2上安装。同时请不要安装docker，使用安装脚本会为你安装合适的docker版本好的，谢谢，另外有个问题请教下，
当前日志监控模块安装完成后可以通过访问什么页面或者什么命令验证成功呢？开发流水 代码仓库，分支，合并请求，gitlab上流水线显示已通过，但打开持续集成
页目却空白浏览器控制台有错误，devops-service 无任何相关日志，如何排查
该bug是由于持续集成的数据 没有状态导致的白屏，可以升级devops-service服务到0.10.4，然后调用devops-service 里面的devops-check-contoller的第一个平滑升级接口  参数0.10.4  修复持续集成的数据的状态升级了0.10.4并post了0.10.4似乎还是白页
该条数据是因为该pipeline数据没有关联上commit信息，所以在修复的时候跳过修复，我们这边会检查代码确认没有pipeline没有关联commit的原因然后进行修复。不知道为什么devops_gitlab_pipeline表里面同个APPId下有两条记录,有条状态是NULL,另一条是success，我把null的那条手动删了，现在页面能显示了恩 是因为0.10.4之前 由于gitlab版本不一致，导致通过webhook存储pipeline的时候,status字段取值错误，所以导致的status为空。之前取的是detailed_status，但是gitlab的0.11版本之前的有些版本 没有这个属性，所以现在0.10.4版本改成了取status，之后新建pipeline不会出现status为空的情况了。刚又发现 了个问题，如果不是通过po角色的用户而是通过admin帐号去创建应用的标记，会造成该 流水线失败
admin没有该组的权限, ci执行的权限必须在gitlab组内有权限，你可以在我们平台给admin在项目下分配项目所有者角色，或者手动给admin在gitlab组内添加权限OK,明白你好，现在又有新的疑问，图中的标识#6通过刷新操流水线重新跑通后，在平台应用版本中已经可以看到新的版本(0.0.1)
跑完ci生成版本后，如果你需要发布出去，那就需要每次都发布，如果你不需要发布出去，就不需要发布。意思是可以只发布你想发布出去的应用版本应用发布一次之后就成了已发布应用了，针对每个应用，可以对发布的应用版本进行修改。表里面字段是NULL表示你这个版本没有被发布，是正常的问题是我发不了图中的0.0.1我明白了你的意思，就是应用发布只需要做一次，在部署的时候可以选择版本号，只是现在的情况应用市场中的版本列表里只显示最初发布时的应用版本号，实际上我已经构建过多次了，以至我误解为新的版本并没有发布成功，这个地方是不是应该把所有构建成功的版本号都列出来
理解完全正确。1.每次构建都可以生成部署版本。有些是流水版本，有些是正式版本，猪齿鱼使用的是语义版本规则；
2.管理人员可以选择发布的版本，例如选择正式版本进行发布，而不是流水版本。甘兄，多次代码合并成功构建应用后，应用市场里头此应用始终只有一个最初的版本可选，是否为BUG，观察helm库此应用已有多个包存在这肯定是Bug，我测试一下。然后给你反馈。我描述的不是这个地方，而是应用市场页面，看我前面的截图应用市场中能选到的版本是已经发布的版本，版本也是需要发布之后才能选到的，这里的逻辑主要针对应用维护方一般只会发布已经测试过的版本，而不是所有的流水版本。
发布应用和版本的步骤过程如上图的描述第一步 mvn package 已经成功
第二步 docker build 找不到 jar 包，但有时候重试几次就可以了
经常会在 docker-build 时出现各种错误，比如下面。所有服务 gitlab-ci.yal 文件都是一样的，有些服务CI时成功，有些则报错误，有时候重试几次就可以了。请问这是什么原因，是哪里配置还有问题还是其它？
请问是你自己搭建的平台吗用的你们的Gitlab
code.choerodon.com.cnHi, 你使用自己搭建的runner， 请检查一下你的runner是否已正确挂载了 /cache/这个目录到nfs或其他共享储存设备中, 以及是否有程序对此目录进行自动清理。第一个步骤生成jar包，第二个步骤将jar打包成docker镜像进行发布，两个步骤会用两个容器进行，这两个容器会绑定一个共享的目录来保证jar可以共享，所以需要确定自己的runner是否配置正确， 是否已正确挂载了  /cache/ 这个目录到nfs或其他共享储存设备中, 以及是否有程序对此目录进行自动清理造成第二个步骤中jar找不到的问题。没改什么配置，今天CI都成功了。有个区别，失败的是基于TAG发布，其它成功的都是在 master 上，不知道跟这个有没有关系。
具体情况具体处理吧，之前错误看报错信息是已经回复的情况，其他的如果还有问题就要再具体看了图标有什么格式大小要求吗
可以看下请求的相关状态么？file-uplod的相关请求sorry,是我修改了对像存储的密码，没有更新到file-serivce上Choerodon平台版本：0.10.0运行环境：自主搭建问题描述：创建空间选择空间图标，这个功能不好用。
是不是因为图标太多了，然后图标表达也不形象，导致不知道该选择哪个图标是吗？是的。嗯，我们考虑下优化方案。Choerodon平台版本：0.10.0运行环境：SaaS问题描述：操作之后，“再”此更新，没有办法截图，然后黏贴图片。https://wiki.choerodon.com.cn/bin/view/O-Choerodon/P-Choerodon产品运营/文创/WebHome/闪哥，你这个问题啥意思呀？我有点没看懂。1.新建一个页面
2.截取图片，然后黏贴进去
3.保存，关闭
4.再打开
5.截取图片，然后黏贴进去，无法黏贴闪哥，你用的是微信的截图吗？我根据你这个步骤，第一次和第二次粘贴图片都是可以成功的，你这个问题是不是因为图片没有截取成功呢？Choerodon如何离线安装？离线安装非常麻烦，建议服务器要连接外网。你这边是必须要离线安装吗？希望能在内网搭建该框架，如果要实现的话需要怎么操作呢？需要如下步骤：
1.把所有涉及到的Docker镜像等先下载到离线服务器
2.在安装过程中需要更新服务器各种底层的程序包
3.需要对Choerodon猪齿鱼的架构部分，操作系统，等非常熟悉才有可能安装成功。比如操作系统程序包的更新，需要联网的，不然也是需要下载到离线服务器，然后再安装，过程非常的繁琐。好的多谢，我去反馈下暂时不考虑内网了。
还有一个问题希望能指点下，Choerodon的日志监控能不能与k8s集群分开装在两台服务器上呢？可以的，本来就是分开安装的。另外，提示一下，安装Choerodon以及日志监控需要的资源还是蛮多的。
1.Choerodon安装http://choerodon.io/zh/docs/installation-configuration/pre-install/2.日志监控安装http://choerodon.io/zh/docs/installation-configuration/steps/operation/文档里的日志监控需要依赖k8s的helm工具，是不是意味着与原来的k8s在同一个集群，还是说要另开一个k8s集群？不在一个集群也可以。Choerodon平台版本: 0.8.0遇到问题的执行步骤: Kubernetes集群部署执行
ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/
测试环境模式环境信息(如:节点信息):localhost报错日志:
FAILED - RETRYING: Master | wait for kube-scheduler (3 retries left).
FAILED - RETRYING: Master | wait for kube-scheduler (2 retries left).
FAILED - RETRYING: Master | wait for kube-scheduler (1 retries left).
fatal: [k8s-master]: FAILED! => {“attempts”: 60, “changed”: false, “content”: “”, “msg”: “Status code was -1 and not [200]: Request failed: <urlopen error [Errno 111] Connection refused>”, “redirected”: false, “status”: -1, “url”: “http://localhost:10251/healthz”}麻烦粘贴一下前面的日志RUNNING HANDLER [master : Master | restart kubelet] ********************************************************************************
Saturday 11 August 2018  15:58:43 +0800 (0:00:00.313)       0:06:49.659 *******
changed: [k8s-master]RUNNING HANDLER [master : Master | wait for master static pods] ********************************************************************
Saturday 11 August 2018  15:58:44 +0800 (0:00:00.352)       0:06:50.011 *******
changed: [k8s-master]RUNNING HANDLER [master : Master | reload systemd] *********************************************************************************
Saturday 11 August 2018  15:58:44 +0800 (0:00:00.384)       0:06:50.396 *******
changed: [k8s-master]RUNNING HANDLER [master : Master | reload kubelet] *********************************************************************************
Saturday 11 August 2018  15:58:45 +0800 (0:00:00.483)       0:06:50.880 *******
changed: [k8s-master]RUNNING HANDLER [master : Master | wait for kube-scheduler] ************************************************************************
Saturday 11 August 2018  15:58:45 +0800 (0:00:00.397)       0:06:51.277 *******
FAILED - RETRYING: Master | wait for kube-scheduler (60 retries left).
FAILED - RETRYING: Master | wait for kube-scheduler (59 retries left).我又重新部署了一次，现在成功了小哥哥你有改什么吗，我重新部署还是报错，请问你是怎么重新部署的呢猪齿鱼全部服务的构建都是以alpine作为基础镜像,但alpine系统在某些K8S环境上可能会出现域名解析IP delays 3-5秒的现像,并不是所有K8S环境都会这样,我其中一套K8S环境就出现这种情况,整个猪齿鱼平台出现时快时慢情况,其实k8s社区也有很多人反馈这个问题,https://github.com/kubernetes/kubernetes/issues/56903
大至的原因就是同时解析A和AAAA记录,造成netfilter race,详细的分析可以参考这个https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/
非alpine类的的基础镜像可以通过修改 etc/resolv.conf
options single-request解决这个问题https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
但这个设置对alpine( musl/libc)是不生效的,所以我建议改用jessie-slim也只是30M而已,如果继续使用alpine推荐使用这个移除解析AAAA记录的修改版镜像
https://hub.docker.com/r/geekidea/alpine-a/
,https://github.com/kubernetes/kubernetes/issues/56903#issuecomment-409603030你好，感谢你的建议，我们会对该问题进行测试。能否提供一下你们出现这个问题的k8s环境信息?v1.10.9，其实这个情况有一定机率出现，因为我有很多套集群，而且所有的集群相同的内核相同的网络插件一样的k8s版本唯一不同可能是硬件级别有些差异，但确实碰到了这个情况，conntrack状态的如下图https://github.com/kubernetes/kubernetes/issues/56903#issuecomment-410515255 感兴趣可以了解下，不过我通过其它方法部署coredns也解决了这个问题Choerodon平台版本：0.10.0运行环境：自主搭建问题描述：
平台我在测试中发现一个情况,一个项目打包好的应用在多个环境部署的时候,数据源肯定是不一样的,那就得在"选择环境及修改配置信息"手动修改或增加环境变量",因为chart的默认value并不适用所有环境,比如redis地址,zookeeper地址都可能不同,那么用户每次部署时都要进行修改显得非常不便捷建议：
类似K8S的dashboard,

我们在环境流水线预设环境变量键值,可以在这个修改环境流水线的界面增加这功能
你好，首次部署的配置确实需要手动填写的。目前可以通过GitOps 在git 库进行提交修改，来减少重复的页面操作。我们后续会考虑增加环境的整体复制，从一套环境直接部署到另一套环境。你好，这方面功能我们已经在规划中了。Good通过部署文档默认安装完zookeeper、kafka后跑一阵子就会发现服务会经常性重启，
一个原因，zookeeper oom挂掉后 ，kafka也会跟着挂（livenessProbe去检查zk 状态，个人觉得不该这么做）
另一个原因，没有设置JAVA参数，内存使用虚高，建议zookeeper的jdk升级到8u131以上（现在是8u111），然后新增环境变量SERVER_JVMFLAGS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2 (让JVM能够识别容器limit内存限制,堆和非堆各占一半)
KAFKA的jdk是8u144,所以可以直接添上以下环境变量,让jvm能够支持容器cgroup策略
KAFKA_HEAP_OPTS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2
以上设置的前提是有做resources limits你好，感谢你的建议，关于 JVM 的限制我们已经添加到下个版本的服务镜像中了，有关于zookeeper 和kafka的问题，我们会对其进行测试。0.8.0 版本的 eureka-server，0.8.1 版本的 manager-service
服务注册到eureka后，topic register-server 成功写入数据，
{“status”:“UP”,“appName”:“manage-j”,“version”:“unknown”,“instanceAddress”:“10.211.131.180:8963”,“createTime”:“2018-11-08 19:46:48”}manager-service没有消费日志mgmt_swagger表也无数据插入server用的是eureka-server，是本地开发环境吗，看下有没有启动kafka{“status”:“UP”,“appName”:“manage-j”,“version”:“unknown”,“instanceAddress”:“10.211.131.180:8963”,“createTime”:“2018-11-08 19:46:48”}启动了，这是写入的jsoneureka-server没有报错信息吗？manager-service也没有receive message from register-server, {}之类的日志信息？如果都没有的话，估计是kafka的问题重新部署kafka后解决了问题管理、待办事项下问题较多，通过目前的搜索条件很难定位具体问题。
问题管理规范的项目一般都会内部要求问题名称规范，但是目前又不支持按照名称搜索，也不支持按照创建人搜索。希望尽快优化一下。项目上千个问题，真的很难找。你好，关于问题搜索功能的增强，我们已经在优化，问题管理模块的搜索会在下个版本发布。
感谢你的反馈。谢谢！使用这个方法去获取登陆用户信息时，一直返回为0，请问，哪里配置可以影响这个获取的用户信息吗？Choerodon平台版本: 0.10.0遇到问题的执行步骤:
执行bash install.sh choerodon.cfg之后一直提示c7n-mysql未就绪文档地址:环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:
图片上的错误信息是什么意思？请执行下面命令，提供一下返回结果请提供一下choerodon.cfg文件内容，谢谢这些域名我都内部解析到了单个服务器看上面信息貌似并没有问题，可以远程帮你看看哈非常感谢您，您怎么远程我呢 ，我的qq993961379，或者我加您的qq？请问这个问题有解决吗？我这出同样的错。查看namespace c7n-system中的是否还存在处于Terminating状态的pod，若存在请等待删除完毕后再进行部署。请问删除命名空间的使用一直处于Terminating状态，

查看里面的资源的时候什么也没有

请问这个有办法解决吗？=====================hosts===================================
========================================================
kubectl describe po  gitlab-mysql-595696b7bd-7ktb8 -n c7n-system  如下提示：========================================================
node1上执行 journalctl -u kubelet -n 30
环境： 4CPU 16G    VirtualBox 虚拟机 6节点麻烦看下 mysql的日志mysql pod没起来， 没法看到容器日志。请直接在虚拟机上运行下面语句，查看是否可以运行成功感谢回复， test-mysql可以成功运行Choerodon平台版本: 0.10遇到问题的执行步骤:在新建页面那一步操作python ./demo/node_modules/choerodon-front-boot/structure/configAuto.py demo 报错文档地址:
http://choerodon.io/zh/docs/development-guide/front/new-func/new_page/环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
python 版本：疑问:提出您对于遇到和解决该问题时的疑问
我已经有PyYAML 这个功能了，而且之前也有一个类似的问题，但是并没有告知是如何解决的，谢谢你好，可以查看下本地是否有安装python3。python2.7和python3 共存的情况下可能会出现这种问题查看下本地是否有安装python3你好，重装python也还是不行。。。应该是macos 下安装pip有点问题。https://blog.csdn.net/qimingstack/article/details/78900495可以重新安装下pip，或者手动通过安装pyyamlYAML parser and emitter for Pythonpython setup.py install是的，是这个问题，这个方案可以解决我的问题，谢谢。Choerodon平台版本: 0.10.0遇到问题的执行步骤:
根据文档安装了日志组件，安装全部顺利完成，且所有相关pod都运行正常。但发现在kibana中没有ES的数据文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/operation/logging/环境信息(如:节点信息):
4节点  4核  32g报错日志:
是否已经有应用产生日志并且正确配置收集规则呢？该集群上面部署了0.10.0版本的猪齿鱼平台所有服务Choerodon平台版本：0.10.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。
如：每次拉代码都会出现找不到jar包



你好，可以选择github 上已有tag的代码。master上的代码是我们在持续更新的。基础依赖还未发布0.7.0https://search.maven.org/search?q=g:io.choerodonChoerodon平台版本：0.10.0运行环境：PASS环境问题描述：登记工作日志，今天是11月1日，工作日期选择10月31日时无效，以前10月份是可以选到前一天的数据。
请问是有问题，还是设计是这样。您好，登记工作日志的时候日期选择是不是指的下图
你可以描述点击填写的路径，这样方便查找问题，比如点击冲刺活跃→点击看板中的卡片→打开问题详情→点击登记工作日志，然后出现什么样的问题再加上截图您好，感谢反馈，我们正在修复~Choerodon平台版本: 0.6.0遇到问题的执行步骤:
执行命令 ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml 安装k8s出错文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/
私有云部署章节环境信息(如:节点信息):
CPU 2核
内存 6G报错日志:原因分析:1、防火墙已经关闭2、通过命令 ansible-playbook -i inventory/hosts reset.yml 重置后安装报同样的错误疑问:会不会是因为配置过低造成的？这台服务器只有 6G内存、CPU是2核双线程？你好，是由于服务器配置太低导致的。根据Choerodon安装要求，安装Choerodon相对较低的服务器配置：
您可以阅读安装要求及约定到章节：http://choerodon.io/zh/docs/installation-configuration/pre-install/你好，1.请问你的服务器配置是什么样子的？例如  4台服务器，每台16G 内存，4H CPU等；
2.安装方式是采用一键安装还是分步安装目前已解决，感谢您的回答！厉害！！！Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：猪齿鱼正式环境遇到问题时的前置条件：问题描述：应用管理->修改应用名称（大写改小写）报名称已存在
原因分析：数据库大小写不敏感，校验没有去除自身疑问：如果就是这么设计的，我没啥可说的了。。是设计不周，会在下个版本进行修复，谢谢反馈客气啦Choerodon平台版本: v0.10遇到问题的执行步骤:
ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):报错日志:
TASK [etcd : Generate Ca certs] ********************************************************************
fatal: [node1 -> 192.168.56.11]: FAILED! => {“changed”: true, “cmd”: “/etc/ssl/etcd/gen_cert.sh”, “delta”: “0:00:00.005464”, “end”: “2018-11-02 13:50:35.056508”, “msg”: “non-zero return code”, “rc”: 126, “start”: “2018-11-02 13:50:35.051044”, “stderr”: “/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory”, “stderr_lines”: ["/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory"], “stdout”: “”, “stdout_lines”: []}NO MORE HOSTS LEFT *********************************************************************************
to retry, use: --limit @/vagrant/cluster.retryPLAY RECAP *****************************************************************************************
node1                      : ok=43   changed=0    unreachable=0    failed=1
node2                      : ok=38   changed=0    unreachable=0    failed=0
node3                      : ok=38   changed=0    unreachable=0    failed=0原因分析:找不到gen_cert.sh  尝试打开 用sudo vi 可以打开 但是这里找不到疑问:怎样修改这个文档的路径或者是如何修复这个问题呢你用了windows的编辑器编辑了 文件吗刚才尝试了把这个文件转换成unix格式 问题解决啦格式转换参考https://blog.csdn.net/u013686019/article/details/79268994你好，前端在一定时间没有操作，是会自动断开的。这个“一段时间”是否可以自己来设置？Choerodon平台版本：0.10.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。
现在做单点登录，如何做数据库的密码做验证？已经找到了这个用的是hash加密，是不可逆的。spring提供的BCryptPasswordEncoder的 只有encode和matches两个方法。BCrypt加密这个加密还是比较安全的，不容易破解，但是加密的过程比较耗时，性能略差Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：敏捷管理–>问题管理–>故障，无法按“标签”搜索故障，导出故障结果中也没有“标签”字段
执行的操作：
1、登录猪齿鱼平台
2、敏捷管理–>问题管理–>故障
3、查看搜索条件
4、导出故障，查看导出结果实际结果：
步骤3 搜索条件没有“标签”
步骤4 导出结果没有“标签”字段期望结果：
步骤3 搜索条件有“标签”
步骤4 导出结果有“标签”字段期望结果：
步骤3 搜索条件有“标签”
步骤4 导出结果有“标签”字段你好，在下一版本中我们会优化问题管理模块，你的意见会一并考虑。谢谢！Choerodon平台版本: 0.9.0原因分析:
虽然限制了pod的内存大小。但是java相关的应用，jvm没有做限制的话，会根据默认系统的内存来设置堆大小。我们的平台现在遇到了这个问题。这篇文章有说
https://yq.aliyun.com/articles/562440你好，可以在环境变量中添加JAVA_OPS进行限制的好的，不过猪齿鱼的java版本是8u121，不能用下面这个。
感谢，我们会对8u131进行测试，如果有效果的话会对java的依赖进行更新您好，我们0.9.0的JVM版本为8u121, 0.10.0的JVM版本已经升级到8u171参考文档 搭建prometheus监控的时候

这是grafana的deployment文件的一部分，为什么这个volumeMounts没有数据到nfs持久化存储里面，这个目录应该是挂载了的，已经有子目录grafana，但是没有数据，进容器里面是可以看的如下数据的
Hi, 看下 containers使用命令查看吗？
[root@node3 mnt]# kubectl get po monitor-grafana-8486c68865-crw7k -n monitoring -o yaml
apiVersion: v1
kind: Pod
metadata:
annotations:
choerodon.io/metrics-group: grafana
choerodon.io/metrics-path: /metrics
labels:
choerodon.io/infra: grafana
choerodon.io/metrics-port: “3000”
choerodon.io/release: monitor
pod-template-hash: “4042724421”
name: monitor-grafana-8486c68865-crw7k
namespace: monitoring
ownerReferences:grafana.ini这个文件没有映射出来，无法修改这个文件啊，现在我想添加邮件服务器，请问有其他方法吗？您好， 目前暂时无法直接修改， 你可以手动绑定一个新的卷或者configmap。
这个initContainers是做什么的，是不是执行完成后就没有了？这个只是给文件夹设置权限，执行完成后就没了k8s集群，etcd问题
为什么使用etcdctl 命令查看的时候报 etcd cluster is unavailable or misconfigured
用etcdctl进行健康检查，报unhealthy。
kubectl命令检查是正常的，请问怎么才可以使用etcdctl命令去查看etcd里的内容呢？您好，请参考这里配置Distributed reliable key-value store for the most critical data of a distributed system - etcd-io/etcdChoerodon平台版本：0.9.0运行环境：自主搭建问题描述：如题，用户故事地图拖拉史诗没有作用，拖完后，又自动回去了。你好，这个问题在0.10的版本中已经修复了。用户故事地图中的史诗卡片只能横向拖拽排序，而不能向下拖拽到任务卡片的列中。已经解决，我们也是0.10，是安装时候的问题。Choerodon平台版本: 0.9.0原因分析:
安装了choerodon-logging以后，日志里报找不到CA ，看了下choerodon-logging的charts，好像只能修改configMaps.于是做了修改如下
你修改了哪里呢[FILTER]和[OUTPUT]里面增加了
tls   Off
tls.verify Off目前暂不支持修改这个属性， 你的k8s是什么版本？version 1.11.2我现在的解决办法是：
新建了一个configmap, 把tls的配置添加进去。 修改daemonset指向的configmap为新的这个，可以解决问题。不过日志里面有这些error，是不是正常
这个 应该是你的 input里面有点问题， 检查下 parser里面是否有 spring-boot 这个parser里面没有。除了tls，其他我没有动过。
parse部分
把parser 里面的 java-spring 复制一份, 名字改为 spring-boot， 然后重启一下 fluent-bit， 你的日志是什么时候装的？就这两天这个没问题， 只是说明这个pod所在的服务器上没有这些服务的日志，ok,谢谢发现一个问题。通过猪齿鱼平台部署的这些服务，日志都没有在/var/log/containers下面。不知道放在哪里原来是docker的配置。阿里云默认没有配置成json-file如果用我们脚本装的话是 默认json-file的阿里云的容器是json-file的，我看错了。。。请帮忙分析一下kubectl logs -f gitlab-68c56cb755-94ncn -n c7n-system你添加了 gitaly 的配置 ？Choerodon平台版本：0.10.0运行环境：SaaS问题描述：点击卡片，“有时候”卡片顺序会发生变化。这个问题是因为活跃冲刺的子任务没有做排序处理，导致每次加载的排序都不确定。这个问题已经在我们的优化列表中了，会在下个版本中修复。好的，期待~~Mybatis从3.0.x版本开始，null参数的默认JDBC类型是Types.OTHER，某些JDBC驱动程序（如Oracle 10g）不支持它，所以choerodon-starter-mybatis-mapper做了如下配置
无法对JVM进行限制。好的，下一个版本我们会修复这个问题，你可以先通过pod进行限制pod不能限制JVM先限制pod的内存和cpu目前的平台环境流水线架构是这样:每一个项目需要单独创建多个环境,比如我有开发,测试,灰度,正式4个或者更多,那么通过helm在k8s上部署了4个以不同环境编码命名的agent deployment,如果一个平台下只有一两个项目倒没什么问题,像我们企业这样可能同时有上百个项目同时在进行,那岂不是要执行400多次agent安装,而且你还要避免环境编码不能跟其它项目已经使用的环境编码一样,那结果就造成我100个项目,在K8S(集如我中有一个集群)上就会创建400个不同的命名空间,每一个命名空间下运行着一个agent(因为生成的helm执行脚本里namespace跟name都是等于环境编码),一个agent我观察要消耗500M内存,这是不是太疯狂了,其实不管我在我组织下创建了多少个项目,我的需求只不过是各个项目共享这4个环境而已, C7N的架构师当初是到底是怎么考虑这个问题的?你好，这个我们已经有考虑并且正在开发新版本，下个版本一个集群将只有一个agent，然后agent真实运行的内存不超过50M，我们这个一般是二十多M左右，是申请的有点多了。是0.11.0解决这个问题还是0.10.x,我昨天部署了个agent一直没理,今天在k8s dashborad上看到内存使用380几M,不断有git clone  --mirror:fatal的日志抛出,可能这样造成内存不断增加应该是要0.11Choerodon平台版本：0.10.0运行环境：自主搭建环境helm脚本可以正常部署agent,但环境总览日志里头看到有错误信息,见图
另外devops-service部署的时候
这两个环境变量到底怎么理解env.open.SERVICES_GITLAB_URL这个值要不加上git@仓库地址.
示例是这样的 --set env.open.SERVICES_GITLAB_SSHURL="git@gitlab.example.choerodon.io" \这个环境变量中git@是不需要的。我们文档中的示范也是没有加git@的。所以是文档错了?实例管理里面，有些实例一直在处理中，实际是没用的，又无法删掉，应该是与K8S失去关联了吧，不能给个删除按钮吗？还是状态本来就没处理正确。
这是之前同步逻辑的bug，请问这个是0.10.0之后的版本还存在么？还是0.9.*版本残留的实例？应该是0.10之前，时间比较久了。还有就是这些实例状态已经是 failed 了，但是删除时提示 关联文件不存在，这个怎么解决？这个你可以 你把集群真正起作用的实例列表给我，我们这边把以前留下的脏数据清掉。1.希望能优化k8s的网络组建，使用ovs来构建网络
2.希望能提供基于ipam的ip地址池管理功能Hi，这个k8s组建范畴，我们选择比较通用的方案为用户提供一个基本的k8s搭建脚本，如果您选择定制，可根据自己的需求进行改造和扩展。 Success of install Choerodon register server.[Step 18]: install Choerodon manager service …
Error: Job failed: BackoffLimitExceeded
 Success of install Choerodon manager service.
➜ config-server not ready,sleep 5s,check it.
➜ config-server not ready,sleep 5s,check it.
➜ config-server not ready,sleep 5s,check it.
➜ config-server not ready,sleep 5s,check it.
 Choerodon config server is ready.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found看起来有问题的也显示启动容器正常进一步分析发现这个创建数据库的容器有问题这种情况一般是 节点资源占用过多导致 k8s连不上该节点docker的 api， 你可以将 node1 docker重启一下今天又部署了一次，还是出现这个问题 ，帮忙指导一下状态是这个样子的最新情况，手动pull了所有docker images之后可以起来， 出现gitlab 503的情况，日志如下看到另一个gitlab也出现情况你是在重新安装吗，如果mysql意外退出，容易导致数据出现问题， 重装的话最好把数据清理之后再重装。我是重置了服务器， 那就等于是一个新的系统环境现在看日志是这样的看下gitlab-mysql能不能连上怎么看，请帮忙给点指引到gitlab-mysql那个pod里面执行 mysql -u用户名 -p密码 试试能不能连上哈是不是这里的主机名有问题？看起来密码不对主机名不用加， 如果密码没输入错的话就是mysql安装失败了，你用了已有数据的NFS吗这些全部都是全新安装的， 我是在aliyun里面重新初始化系统之后安装的。所以不存在已有数据的情况但是我从主的mysql进去是可以的看起来gitlab的部署还是有些问题Choerodon平台版本：0.10.0运行环境：自主搭建问题描述：
搭建好环境后，部署一个应用到测试环境中，失败了，失败的日志如下：git%2540git@gitlab.51gongyi.cn:operation-daguogongchang-gitops/devp-dg.git: Connection refused (Connection refused):
也无法删除实例，删除实例的时候提示：关联的文件对象关系不存在，请检查gitops库对应文件是否存在环境变量配置错误，这里的地址是通过环境变量获取的，需要检查下服务的环境变量设置这个网址无法打开是我的网络问题还是服务器问题, 请帮忙验证一下https://file.choerodon.com.cn/choerodon-ui/fonts/icomoon.ttf是的打不开,这个竟然没打包到本地@minliacom @Bruce   Hi, 我们的服务器受到了不明来历的DDOS攻击，目前正在分析处理，请稍后再试。@minliacom @Bruce  已恢复正常，麻烦再试一下看起来可以打开了， 祝愿早日把这帮发起ddos的家伙抓起来Choerodon平台版本: 0.6.0遇到问题的执行步骤:
安装kubernetes出错环境信息(如:节点信息):
hosts改成了一个节点 指向本机ip报错日志:
原因分析:安装kubernetes显示无法连接file.choerodon.com.cn at port 443Hi, 我们的服务器受到了不明来历的DDOS攻击，目前正在分析处理，请您稍后再试。@qianchun 已恢复正常，麻烦再试一下早上坐地铁，在极客时间上突然看到这个开源项目，看了介绍之后感觉跟自己想法挺一致的，自己最近也刚好在写这一块的开源，就赶紧关注了下，本想观摩下大神的代码，但结果有点微微失望，感觉代码上有些地方还是可以优化优化的。因为只看了oauth-server的代码，说说自己的看法：
1.可以引入lombok简化代码(Gett、Sett以及日志和构造器对象创建)
2.ConvertorI内可以引入orika对对象进行灵活转换(可以通过自定义注解的方式)
3.业务逻辑中if层次太多，try()catch()太多(可以引入全局异常处理消除try()catch()，个人觉得业务逻辑中不到万不得已一般不会出现try()catch()代码,可以全部往上一层抛，由最外层进行处理)
4.安全处理这块其实可以封到统一的模块里
5.分层和命名，无法第一时间联想出一个清晰的基础架构最后希望Choerodon能越来越好，自己也会一直关注你们，也希望自己能和你们一起慢慢成长，加油。。。开源项目不适合使用 lombok吧，因为这限制了所有想参与开源开发的人员，都必须装lombok的插件。
虽然lombok插件非常好用。。（本人是他的忠实fans。。。）感谢您的关注与建议,多吸收外界的发声,知道不足我们才能成长和进步.嗯，也是呢，没怎么写过开源项目，还是欠缺考虑哈。非常感谢您的建议，代码质量这块我们还有比较多的工作要做，你提到的几个工具库，我们也会去研究，在后续的开发中看以什么方式来使用。也期待您提出更多的建议，让我们把Choerodon做的更好！代码有时间了肯定会重构优化；关于lombok要求每个人装个插件不是很方便类，自己写的项目倒是都用；ConvertorI主要解决依赖，bean拷贝我们主要用BeanUtil。如果实现一个功能需要自己写两个类实现或者引入一个jar包加个方法实现，现在可能会优先考虑自己实现。多谢支持:hugs:有思想，很高兴看到一帮都在干实事的人在一起，我们也尝试做了些模块，有涉及到 fjtnylk提到的一些点，计划后续能集成到一起，共建美好未来Choerodon平台版本：0.10运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：在我的应用管理中创建应用，一直处于创建中的状态疑问：通过模板创建的时候我gitlab中的项目是创建成功了的，但是系统中显示创建失败，并且在应用管理里面也没有，无法进行管理请点击管理，全局事务，事务实例里面查看这种事务
创建这个问题我碰过用管理员账号去设置里把分支保护关掉就行了能截个图吗？好像社区版的没有这个设置？您好,目前这个版本可以先通过更改上述去掉默认保护分支的设置后，在创建应用，下个版本我们将通过代码限制创建保护分支您好,目前这个版本可以先通过更改上述去掉默认保护分支的设置后，在创建应用，下个版本我们将通过代码限制创建保护分支您好，我今天还有一个问题，提交了代码或者增加了分支，都无法同步刚才提交那个的版本，并且后台事物报错，您好，这个空指针是当用root用户去存储pipeline数据的时候，找不到用户的bug,这个bug已经修复了，下个版本将没有这个问题。  这个问题只会影响平台开发流水线－持续集成的显示，不会影响版本生成您好，这个空指针是当用root用户去存储pipeline数据的时候，找不到用户的bug,这个bug已经修复了，下个版本将没有这个问题。 这个问题只会影响平台开发流水线－持续集成的显示，不会影响版本生成但是我提交代码或者打好分支后，我的版本库没有版本可以管理啊请截下 创建好gitlab应用库的pipelines列表图
请截下 创建好gitlab应用库的pipelines列表图您可以点击pipeline 错误stages 可以查看失败的原因，然后截下图line 错误stages 可以查看pipeline ci 不仅仅单纯跑mvn build ,它还包含其它一些前置条件，比如runner分配pod跑测试，校验权限等
retry this job 试一下版本是0.9.apiVersion: v1
kind: Service
metadata:
name: {{ template “prometheus.fullname” . }}  # 这个名称使用的模板
labels:
app: {{ template “prometheus.name” . }}
chart: {{ template “prometheus.chart” . }}
release: {{ .Release.Name }}
heritage: {{ .Release.Service }}
spec:
type: {{ .Values.service.type }}
ports:
- port: {{ .Values.service.port }}
targetPort: http
protocol: TCP
name: http
selector:
app: {{ template “prometheus.name” . }}
release: {{ .Release.Name }}{{- define “prometheus.fullname” -}}
{{- if .Values.fullnameOverride -}}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix “-” -}}
{{- else -}}
{{- $name := default .Chart.Name .Values.nameOverride -}}
{{- if contains $name .Release.Name -}}
{{- .Release.Name | trunc 63 | trimSuffix “-” -}}
{{- else -}}
{{- printf “%s-%s” .Release.Name $name | trunc 63 | trimSuffix “-” -}}
{{- end -}}
{{- end -}}
{{- end -}}问题是如果我使用 .Values.fullnameOverride 定义了 SVC 的名称后在页面上的"网络"里就看不到这个 SVC,域名里也没有.但是在"实例详情"里是可以正确看到应该存在的 SVC.这个问题让我们没办法固定住一个 SVC 名称.顺便是否有计划增加在部署时可以自选实例名称的功能?
即让用户填写实例名称,而不是平台自动计算出的值.这个意思是部署后的实例名与平台中存的实例名不一样，这样的话平台是收集不到的，在0.9版本中，你可以通过最直接在GitOps配置库中创建实例，这样是可以直接指定实例名的。我现在碰到一个问题,helm 的 chart 我直接使用 helm install 时一切如预期那样,但是使用猪齿鱼进行部署时像是忽略了某些值一样.我的 values.ymal 是这样的.values.yamltags:
elasticsearch: true
zookeeper: false这两个是我的 chart 依赖的两个子chart,预期是不启用 zookeeper 只启用 es.这个预期在直接使用 helm install 时是没问题的.但是猪齿鱼部署时就会将 zookeeper 也启动,我看了 k8s 接收到的模板是有 zookeeper .同样把elasticsearch设定为 false 也是无效的,同样会启动.requirements.yamldependencies:这是依赖定义文件,之前使用的是 condition: xck-elasticsearch-5.enabled 这种方式也同样 helm 命令正确猪齿鱼部署这个开关就无效.请问这是什么问题?猪齿鱼对 values.yaml 有什么要求吗?还是对于依赖这种开头有特殊要求?请问一下你用的choerodon是哪个版本？使用的是0.9版本.requirements.yaml这种复杂类型chart我们平时用的不多，我们先测试一下。如果这边有问题我们尽快发个补丁修复。好的,我的依赖是这样的.
xck-skywalking -> xck-elasticsearch  开关无效你好，前段时间在忙别的，这个问题我们已经修复了，你们现在用的版本是几，可以发一个hotfix版本。现在使用的是0.9版本,还没有用0.10.能说明一下具体修改了什么吗?因为我们也有一些自己的修改,所以我们可能需要做一些合并.可以啊，这个改的是choerodon-agent，就加了两行代码，我可以基于0.9.X的版本发一个新的，然后你们只需要改一下，devops服务的环境变量就行了，然后0.9.X的agent，我建议你用0.9.9，修复了一些之前的bug，是和0.9.X的devops服务兼容的，https://github.com/choerodon/choerodon-agent/commit/8421cf8d9e7d017638da8dce8c37b6730352d664非常感谢.对了.这个修复你会在0.9.9上修复吗?还是发一个新版?对了.这个修复你会在0.9.9上修复吗?还是发一个新版?我生成一个0.9.92的tag吧，应为0.9.x的版本取名0.9.10会有问题。
这边我已经生成0.9.92的tag，等代码同步到github出现0.9.92tag就可以了。
对于0.10的平台也有会0.10.x的版本。经过测试,依赖开关已经可以正确工作了.谢谢.Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：
获取修改配置错乱。出现过多次（包括其他应用）。
下图里面操作的应用x-zuul-gateway，但是拿到的配置是phoneix-config-app
我们现在在同一个项目下部署了27个应用。然后使用的猪齿鱼平台是由另一个猪齿鱼平台部署的，会局部更新一些猪齿鱼的组件。更新devops-service会不会有影响？可以通过平台更新devops-service，修改配置错乱的devops服务版本是哪个版本，有截图吗。v0.9.3你好，可以麻烦看一下数据库中，此应用版本，对应app version value不是代码库里面values文件对应的内容而是另一个应用的，如果是这样的话，应该是生成应用版本的时候就由于代码问题存错了。Choerodon平台版本: 0.10遇到问题的执行步骤:
LDAP配置
报错日志:
确认openldap server可用。在其他平台上都使用正常。怀疑是不是绑定的字段有问题？
不好意思，这是我们的一个bug，这个地方去查mail=test的查不到抛了个NamingException

然后就跳出循环了，导致登陆不上去，我们修改下这块逻辑，感谢反馈好的。麻烦到时候给个修复方法。谢谢！我连WINDOWS AD同样有这个问题，是否是同一个BUG,可以确定管理员帐号和密码没有问题
018-10-18 15:19:14.904  INFO [iam-service,13d056249ceb3ff4,4fe3f01aed2c4e86,true] 1 — [XNIO-3 task-133] i.c.i.infra.common.utils.ldap.LdapUtil   : ldap search fail: {}javax.naming.NamingException: [LDAP: error code 1 - 000004DC: LdapErr: DSID-0C0907C2, comment: In order to perform this operation a successful bind must be completed on the connection., data 0, v2580]
at com.sun.jndi.ldap.LdapCtx.mapErrorCode(LdapCtx.java:3194) ~[na:1.8.0_171]
at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:3100) ~[na:1.8.0_171]
at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:2891) ~[na:1.8.0_171]
at com.sun.jndi.ldap.LdapCtx.searchAux(LdapCtx.java:1846) ~[na:1.8.0_171]
at com.sun.jndi.ldap.LdapCtx.c_search(LdapCtx.java:1769) ~[na:1.8.0_171]
at com.sun.jndi.toolkit.ctx.ComponentDirContext.p_search(ComponentDirContext.java:392) ~[na:1.8.0_171]
at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:358) ~[na:1.8.0_171]
at com.sun.jndi.toolkit.ctx.PartialCompositeDirContext.search(PartialCompositeDirContext.java:341) ~[na:1.8.0_171]
at javax.naming.directory.InitialDirContext.search(InitialDirContext.java:267) ~[na:1.8.0_171]
看日志应该是一个问题，这个问题已经记录下来，我们将尽快修复我发现ldap这块还有很多问题，我调试了下源码 ldap search fail: {}并不是ldapcontext.search查不到抛异常，而是ldapConnect(String url, String baseDn, String port, Boolean useSSL)这个方法是通过匿名的方式去创建的连接，直接就被WINDOWS AD拒了，我不知道其它的ldap服务是不是充许匿名binding，但AD默认肯定是不可以匿名的，且前端设置的管理员用户和密码就没拿去作连接验证，另外一个还是ldap兼容性问题：“源码注释： 根据ldap配置和用户名从ldapContext中搜寻该用户是否存在和得到userDn“”，AD验证不能用userDn格式作为用户（至少我的环境是不通过的），它的格式是user@domain这种格式，所以getUserDn（）这方法对AD就没啥作用了，毕竟大部分企业用的是WIN AD吧这块确实有一些问题， 我们正在做测试和修复，旧的代码本地搭了个active directory服务做测试，确实是无法通过认证的，现在已经改了代码可以认证了，但是还要做一些测试和完善，感谢指正还有点非常不解的是，既然平台支持ldap认证了为什么还要手动同步用户信息到平台数据库，企业人员变动非常频繁，不可能经常去同步操作，你们可以了解下Harbor Registry AD/ldap的集成方案，其实大部分软件也是这么做的，用户登陆验证通过自动获取AD/ldap相关属性并更新到本地数据库，再进一步做角色设定，c7n也完全可以自动同步用户邮箱电话这些信息过来，根本不需要管理员手动同步用户这个操作啊你好，你说的这种我们也有考虑过，不过choerodon这边的用户同步还会涉及到后面gitlab，wiki等等一系列的操作。并且用户同步过来还是需要对用户进行手动分配角色的操作。有关于手动同步的问题，我们在新的版本中添加了定时任务，可以设定系统自动定时来同步ldap用户，避免了手动操作。Choerodon平台版本：0.10.0运行环境：PASS环境问题描述：今天早上开始，任务无法更新状态执行的操作：
如:给成员分配了项目所有者、项目成员、部署管理员三个角色，在：活跃冲刺-选中问题- 指派人员，无法选择到该成员。
已维护项目成员30人，只有3人无法选择到，其他成员都能够在指派人员中选择到。报错信息(请尽量使用代码块或系统截图的形式展现)：
建议：提出您认为不合理的地方，帮助我们优化用户操作你好，请刷新页面确认下问题是否已经被删除了找到问题了 我们会尽快修复 感谢反馈你好 该问题已经修复了， 请再重试一下你好，任务无法更新状态的问题，我们已经修复了。
对于分派找不到人员的问题，请先在项目设置-项目角色分配确认人员已经分派了角色。然后分派时，直接搜索工号或者名字试试，看能否选到人员。已解决Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：我们这边有一个需求，想通过Choerodon管理应用。但是我们的应用目前运行方式是通过tomcat来运行的，程序打包后是一个war包。可是我们发现在Choerodon上并不能找到如果部署war的方式，所以前来社区问问，是否Choerodon能够通过自定义应用模板等方式完成war包的部署？你可以选择微服务模板，gitlab-ci中添加构建war包的命令,移除原构建jar的命令, 修改 /src/main/docker/Dockerfile 使用 tomcat作为基础镜像， 并在dockerfile中将war拷到tomcat对应位置您好，我目前使用的是微服务的模板，因此项目中chart文件夹中也是使用的微服务的那一套路。结果现在部署应用到k8s上的时候，报了以下错误，我感觉是要对chart中的文件进行修改。错误信息如下：
如果你无需在应用启动前执行命令或者更新数据库，直接把 chart中的 job删除即可Choerodon平台版本：0.10运行环境：一键搭建问题描述：我现在需要创建一个项目，到那时发现跟文档描述的完全不一样，完全没办法进行下一步，请问需要配置其他的东西么，现在这是我项目的首页优化：
页面session过期后没有跳转至登陆处理，操作一直403，有时候让人莫名其妙你好，按照文档里的应该是可以创建项目的
http://choerodon.io/zh/docs/quick-start/admin/project/有关于session 失效的问题，可以参考这一篇发布应用的时候如何利用现有集群中的负载均衡服务你这边是部署在云上，还是idc中，因为loadbalancer类型的service用的较少，所以目前发布的版本没有做支持，可以在接下来的版本支持目前版本还未支持，已经记录该需求，会在后面的迭代中进行更新支持谢谢，非常期待[vagrant@node1 vagrant]$ helm version
Client: &version.Version{SemVer:“v2.8.2”, GitCommit:“a80231648a1473929271764b920a8e346f6de844”, GitTreeState:“clean”}
Error: cannot connect to Tiller
[vagrant@node1 vagrant]$kubectl get po -n kube-system[vagrant@node1 vagrant]$ kubectl get po -n kube-system
NAME                                        READY     STATUS    RESTARTS   AGE
default-http-backend-6dd4d5b7c9-z26lv       1/1       Running   0          3h
heapster-746d67c7b9-28xqc                   1/1       Running   0          3h
kube-apiserver-node1                        1/1       Running   0          3h
kube-controller-manager-node1               1/1       Running   0          3h
kube-dns-79d99555df-cflbt                   3/3       Running   0          3h
kube-flannel-d8fql                          2/2       Running   0          3h
kube-lego-6f45757db7-7dxxn                  1/1       Running   0          3h
kube-proxy-7vpbz                            1/1       Running   0          3h
kube-scheduler-node1                        1/1       Running   0          3h
kubernetes-dashboard-dc8fcdbc5-z5kjf        1/1       Running   0          3h
nginx-ingress-controller-5d77d4945d-kwwnw   1/1       Running   0          3h
[vagrant@node1 vagrant]$是否执行过 helm init ?没有，我多执行了几遍就好了，太神奇了。正常情况下 你在 kube-system 可以看到 tailler 的服务。您好，我在搭建过程中没有看到tiller-deploy，怎么解决呢？我也遇到helm version卡住不动的情况了，求助。
请问你是执行的下面语句初始化helm的吗是的，我发现问题了，使用这个命令之前，我没有执行过以下两个命令：
kubectl create serviceaccount --namespace kube-system helm-tillerkubectl create clusterrolebinding helm-tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:helm-tiller现在重新安装后，可以了。谢谢您。Choerodon平台版本: 0.6.0遇到问题的执行步骤:
1、参考服务器安装模式安装部署K8S无法执行kubectl命令
步骤1 ：部署前均参考文档完成执行正常
步骤2：inventory/hosts 修改如下：
[root@sonarqube kubeadm-ansible]# vi inventory/hosts
[all]
node1 ansible_host=192.168.11.166 ip=192.168.11.166 ansible_user=root ansible_ssh_pass=handdba ansible_become=true
node2 ansible_host=192.168.11.168 ip=192.168.11.168 ansible_user=root ansible_ssh_pass=handdba ansible_become=true
node3 ansible_host=192.168.11.169 ip=192.168.11.169 ansible_user=root ansible_ssh_pass=handdba ansible_become=true[kube-master]
node1[etcd]
node1[kube-node]
node1
node2
node3步骤3：执行ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml正常，无失败信息
步骤4：kubectl get po -n kube-system执行提示kubectl ：command not found文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):
参考hosts修改，操作服务器为青浦机房服务器报错日志:
暂无原因分析:配置错误？
需要优先完成端口号开发吗？尚未完成http://choerodon.io/zh/docs/installation-configuration/pre-install/#%E9%9C%80%E5%BC%80%E6%94%BE%E7%9A%84%E7%AB%AF%E5%8F%A3%E5%8F%B7文档中的端口号处理疑问:暂无kubectl get po -n kube-system请问你执行kubectl命令时是在master节点上面执行的吗？您好，是的呢，我就是在192.168.11.166机器上执行的我现在重新按reset失败部署，处理再执行了，现在报端口拒绝
[root@sonarqube kubeadm-ansible]# kubectl get po -n kube-system
The connection to the server localhost:8080 was refused - did you specify the right host or port?请在master节点执行下面命令  查看是否有报错日志journalctl -n 100 -f -u kubeletOct 12 08:55:40 node1 systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 12 08:55:40 node1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 12 08:55:40 node1 systemd[1]: Starting kubelet: The Kubernetes Node Agent…
Oct 12 08:55:40 node1 kubelet[21880]: I1012 08:55:40.701807   21880 feature_gate.go:156] feature gates: map[]
Oct 12 08:55:40 node1 kubelet[21880]: I1012 08:55:40.701923   21880 controller.go:114] kubelet config controller: starting controller
Oct 12 08:55:40 node1 kubelet[21880]: I1012 08:55:40.701934   21880 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 12 08:55:40 node1 kubelet[21880]: error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory
Oct 12 08:55:40 node1 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 12 08:55:40 node1 systemd[1]: Unit kubelet.service entered failed state.
Oct 12 08:55:40 node1 systemd[1]: kubelet.service failed.报kublet主线程退出
Oct 12 08:55:40 node1 systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 12 08:55:40 node1 systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 12 08:55:40 node1 systemd[1]: Starting kubelet: The Kubernetes Node Agent…
Oct 12 08:55:40 node1 kubelet[21880]: I1012 08:55:40.701807   21880 feature_gate.go:156] feature gates: map[]
Oct 12 08:55:40 node1 kubelet[21880]: I1012 08:55:40.701923   21880 controller.go:114] kubelet config controller: starting controller
Oct 12 08:55:40 node1 kubelet[21880]: I1012 08:55:40.701934   21880 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 12 08:55:40 node1 kubelet[21880]: error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory
Oct 12 08:55:40 node1 systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 12 08:55:40 node1 systemd[1]: Unit kubelet.service entered failed state.
Oct 12 08:55:40 node1 systemd[1]: kubelet.service failed.请问你主机配置是怎么样的呢？  cpu、内存、硬盘大小等信息[root@sonarqube kubeadm-ansible]# top
top - 09:16:10 up 16:22,  2 users,  load average: 0.00, 0.01, 0.05
Tasks:  99 total,   1 running,  93 sleeping,   5 stopped,   0 zombie
%Cpu(s):  2.0 us,  0.5 sy,  0.0 ni, 97.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  8009704 total,  6664672 free,   247444 used,  1097588 buff/cache
KiB Swap:  2097148 total,  2097148 free,        0 used.  7391708 avail Mem方便的话您可以ssh登录下看看，青浦机房的服务器嗯  稍等  我上去看看，由于涉及敏感信息，我先删除楼上。好的，感谢感谢呢。方便的话电话或微信联系，文建东，微信89699049发现的问题：
1.在windows主机上克隆和编辑kubeadm-ansible项目代码，导致出现下面错误，现已重新克隆代码。
2.node1节点开启防火墙，但又没有允许需开放的端口暴露出来，导致安装时健康检查出错
3.由于你所提供的三台服务器为openstack创建的虚拟机，在openstack中的floating-ip是不会在虚拟机上创建一个新网卡的，而是通过l3-agent的转发实现。故你不能使用192.168.11.166-168-169这三个ip进行集群部署。应使用下面命令查看真实内网ip，选择eth0网卡的ip故正确的host文件应为4.node3节点无法访问node1节点6443端口，请联系it人员进行解决
感谢感谢呢！node3我直接先去除了，暂不部署了，当前部署已完成。麻烦再问下，我要在master上加dashboard界面的话，需要再怎样操作呢？若知请指导，谢谢。已为你设置通过node2进行访问dashboard界面https://192.168.11.168/#!/logintoken已私信于你您好！感谢感谢，访问没有问题了，有劳您把设置方法发下我把，这边需要复盘部署文档编写说明:wink:非常感谢您的再次回复，谢谢通过kubeadm-ansible项目脚本部署集群默认是安装了dashboard了的，这里只需要将dashboard的svc开放出来就好，比较简单粗暴的方式就是在dashboard的svc上直接添加external-ips属性。执行编辑命令：详细介绍请查看这里：https://kubernetes.io/docs/concepts/services-networking/service/#external-ips通过kubeadm-ansible项目脚本部署集群默认是安装了dashboard了的，这里只需要将dashboard的svc开放出来就好，比较简单粗暴的方式就是在dashboard的svc上直接添加external-ips属性。执行编辑命令：详细介绍请查看这里：https://kubernetes.io/docs/concepts/services-networking/service/#external-ips好的，好的，非常感谢呢，谢谢！您好，再麻烦问下，那对应的token从哪里获取呢？修改添加externalIPs属性后，是否需要重启或怎样操作处理呢？Choerodon平台版本：0.10.0运行环境：SaaS问题描述：待办事项中对应的冲刺的数量和活动冲刺中的数量不一致。具体是“确定北京敏捷沙龙知识分享内容”在活动冲刺中有，但是在待办事项的冲刺中却没有。参见截图：待办事项截图：
活动冲刺截图：
你好，待办事项不显示已完成的问题，所以两个页面上的问题数量会不一致但是，我这个任务已经完成了，还在上面。如果我想把已经完成的，从活动冲刺中去掉，该如何操作呢？目前只能手动修改问题的冲刺，或者完成该冲刺，才能将已完成的问题去除掉Choerodon平台版本：0.10运行环境：自主搭建通过自带的应用模版创建应用，约10几秒刷新状态显示失败，获取不到git地址，
但是打开gitlab发现模版代码已经上传，包括平台新建项目的时候，gitlab也能正常创建对应的群组，该如何排查？
devopsservice及gitlabservice非常多类似以下的日志讯息
2018-10-19 16:44:44.744 ERROR [devops-service,] 1 — [scoveryClient-2] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null
at java.util.concurrent.FutureTask.get(FutureTask.java:205)
at com.netflix.discovery.TimedSupervisorTask.run(TimedSupervisorTask.java:63)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)2018-10-19 16:45:44.806 ERROR [devops-service,] 1 — [scoveryClient-0] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null
at java.util.concurrent.FutureTask.get(FutureTask.java:205)
at com.netflix.discovery.TimedSupervisorTask.run(TimedSupervisorTask.java:63)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)2018-10-19 16:45:44.866 ERROR [devops-service,] 1 — [scoveryClient-1] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null：2018-10-19 16:44:44.744 ERROR [devops-service,] 1 — [scoveryClient-2] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null
at java.util.concurrent.FutureTask.get(FutureTask.java:205)
at com.netflix.discovery.TimedSupervisorTask.run(TimedSupervisorTask.java:63)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)2018-10-19 16:45:44.806 ERROR [devops-service,] 1 — [scoveryClient-0] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null
at java.util.concurrent.FutureTask.get(FutureTask.java:205)
at com.netflix.discovery.TimedSupervisorTask.run(TimedSupervisorTask.java:63)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)2018-10-19 16:45:44.866 ERROR [devops-service,] 1 — [scoveryClient-1] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null2018-10-19 16:47:51.488 ERROR [gitlab-service,] 1 — [scoveryClient-1] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null
at java.util.concurrent.FutureTask.get(FutureTask.java:205) [na:1.8.0_171]
at com.netflix.discovery.TimedSupervisorTask.run(TimedSupervisorTask.java:63) ~[eureka-client-1.6.2.jar!/:1.6.2]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_171]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_171]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_171]
at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]2018-10-19 16:47:51.644 ERROR [gitlab-service,] 1 — [scoveryClient-2] c.netflix.discovery.TimedSupervisorTask  : task supervisor timed outjava.util.concurrent.TimeoutException: null
at java.util.concurrent.FutureTask.get(FutureTask.java:205) [na:1.8.0_171]
at com.netflix.discovery.TimedSupervisorTask.run(TimedSupervisorTask.java:63) ~[eureka-client-1.6.2.jar!/:1.6.2]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_171]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_171]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_171]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_171]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_171]
at java.lang.Thread.run(Thread.java:748) [na:1.8.0_171]2018-10-19 16:47:52.492  WARN [gitlab-service,] 1 — [eshExecutor-747] com.netflix.discovery.DiscoveryClient    : Not updating application delta as another thread is updating it already又偿试了一次创建 应用，这次看到了如下日志
gitlab-service的日志
你还，创建应用不止创建代码库一个步骤，可以管理菜单中事物实例重试，也可以删除再重新创建。没用,重新创建也同样的提示好我们看一下原因，是每个应有都创建失败吗，能删除成功吗。你好，我找到原因gitlab访问日志10.3.254.22 - - [20/Oct/2018:13:46:01 +0800] “POST /api/v4/projects/132/protected_branches HTTP/1.1” 409 54 “” “Jersey/2.25.1 (HttpUrlConnection 1.8.0_171)”
409状态码，提交保护分枝有冲突，devops-service也提示message":“Protected branch ‘master’ already exists”，因为我的gitlab是原有企业就部署好的，默认开启了分支保护，关掉这个功能就可以创建成功了嗯 了解Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。部署一个ngnix的时候 dns解析异常。
我查了所有的pod，dns server都svc地址。内部解析都正常，但是使用了gitlad的ci功能后，pod中dns默认为1114.114.114.114执行的操作：
参考官方文档部署一个nginx代码一样。报错信息(请尽量使用代码块或系统截图的形式展现)：
建议：提出您认为不合理的地方，帮助我们优化用户操作我们的runner使用的主机的DNS， 你需要将主机的 DNS server修改为你自己的DNS server ，cat /etc/resolv.conf (ps:是否能直接修改此文件，请咨询您的云服务商)所有node节点的dns server就是其中master节点，域名也可以正常解析的
[root@node1 ~]# for i in cat k8s.list| awk '{print $1}';do echo “#######$i#########”;cat /etc/resolv.conf;done
#######10.201.143.51#########
nameserver 10.201.143.51
nameserver 10.201.143.150
#######10.201.143.52#########
nameserver 10.201.143.51
nameserver 10.201.143.150
#######10.201.143.53#########
nameserver 10.201.143.51
nameserver 10.201.143.150
#######10.201.143.54#########
nameserver 10.201.143.51
nameserver 10.201.143.150
#######10.201.143.55#########
nameserver 10.201.143.51
nameserver 10.201.143.150我刚刚也捕获到了runner的dnserver，确实是kube-dns的svc地址，但是为什么报错还显示114.114.114.114？
这个dns到底是从哪里继承来的？node节点还是docker本身了？
你可以在一台节点上 docker run 一个容器 ，然后 ping一下这个gitlab的域名这样解析是对的你的runner在那个主机上运行的， 你重新执行下 ci, 然后看下 新建的pod在哪个主机Choerodon平台版本: 0.10遇到问题的执行步骤:
根据文档安装到第六步，发现有些地方不明白，且出现以下问题了文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/gitlab-runner/环境信息(如:节点信息):
master节点报错日志:
token要按照文档中的位置获取哦没有此页面，已联系timebye处理出现了几个问题 ，请帮忙排查一下先看下 各个服务的POD是不是在正常运行中所有服务的POD都是Running中结果都一致正在尝试重新安装出现新情况及时汇报上来哈。好像这会的aliyun mirror挂了但是在服务器上能ping 通，看起来像是 http服务不通你用的阿里云服务器吗，有修改它的 dns吗 和 yum.repo 吗 ， 阿里云主机一般会从内网拉取我在服务器上添加了个 nameserver 8.8.8.8 就挂了，删掉就好了你这定位问题怎么可以这么精准，太nb了！！！此问题在于通过shell自动安装时做了多余的转义，以变量形式进入到cm时不需要转义域名  example\.choerodon\.com
此前专门写了脚本将输入的域名转义成这个样子，改成正确的域名就ok了谢谢大大的支持通过一个多月安装文档的提练与实践，干了个脚本出来，有需要的私聊哈欢迎共享给Choerodon猪齿鱼社区:grinning:Choerodon平台版本: 0.10遇到问题的执行步骤:
一键安装gitlab获取token异常文档地址:环境信息(如:节点信息):报错日志:
[Step 28]: get gitlab token …% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current100   195    0     0  100   195      0     12  0:00:16  0:00:15  0:00:01     0100   195    0     0  100   195      0      6  0:00:32  0:00:32 --:–:--     0rm: cannot remove ‘/tmp/cache-gitlab.jar’: No such file or directory% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current100   107    0     0  100   107      0    113 --:–:-- --:–:-- --:–:--   113rm: cannot remove ‘/tmp/cache-gitlab.jar’: No such file or directory% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current100   250    0     0  100   250      0      7  0:00:35  0:00:32  0:00:03     0 自动获取Gitlab Impersonation Token失败，请手工获取后继续部署。 http://gitlab.example.choerodon.io请输入手工获取的Gitlab Impersonation Token:➜ Gitlab Impersonation Token: Success check Gitlab token. Gitlab token:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请问这一步你有设置过吗？http://choerodon.io/zh/docs/installation-configuration/steps/dns/已写host解决
谢谢首先看到gitlab无法访问到后端服务wiki情况和gitlab类似看起来http://c7n.hi.choerodon.io这个域名下面的内容都正常你好  请排查一下这一步是否配置正确http://choerodon.io/zh/docs/installation-configuration/steps/dns/执行这一句的时候出现以下情况：请使用主机的内网IP执行完后有时会正常，有时候的安装出现其它问题出来。
见： http://forum.choerodon.io/t/topic/1034Choerodon平台版本: 0.9.0遇到问题的执行步骤:
之前安装好之后，访问gitlab是可以访问的，之后配置 启用SSH协议之后，就访问不了；
卸载重新部署也是没有用，之后整个猪齿鱼平台全部卸载，重新安装也是不行；文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/环境信息(如:节点信息):
腾讯云三台机器报错日志:
第一张：
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
第一点：我使用helm install c7n/gitlab \ 之后，他只有COnfigMap ,Deployment,ingress,和pod 没有Service
的部署；所有我怀疑是这个问题，可是不清楚为什么没有，
第二点：这个远程仓库的疑问:提出您对于遇到和解决该问题时的疑问你好，我们现在先解决浏览器访问503的问题，请执行以下语句进行svc部署谢谢大神v0.10.0版本
执行完成以上语句之后出现如下错误，请帮忙分析一下硬件资源由4台不同的16G内存的局域网主机组成
IP分别为
192.168.43.11,12,13,14
192.168.43.11作为主节点[kube-master]
node1[etcd]
node1[kube-node]
node2
node3
node4Warning  FailedScheduling  2m (x24 over 8m)  default-scheduler  No nodes are available that match all of the predicates: Insufficient memory (3), PodToleratesNodeTaints (1).kubectl describe po -n c7n-system kafka-0
Name:           kafka-0
Namespace:      c7n-system
Node:           
Labels:         choerodon.io/infra=kafka
choerodon.io/release=kafka
controller-revision-hash=kafka-8688599b89
Annotations:    kubernetes.io/created-by={“kind”:“SerializedReference”,“apiVersion”:“v1”,“reference”:{“kind”:“StatefulSet”,“namespace”:“c7n-system”,“name”:“kafka”,“uid”:“f33405b9-d102-11e8-853b-08002737f846”,"apiVers…
Status:         Pending
IP:
Created By:     StatefulSet/kafka
Controlled By:  StatefulSet/kafka
Containers:
kafka:
Image:  registry.saas.hand-china.com/tools/kafka:1.0.0
Port:   9092/TCP
Command:
sh
-c
./bin/kafka-server-start.sh config/server.properties --override zookeeper.connect=zookeeper-0.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.c7n-system.svc.cluster.local:2181 --override log.dirs=/opt/kafka/data/logs --override num.partitions=10 --override message.max.bytes=6525000 --override socket.request.max.bytes=104857600 --override log.cleanup.policy=delete --override log.cleaner.enable=true --override log.retention.hours=72 --override log.retention.bytes=10485760 --override default.replication.factor=3 --override broker.id=${HOSTNAME##*-}
Limits:
memory:  2560Mi
Requests:
memory:     2Gi
Liveness:     exec [bin/kafka-topics.sh --zookeeper zookeeper-0.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.c7n-system.svc.cluster.local:2181 --list] delay=240s timeout=5s period=10s #success=1 #failure=5
Readiness:    exec [bin/kafka-topics.sh --zookeeper zookeeper-0.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.c7n-system.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.c7n-system.svc.cluster.local:2181 --list] delay=30s timeout=5s period=10s #success=1 #failure=5
Environment:  
Mounts:
/opt/kafka/data from kafka (rw)
/var/run/secrets/kubernetes.io/serviceaccount from default-token-jghv9 (ro)
Conditions:
Type           Status
PodScheduled   False
Volumes:
kafka:
Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
ClaimName:  kafka-kafka-0
ReadOnly:   false
default-token-jghv9:
Type:        Secret (a volume populated by a Secret)
SecretName:  default-token-jghv9
Optional:    false
QoS Class:       Burstable
Node-Selectors:  
Tolerations:     
Events:
Type     Reason            Age               From               MessageWarning  FailedScheduling  8m (x2 over 8m)   default-scheduler  PersistentVolumeClaim is not bound: “kafka-kafka-0” (repeated 4 times)
Warning  FailedScheduling  2m (x24 over 8m)  default-scheduler  No nodes are available that match all of the predicates: Insufficient memory (3), PodToleratesNodeTaints (1). Success create database for Choerodon.[Step 15]: install kafka …
NAME:   kafka
LAST DEPLOYED: Tue Oct 16 13:18:43 2018
NAMESPACE: c7n-system
STATUS: DEPLOYEDRESOURCES:
==> v1/Service
NAME            TYPE       CLUSTER-IP  EXTERNAL-IP  PORT(S)   AGE
kafka-headless  ClusterIP  None               9092/TCP  0s==> v1beta1/StatefulSet
NAME   DESIRED  CURRENT  AGE
kafka  3        1        0s==> v1/Pod(related)
NAME     READY  STATUS   RESTARTS  AGE
kafka-0  0/1    Pending  0         0s Success of install kafka.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.
➜ kafka not ready,sleep 5s,check it.初步看起来不说内存不足, 待进一步调查结果从node1里面看起来内存空闲得有8gfree -m
total        used        free      shared  buff/cache   available
Mem:          13632         870        8384          10        4377       12363从日志里面没看出来是哪个节点出现内存不足的情况, 请大神帮忙睇睇看样子是 你把其中的一台禁用了哦是说节点被禁用了吗, 我没有作类似操作啊你的kube node只有三台机器， 内存不够分了， 你可以 describe no 看下 每个服务占用的内存情况那应该是让MASTER也成为节点吗我先改成这样子试试看这样子和上面的配置哪个适合在我这个环境跑起来一些比较推荐后边的这一种我先按后面这种配置跑一遍放弃本地治疗，转移到阿里云上云了。等部署成功后再来汇报。谢谢大大支持。Choerodon平台版本: 0.9.0疑问:
这个K8S集群既安装猪齿鱼平台，也创建环境用于部署应用。今天想添加一个节点。参照http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/这个教程来弄。因为master的那个节点ssh端口被改掉了。所以inventory/hosts的修改包括增加新节点信息和master的那个节点端口也指定了一下
inventory/vars保持原样，没有修改
执行如下添加节点命令
ansible-playbook -i inventory/hosts -e @inventory/vars scale.yml -vvvvvv日志卡在这里就不动了
请检查inventory/hosts文件中新增节点的配置信息是否正确，执行ansible命令的节点是否能够ssh登录新节点。配置没有问题，ssh可以登录。
今天我把那个master节点的ssh端口改回22，还是这样。报的错误是
Choerodon平台版本: 0.10遇到问题的执行步骤:
第一次安装遇到一直Kafka not ready,sleep 5s,check it，然后重新执行脚本卸载安装又一直出zookeeper not ready,sleep 5s,check it，不知道原因，也没有日子可以查找，很是崩溃文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/choerodon/环境信息(如:节点信息):报错日志:
zookeeper not ready,sleep 5s,check itkubectl get po -n c7n-system | grep nfs这个问题出现在使用helm删除zookeeper时，helm执行删除后，但是由于服务器性能原因未能快速删除zookeeper对应容器导致的。按照脚本提示执行helm命令删除后，请使用以下命令进行检查是否删除完毕这个问题出现在使用helm删除zookeeper时，helm执行删除后，但是由于服务器性能原因未能快速删除zookeeper对应容器导致的。好的，我试试这些已经处于删除状态了    请等待他删除完毕你也可以重启一下每个节点的docker,然后再看看删除完毕没有，才重启完docker后 执行kubectl命令可能会报错，请耐心等待一会儿再执行。ok，我弄一下，有问题再咨询您:grin:那些pods还是存在呢？有没有强制删除的办法呢重启docker后还是一直删不掉吗？对，我直接重启了服务器:laughing:接重启了服务器:laughing:还有个问题。麻烦看下kubeadm-ansible/inventory/vars 变量文件中你添加这变量了吗？http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/嗯，添加了这是阿里云上不需要的部署，直接执行下面命令删除即可kubectl delete ds kube-flannel -n kube-systemok，还有一个问题，首先请将一个泛域名映射到集群中任意一个master节点公网IP（比如：node1的公网IP）情况一：
启用Gitlab SSH功能时进行配置的就是node1那么就不需要其他映射了。情况二：
启用Gitlab SSH功能时进行配置的不是node1那么就还需要将gitlab的域名指向该节点。多谢。。。建议你们整个QQ群或者微信群:sweat_smile:那样更可以吸纳人员，处理过问题的也可以帮你们解决问题，活跃度更高。。不过也很感谢你们及时的反馈我们的提问Choerodon平台版本：0.10运行环境：自主搭建问题描述：
组织管理中并不能新建组织，还是说没开放这个功能你好，目前只内置运营组织一个组织。新建组织相关的功能没有开放。Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：自主搭建问题描述：
我们这边接到一个需求，是要实时获取到猪齿鱼平台和基于猪齿鱼开发框架构建的K8S集群内部所有的pod的日志及cpu、内存等资源信息。我目前了解到日志信息是都存在ES里的，资源信息是存在prometheus里面的。我推断这些信息应该也都是从K8S的api获取到的，大佬们是否可以讲解一下具体获取这些信息的原理及方法吗？想要达到的效果是不使用ES和prometheus等直接获取监控数据。prometheus是google推出的监控工具，具有强大的聚合计算功能，与k8s具有良好的兼容性，不建议你切换到其他工具，监控信息一般在应用服务 /metrics 路径下能够看到 ，具体地址你可以在prometheus 界面中找到。大佬 ，你好，可以举个例子演示一下吗，没有用过prometheus下面是prometheus的架构图
prometheus 主动拉取性能数据，经过proemtheus处理之后存储在它自身的数据库中，它提供了PromQL语句进行查询，具体查询语句你可以在 官方文档中找到
https://prometheus.io/docs/prometheus/latest/querying/basics/
iam模块的依赖 用的是0.8.0版本的，在调查询语言的时候SQL报错
我们是在0.10.0版本中对oracle进行支持

如果想使用oracle数据库请升级到0.10.0，改了不少不规范的sql0.10.0 要怎么下载github上有0.10.0版本的tag，可以拉下来升级后，调用插入方法，值能插进去，但是返回的dto里没有id
在主键上加的@SequenceGenerator你的dataobject有没有加注解
写了，可这不是mysql写法吗，oracle 写法要怎么写我们就是这样写的，没有问题的，oracle也是可以回传Id的。
mybatis-starter加载的时候会根据数据源连接判断使用了什么数据库，默认设置为mysql数据库。

如果是sequence方式的话，会查sequence的
你的sequence是怎么写的，这样的话数据库建序列的时候要以表名+_S结尾了
不行啊，实测用Oracle数据库的时候DO的主键需要@GeneratedValue(strategy = GenerationType.IDENTITY, generator = “”)注解，不然的话你上面截图里的identity计算出来是"JDBC"进而报错。但是这样的配置放到mysql数据源下运行时就会报错。我看了一下getIdentity()方法是读取的配置文件？所以能不能给个DEMO，演示一下Oracle数据库和mysql应该具体怎样配置。还有，现在提供的insertList方法只支持mysql数据库吧？换到Oracle源的时候动态拼接的sql依旧是mysql版的(io.choerodon.mybatis.provider.special.SpecialProvider.insertList(MappedStatement))你用的starter依赖是什么版本的，要用0.6.3用的0.6.4io.choerodon:choerodon-starter-core:jar:0.6.4.RELEASE:compile你把本地的maven仓库 io.choerodon 库清一下，更新下依赖这个是maven公库的Jar包，0.6.4我下载下来反编译也没有问题呀。
0.10全新安装完毕后访问wiki，密码验证通过后,过一阵抛出500代码
你好，你的第一张图片的报错页面下面的日志可以截取给我看下吗？你好这是完整的截图
这里似乎说你这个域名找不到。解决了，thank youChoerodon平台版本：0.9.0UI 组件版本：0.3.4运行环境：一键安装问题描述：如何把choerodon 换成自己公司的logo?执行的操作：报错信息(请尽量使用代码块的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作你好，暂时不支持在页面上修改，需要修改源码。可以修改根目录的config.js 文件。将favicon 和titlename 修改为对应的图片logo地址和title。然后重新打包即可。当前版本(0.10.0)没有替换logo的配置， 但是可以通过样式覆盖的方式来替换logo怎么覆盖呢？覆盖class=“c7n-boot-header-logo” 中的background-image等样式choerodon-front 工程？github 还是0.9版本？
Choerodon平台版本: 0.10.0遇到问题的执行步骤:
[root@node1 kubeadm-ansible]# cat inventory/hosts
[all]
node1 ansible_host=10.43.1.61 ip=10.43.1.61 ansible_user=root ansible_ssh_pass=123456 ansible_become=true
node2 ansible_host=10.43.1.62 ip=10.43.1.62 ansible_user=root ansible_ssh_pass=123456 ansible_become=true
node3 ansible_host=10.43.1.63 ip=10.43.1.63 ansible_user=root ansible_ssh_pass=123456 ansible_become=true[kube-master]
node1
node2
node3[etcd]
node1
node2
node3[kube-node]
node1
node2
node3
[root@node1 kubeadm-ansible]#84533355ed97        registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kubernetes-dashboard-init-amd64   “/bin/bash bootstr…”   2 minutes ago       Exited (0) 2 minutes ago                       k8s_kubernetes-dashboard-init_kubernetes-dashboard-dc8fcdbc5-kbgnh_kube-system_48cf5958-d2be-11e8-aff9-005056a15a79_0
63c6b7691d81        registry.cn-hangzhou.aliyuncs.com/choerodon-tools/flannel                           “cp -f /etc/kube-f…”   3 minutes ago       Exited (0) 3 minutes ago                       k8s_install-cni_kube-flannel-sjffh_kube-system_40e350a0-d2be-11e8-aff9-005056a15a79_0文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请粘贴“部署失败”的日志，上面docker命令查看到的是正常情况。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
第四步文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/dns/报错日志:执行：ubectl exec -n c7n-system $(kubectl get po -n c7n-system -l choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host wl.choerodon.io报错：;; connection timed out; no servers could be reached
command terminated with exit code 1kube-dns.yml:
apiVersion: v1
kind: ConfigMap
metadata:
name: kube-dns
namespace: kube-system
data:
stubDomains: |
{“wl.choerodon.io”: [“120.79.28.123”]}求教@ TimeBye请问你们的集群确实是没有公网域名吗？没有公网域名吗我们是研发测试环境，没有域名，上面修改成内网IP就好了，我写的外网IP，另外，请问我一键安装choerodon的时候一直在等待安装kafka，有什么办法能看到安装进度么，都一个小时了！请问你的服务器是私有云还是公有云？    如果是公有云，以阿里云为例，你在服务器集群外部通过自己定义的域名是无法进行访问的，阿里云会进行拦截让你购买域名并备案。请按以下命令检查一下阿里云，，，。。。这个是国家法律法规要求的哈，我这里也没有更好的办法了。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
Kubernetes集群部署
第一步
集群部署
ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):报错日志:
FAILED - RETRYING: wait for etcd up (10 retries left).
FAILED - RETRYING: wait for etcd up (9 retries left).第一次使用，原因未知，请指教谢谢你是否启用了防火墙？关闭了的iptables和firewall都关闭了请ifconfig查看每个节点实际IP地址（一般为eth0这个网卡的inet值），并将这个IP地址配置在inventory/hosts文件中后重试，谢谢。试已经搞定了，另外，再请问一个问题，我其他节点部署的时候执行kubectl apply -f kube-flannel-aliyun.yml，添加多条路由条目时报
Failed to download OpenAPI (Get http://localhost:8080/swagger-2.0.0.pb-v1: dial tcp 127.0.0.1:8080的错，不太明白，能否解答一下么，十分感谢节点其中有一个错误为：
atal: [node1 -> 172.18.201.179]: FAILED! => {“changed”: true, “cmd”: [“kubectl”, “taint”, “nodes”, “node1”, “node-role.kubernetes.io/master-”], “delta”: “0:00:00.249921”, “end”: “2018-10-18 11:46:49.362666”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-10-18 11:46:49.112745”, “stderr”: “error: taint “node-role.kubernetes.io/master:” not found”, “stderr_lines”: [“error: taint “node-role.kubernetes.io/master:” not found”], “stdout”: “”, “stdout_lines”: []}hosts的配置为
[all]
node1 ansible_host=172.18.201.179 ip=172.18.201.179 ansible_user=root ansible_ssh_pass=****** ansible_become=true
node2 ansible_host=172.18.201.180 ip=172.18.201.180 ansible_user=root ansible_ssh_pass=****** ansible_become=true
node3 ansible_host=172.18.201.181 ip=172.18.201.181 ansible_user=root ansible_ssh_pass=****** ansible_become=true[kube-master]
node1[etcd]
node1[kube-node]
node1
node2
node3kubectl apply -f kube-flannel-aliyun.yml  这个命令在任意一个master节点执行一次即可，不是每个节点都去执行哈。添加多条路由条目   貌似文档中没有这个操作，请问这个是怎么出来的呢？报错应该是因为执行了两边安装命令，但是这个错误是可以忽略的，他只是将master节点变更为可以调度的节点。好的Choerodon平台版本: 0.9.0疑问:
前端除了choerodon-front-boot, 其他是自己下了源码编译的包找不到getCodeMirror方法找不到。这个方法初始化的地方在哪里？
codeEditor搜索了以后没有引入库的地方。你好，这个是获取容器日志内显示日志信息的组件实例的方法。
const editor = this.editorLog.getCodeMirror();关闭后销毁组件实例，再调用会undefined。
0.9版本正常的开关应该不会有这个问题，可能会报undefined的情况后续版本也修复优化了。根据官方教程安装没有这个问题。
自己打包安装的，点击日志按钮，就没有反应，就已经报错。前端日志组件用的 react-codemirror 这个依赖包。` “choerodon-front-boot”: “^0.7.1”。可以检查看看有啥问题没我们用的是0.7.2，react-codemirror依赖有配置this.editorLog.getCodeMirror()看了下ci日志，有这种警告
你好，0.9 -> 0.10 日志这块有较大改动。ci的警告可以忽略，目前我们不好定位您出现的问题。您自己打包安装的前端代码也是0.9.0吗？您可以试试前端更新到0.9.6.或者ContainerHome.js这里报的错。我拉取了github上choerodon-front代码，查看了下它的子模块devops里面的这个文件有983行。
当时直接去choerodon-front-devops库里查tag0.9.*的ContainerHome.js代码都只有4百行左右。主要是各个模块之间版本怎么对应。一般大版本都是对应的，这里是文档http://choerodon.io/zh/docs/release-notes/。使用0.9系列版本，那各模块都更新到0.9.x的最新版本就可以的。github 上发布的已经是0.10版本了，有较大优化改动。不想更新的话，在ContainerHome.js 里325行附近删除 destroyOnClose 试试devops 版本0.9.6和0.9.7的内容发生了变化该问题是react-codemirror库在browser环境下没有fs模块造成的，解决方案是在config.js加入以下配置：chorerodon 0.10中已修复该问题， 届时可以删除该配置。看起来已经有了。
你是单独启动devops前端还是启的总前端?启动总前端需要注意，config只有总前端的才会生效，并且保证所有子模块下没有node_modules目录，不然会有依赖冲突。噢，我是用choerodon-front打包的。 那我试试在总的config里加看看。webpackConfig: function(wp) { wp.node = { fs: ‘empty’ } return wp; }加上这个配置后报这个错。
其实我把devops版本降到0.9.6的时候也报这个错。不过日志页面可以打开。这个错跟配置没关系了，是日志请求websocket 失败，这个错是前端devops版本推到0.9.6的时候
Choerodon平台版本: 0.10.0疑问:
从agent拿到pod的消息后，都是先判断initContainer的状态。InitContainer的状态变成Completed后也是取initContainer的状态，那基本上这个pod就不会正常？
你好 按照K8S的逻辑，initContainer如果没有结束，POD是不会运行起来的是结束了。但是结束以后，没有更新成正常容器的状态
@chazz @crockitwood
你好，请问配置域名是这个路径配置有没有什么要求呢？是不是随便配置一个然后访问的时候使用
“域名/路径”你好，请问配置域名是这个路径配置有没有什么要求呢？是不是随便配置一个然后访问的时候使用这个路径不能随便配置，如果你的应用服务就是以这个路径开头的话可以，这个路径是会被传进应用服务的。kubernetes的ingress不是使用nginx做反向代理的？
为什么nginx配置反向代理的时候可以配置 location /路径
这个路径是可以随便填写，然后访问加这个路径就可以访问。这是因为kubernetes限制了吗？你好，kubernetes的ingress本质上就是使用nginx做反向代理。在猪齿鱼中的  域名/路径  中的  路径  指的就是ingress对象中的  spec.rules[].http.paths[].path  属性的值。默认情况下，ingress-nginx解析ingress对象时将域名解析到kubernetes的svc对象，svc对象又转发到pod中。而路径则指的是pod中的服务路径。例如：容器运行的是一个tomcat，你放了一个名为demo.war的war包进去，你访问的时候就得是域名/demo才可以访问到demo这个应用。更多信息请参考官方文档：
https://kubernetes.github.io/ingress-nginx/Choerodon平台版本：0.10.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。怎么修改access_token的过期时间，现在界面不操作，30s后就报401
你好，可以在系统中，找到运营组织->组织管理->客户端管理 找到你们前端页面对应的client修改失效时间嗯，谢谢，过期后按道理应该是用refresh_token刷新的啊，不应该报401，我们这边是hzero的前端，是不是前端缺少刷新access_token的机制?
还有一个问题，系统可以设置登出时间吗？就是我很长一段时间不操作，就需要重新登录？因为权限这里有两种，一种是确实没有接口权限，另一种是token失效而没有权限。所以没有统一处理。
有关第二种如果手动刷新页面会自动刷新的。我们这边考虑下如何处理。设置系统登出时间是出于什么考虑？前端的失效和session有关。理论上是不应该一直保存登录的。嗯，是这样的，设置系统登出时间主要是其他系统有，客户要求，所以是可以通过设置session失效时间设置系统登出时间是吗?请问在哪里设置？目前没有提供可以配置的开关，如果是通过源码编译的话，可以通过像HAP的selectOptions()一样的动态查询Choerodon平台版本: 0.10.0遇到问题的执行步骤:
分步安装文档地址:环境信息(如:节点信息):报错日志:
分步安装完后，用admin登录后，点个人中心或者管理中的任何菜单，页面提示403无权限
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:人中心或者管理中的任何菜单，页面提示403无权限确认下kafka是否正常，然后环境变量配置的zookeeper和kafka的配置是正确的，然后重启下管理服务和iam服务，顺便看下这两个服务有没有打印相应log日志iam_permission表为空是什么原因呢，zookeeper和kafka的配置一切正常需要看go-register，manager-service，iam-service进行排错
如果iam注册到go-register，会有如下日志，然后发送事件到manager-service
谢谢，问题解决了，我范了个小错误，我是在现有K8S集群中分步部署的，但我的集群名称不叫cluster.local,直接copy文档的部署代码会导至KAFKA地址不正确，其它服务我注意到了，但go-register给遗漏了:grinning:Choerodon平台版本: 0.10遇到问题的执行步骤:  测试管理升级文档地址:部署成功之后，需要进行数据升级，数据升级将会把执行逆向生成用例，不同文件夹下的相同用例将复制，并在每一个版本下生成一个临时文件夹，以下是修复步骤报错日志:
{
“timestamp”: “2018-10-16 17:12:51”,
“status”: 500,
“error”: “Internal Server Error”,
“exception”: “feign.FeignException”,
“message”: “Request processing failed; nested exception is feign.FeignException: status 500 reading SagaClient#startSaga(String,StartInstanceDTO); content:
{“failed”:true,“code”:“error.saga.notExist”,“message”:“saga不存在”}”,
“path”: “/v1/projects/5/cycle/fix”
}2018-10-16 17:53:50.925  INFO [asgard-service,cde3f7f167925a1d,c75d7cdcc10973be,true] 1 — [ XNIO-3 task-53] i.c.r.h.ControllerExceptionHandler       : exception info io.choerodon.core.exception.FeignException: error.saga.notExist
at io.choerodon.asgard.api.service.impl.SagaInstanceServiceImpl.start(SagaInstanceServiceImpl.java:83)
at io.choerodon.asgard.api.service.impl.SagaInstanceServiceImpl$$FastClassBySpringCGLIB$$5e6d631c.invoke()
at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:736)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:99)
at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:282)
at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:96)
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:671)
at io.choerodon.asgard.api.service.impl.SagaInstanceServiceImpl$$EnhancerBySpringCGLIB$$9c2deabd.start()
at io.choerodon.asgard.api.controller.v1.SagaInstanceController.start(SagaInstanceController.java:48)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133)
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738)
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.choerodon.resource.filter.JwtTokenFilter.doFilter(JwtTokenFilter.java:101)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:111)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:208)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:64)
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132)
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:336)
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)asgard-service 日志报错。asgard-service报错是因为test_manager_service的saga没有扫描进去，参照上面说的 把test_manager_service 服务saga扫描进去就好了这是因为 test_manager_service 服务的saga没有自动扫描进去，重启下 test_manager_service ，可以看下 asgard-service 日志，有没有收到 test_manager_service 服务启动的消息。
==>
asgard-service日志有收到test_manager_service的启动消息：
2018-10-17 09:37:36.238  INFO [asgard-service,] 1 — [trap-executor-0] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
2018-10-17 09:41:08.349  INFO [asgard-service,] 1 — [ntainer#0-0-C-1] i.c.a.a.e.RegisterInstanceListener       : receive message from register-server, {“status”:“UP”,“appName”:“test-manager-service”,“version”:“0.10.2”,“instanceAddress”:“172.20.23.46:8093”,“createTime”:“2018-10-17 09:29:44”}如果asgard收到消息却扫描不进去，添加环境变量 CHOERODON_ASGARD_ISLOCAL 设置成 false
==》
已经添加了此环境变量
但是错误仍然存在。
添加环境变量之后重启了一下test_manager_service吗？2018-10-17 09:41:08.349 INFO [asgard-service,] 1 — [ntainer#0-0-C-1] i.c.a.a.e.RegisterInstanceListener : receive message from register-server, {“status”:“UP”,“appName”:“test-manager-service”,“version”:“0.10.2”,“instanceAddress”:“172.20.23.46:8093”,“createTime”:“2018-10-17 09:29:44”}重启了。重启test-mamager-service之后，asgard-service有没有打印扫描不进去原因的日志，比如error.registerConsumer.fetchDataError或者error.registerConsumer.msgConsumerError没有的。
这是重启过程出现的日志：2018-10-17 15:06:36.514  INFO [asgard-service,] 1 — [ntainer#0-0-C-1] i.c.a.a.e.RegisterInstanceListener       : receive message from register-server, {“status”:“DOWN”,“appName”:“test-manager-service”,“version”:“0.10.2”,“instanceAddress”:“172.20.23.22:8093”,“createTime”:“2018-10-17 14:55:12”}
2018-10-17 15:07:36.299  INFO [asgard-service,] 1 — [trap-executor-0] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
2018-10-17 15:07:43.419  INFO [asgard-service,] 1 — [  XNIO-2 task-5] c.c.c.ConfigServicePropertySourceLocator : Fetching config from server at: http://config-server.devops:8010/
2018-10-17 15:07:43.539  INFO [asgard-service,] 1 — [  XNIO-2 task-5] c.c.c.ConfigServicePropertySourceLocator : Located environment: name=manager-service, profiles=[default], label=null, version=null, state=null
2018-10-17 15:07:44.201  INFO [asgard-service,] 1 — [ntainer#0-0-C-1] i.c.a.a.e.RegisterInstanceListener       : receive message from register-server, {“status”:“UP”,“appName”:“test-manager-service”,“version”:“0.10.2”,“instanceAddress”:“172.20.23.8:8093”,“createTime”:“2018-10-17 14:56:19”}
2018-10-17 15:07:44.246  WARN [asgard-service,f07bde227aae2bb0,f07bde227aae2bb0,true] 1 — [RxIoScheduler-7] io.choerodon.mybatis.helper.AuditHelper  : principal not instanceof CustomUserDetails audit user is 0L您好，麻烦您执行下
helm update
更新一下镜像源，然后使用如下命令升级一下test-manager-service服务，版本为 0.10.3。
helm upgrade test-manager-service c7n/test-manager-service \ -f <(helm get values test-manager-service) \ --version=0.10.3
我们的代码在处理修数据的事件时有些问题，现在已经修复了。抱歉给您造成不便，谢谢您对我平台的支持。好的  谢谢。发现这个版本我发现了好几个bug了  是不是有奖  哈哈Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：昨天晚上部署应用的时候，部署都失败。而且是圆环里面显示0，状态运行中，但是实际环境中并没有。
后来发现cherodon-agent在某个时间点后就没有get command的日志了。不知道为什么。
后来通过helm把环境pod删掉，重建了下就好了。怀疑的地方：
1.之前遇到应用部署后，一直处于处理中的时候，我都直接将devops_app_instance物理删除了。之前没有什么问题。就昨天的话报the applicationInstance in the file is not exist in devops database。于是去清理了下gitlab的gitops库就不报了。2.在这个环境下部署了大量应用。有几十个，有微服务，mysql，efk，redis，rabbit，apollo等等，会不会导致环境pod异常Agent版本是几，先重启一下agent0.9.6.  重启是好了，这个试过的。能做到自动恢复吗?0.9.X的版本可以用0.9.9,这个版本修复了不执行command这个bug发现个奇怪的问题：
安装完成后，把访问主机的dns设置成DNS服务器ip地址，发现和DNS服务器同个网段的主机能够正常解析域名；
如果访问主机的IP地址和DNS服务器不在一个网段，把dns设置成DNS服务器ip地址后，还是无法解析域名，不知道是怎么回事？访问主机的IP地址和DNS服务器不在一个网段时，“访问主机”可以ping通DNS服务器吗？可以ping通的可以ping通，就是无法解析，你有修改过“访问主机”的hosts文件吗？访问主机的hosts文件改了，没有作用，
我直接把访问主机的dns都改了，还是没有作用，
但是和dns服务器同网段的访问主机更改dns就可以正常的解析域名，好奇怪的访问主机的hosts文件改了，没有作用，
我直接把访问主机的dns都改了，还是没有作用，
但是和dns服务器同网段的访问主机更改dns就可以正常的解析域名，好奇怪的Choerodon猪齿鱼是一个开源企业服务平台，是基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理的开源平台，同时提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，致力帮助企业聚焦于业务，加速数字化转型。2018年10月15日，Choerodon猪齿鱼发布 0.10 版本，本次更新对 知识管理、敏捷管理、持续交付 等各项服务增加了新功能，并对一些功能细节做了进一步优化，欢迎各位更新体验，同时特别感谢社区中的朋友给Choerodon猪齿鱼提出的诸多中肯意见。下面就为大家带来详细的版本更新介绍。新增功能01 知识管理同时，新增了手动重试功能，当同步组织、项目失败或者创建空间失败时可手动重试。02 敏捷管理敏捷管理服务新增了史诗燃耗图、版本燃耗图两个报告，并且新增了迭代工作台和报告工作台，详情如下：03 持续交付持续交付本次主要新增了DevOps报表功能，支持查看代码提交、应用构建以及应用部署的情况：代码提交图：跟踪项目团队与个人的代码提交情况，可以了解团队的整体效率与个人效率。
构建次数图：从应用的维度展示某个应用的构建次数，构建成功次数以及构建成功率，可帮助团队快速了解到该应用的构建情况。
构建时长图：从应用的维度展示所选时间段某个应用每一次构建的时长，更直观地了解应用构建的效率。
部署次数图：展示了项目各个环境中各个应用的部署频率，了解团队的部署频次与整体的效率。
部署时长图：展示该项目下某一环境中各个应用部署时长等部署相关的信息，了解各个应用的部署情况。
新增部署总览功能，可以查看所有应用在各个环境下的部署情况，并能在此页面完成应用最新版本的快速部署。
支持使用shell命令操作pod以便于调试新增环境分组，支持按照环境分组查看流水线新增域名证书管理功能，支持域名证书的申请与上传支持创建标记时填写release notes，并支持查看编辑与修改新增查看容器日志时的Stop Following、Go Top功能，并支持全屏查看容器日志新增容器界面，选择环境与应用的下拉框和应用版本界面选择应用的下拉框便于搜索与过滤04 测试管理测试管理此次主要增加以下几个功能：增加了Oracle数据库支持增加了测试计划功能测试用例管理添加文件夹层级用例管理侧边栏具有宽窄两种展示新增测试计划页面
测试执行增加用户筛选05 微服务开发框架微服务开发框架增加了如下的功能：功能优化01 知识管理02 敏捷管理03 持续交付04 测试管理05 微服务开发框架缺陷修复01 知识管理02 敏捷管理03 持续交付04 测试管理05 微服务开发框架删除01 敏捷管理02 持续交付更加详细的内容，请参阅Release Notes和官网。欢迎通过Choerodon的GitHub和猪齿鱼社区进行反馈与贡献，感谢各位朋友陪伴Choerodon猪齿鱼不断成长，Choerodon会持续优化，敬请期待。大家可以通过以下社区途径了解猪齿鱼的最新动态、产品特性，以及参与社区贡献：欢迎加入Choerodon猪齿鱼社区，共同为企业数字化服务打造一个开放的生态平台。Choerodon平台版本: 0.10遇到问题：发现升级到0.10以后，点击页面菜单经常会卡住报错日志:
选择项目也经常没反应。不知道是不是就我一个人遇到   日志也没有报错已知在网络足够慢的情况下可能会发生类似情况，加载新的路由界面的时候会去下载该页面的chunk文件，第一次要下载的chunk会比较大，如果是第一次进入而且平均网速低于30kb/s就会有类似假死的情况，能否提供更多信息，比如浏览器控制台的网络信息或更详细的卡住的症状。我是局域网测试的。
刷新页面就没问题。过一会就要强刷一次。还有点击前台页面经常出现这个，这个是在哪里修改的?
嗯，了解了。这个报错是0.10版本刚加的websocket，可以即时在右上角获得消息通知，要在实例部署里的配置信息里改websocket地址，应该是这个问题。发现不到一分钟时间。点击所有页面都是403我昨天也是你的这个问题，我修改一下这个就好了，步骤：
1：
403时看下gateway-helper的日志，看下403原因是啥？如果是userDetails的问题就是client的超时时间太短了，照着楼上所说超时时间设置长点果然  默认只有60秒你好，我通过一键部署脚本到28步骤就出错了，麻烦帮我看看吧，谢谢！
[Step 28]: get gitlab token …
curl: (6) Could not resolve host: gitlab.example.choerodon.io; Name or service not known
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Dload  Upload   Total   Spent    Left  Speed
0     0    0     0    0     0      0      0 --:–:-- --:–:-- --:–:--     0curl: (6) Could not resolve host: gitlab.example.choerodon.io; Name or service not known
rm: cannot remove ‘/tmp/cache-gitlab.jar’: No such file or directory
curl: (6) Could not resolve host: gitlab.example.choerodon.io; Name or service not known
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Dload  Upload   Total   Spent    Left  Speed
0     0    0     0    0     0      0      0 --:–:--  0:00:04 --:–:--     0curl: (6) Could not resolve host: gitlab.example.choerodon.io; Name or service not known
rm: cannot remove ‘/tmp/cache-gitlab.jar’: No such file or directory
curl: (6) Could not resolve host: gitlab.example.choerodon.io; Name or service not known
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Dload  Upload   Total   Spent    Left  Speed
0     0    0     0    0     0      0      0 --:–:-- --:–:-- --:–:--     0curl: (6) Could not resolve host: gitlab.example.choerodon.io; Name or service not known
curl: (6) Could not resolve host: gitlab.example.choerodon.io; Name or service not known
 自动获取Gitlab Impersonation Token失败，请手工获取后继续部署。 http://gitlab.example.choerodon.io
请输入手工获取的Gitlab Impersonation Token:这一步出错  说明你DNS那一节没有配置正确哈 ，请按文档正确进行配置哈我的DNS应该是正常的，我的一台windows机器把DNS设置成dns服务器的IP地址后，都可以正常Ping 通 gitlab.example.choerodon.io,但是我此台机器就是DNS服务器，所以没有单独设置dns，这应该没有影响的吧？也要配置的哦，谢谢！那我再试试你好，请问以下这句话如何理解：
域名映射
你需要在DNS运营商提供的控制面板上添加一条Gitlab域名与第一步设置SSH端口号的节点IP的记录。建议你先看看这个文章  你就明白了哈https://help.aliyun.com/document_detail/29716.html?spm=a2c4g.11186623.2.10.41ed4c07CPuNvMChoerodon平台版本：0.10.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：
运行demo修改完配置上传打包镜像报错，查了代码http://sonarqube.exmple.choerodon.io 没有这个地址问题描述：
修改的数据：报错信息(请尽量使用代码块的形式展现)：原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问Hi， 你把 .gitlab-ci.yml 中的这句话删除即可。我们的最新模板已删除这一条命令。好的，我试下这个全部删了吗对的哈好的，已经可以了Choerodon平台版本: 0.10.0遇到问题的执行步骤:
升级XWiki
XWiki-0.10.0版本需要依赖Choerodon微服务框架的0.10.0版本功能，在升级XWiki之前，请确保Choerodon微服务框架已经升级到0.10.0版本。请问下这个入口在哪里？平台上没有找到。谢谢！你是0.9升级到0.10吗？这里的意思是说，你登入wiki系统之后，进入到  设置–>内容–>导入。
也就是你可以在Choerodon平台的Wiki管理里面跳转到wiki系统。或者直接访问wiki的域名。
然后按步骤导入这个系统设置是wiki系统的系统设置。我们会修改下文档的说明，让他更容易理解。不好意思！明白了。谢谢！官网文档已经更新，再次感谢您的提醒！知识管理升级 安装v0.10版本，安装到NFS时报错：
[root@node2 kubernetes]# helm install c7n/nfs-client-provisioner \Error: release nfs-client-provisioner failed: namespaces “c7n-system” is forbidden: User “system:serviceaccount:kube-system:default” cannot get namespaces in the namespace “c7n-system”请问你的k8s集群是按我们文档搭建的吗？是的请确认执行命令--set rbac.create=true是否设置正确，若设置正确是不会报使用default这个serviceaccount的错误信息的。你好，请问这个参数是在哪里设置的啊？我记得好像没有这个参数啊文档中这里哦，好的，谢谢！
我看到和0.9.0版本的dns设置有区别，这样做了是不是就不需要每个节点都单独挂载nfs目录了？我正好在这块有个疑问, 借楼问一下哈:如下配置的话是不是只需要在node1 node2 node3安装nfs-provisioner
再在 node4 安装 nfs-client-provisioner谢谢哦，好的，谢谢！
我看到和0.9.0版本的NFS设置有区别，这样做了是不是就不需要每个节点都单独挂载nfs目录了？哦，好的，谢谢！在你没有nfs服务的时候请搭建nfs-provisioner哈在你本来就有nfs服务的时候请搭建nfs-client-provisioner哈好的，谢谢！我再从新搭建一遍试试你好，再请问一下，通过一键部署安装完choerodon 0.10.版本后，Wiki还需要单独安装吗？如果还需要单独安装的话，是在

之前？还是之后安装呢？一键部署从0.10版本 wiki默认安装了哈。这两步需要手动进行哈。哦，好的，谢谢！OK, 明白,谢谢了Choerodon平台版本: 0.10遇到问题的执行步骤:测试管理升级报错日志:
helm安装显示：Error: failed to download “c7n/test-manager-service”Hi, 执行 helm update 之后再试一下哦[root@localhost ~]#  helm update
Command “update” is deprecated, use ‘helm repo update’Hang tight while we grab the latest from your chart repositories…
…Skip local chart repository
…Successfully got an update from the “c7n” chart repository
…Successfully got an update from the “stable” chart repository
Update Complete. ⎈ Happy Helming!⎈可以了，谢谢Choerodon平台版本: 0.10遇到问题的执行步骤:文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/install/parts/base/gitlab/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:在分步部署过程中，如何利用企业已有的gitlab服务（10.5）,原有的gitlab是通过官方的 Omnibus方式非容器部署的，数据库默认是postgresql，用户通过ldap身份验证，看了choerodon的gitlab集成文档似乎要改成Oauth认证，有没有不影响现有业务无缝集成原有gitlab的方案目前是这种已有gitlab的对接是没有的，由于系统对gitlab的集成较高，gitlab相关的配置、设置、权限等等不一致很容易导致系统问题。
但是该需求我们已经在讨论实验中了，会在后续的版本进行方案支持，如果有新的进度我们会及时在这里回复希望将来能够解耦支持外部gitlab部署，通过helm部署的gitlab性能完全不能够用在生产上，而且基本上要用到PAAS平台的企业内部肯定早有gitlab服务，迁移原来的代码仓库也是伤筋动骨的事情，本来想体验下C7N现在就卡在这了好的安装xwiki service后，xwiki service能正常运行，但安装Wiki service 后 Wiki service一直起不来，Wiki pod运行不正常：
choerodon-devops-prod   wiki-service-df795ccc8-j624v                             0/1       Running   0
通过descripe查看pod报错如下：
kubectl describe pod wiki-service-df795ccc8-j624v  -n choerodon-devops-prod
Name:           wiki-service-df795ccc8-j624v
Namespace:      choerodon-devops-prod
Node:           node4/192.168.177.70
Start Time:     Wed, 10 Oct 2018 16:46:41 +0800
Labels:         choerodon.io/metrics-port=9991
choerodon.io/release=wiki-service
choerodon.io/service=wiki-service
choerodon.io/version=0.9.1
pod-template-hash=893517774
Annotations:    choerodon.io/metrics-group=spring-boot
choerodon.io/metrics-path=/prometheus
kubernetes.io/created-by={“kind”:“SerializedReference”,“apiVersion”:“v1”,“reference”:{“kind”:“ReplicaSet”,“namespace”:“choerodon-devops-prod”,“name”:“wiki-service-df795ccc8”,“uid”:"021c87ca-cc69-11e8-…
Status:         Running
IP:             10.233.66.71
Created By:     ReplicaSet/wiki-service-df795ccc8
Controlled By:  ReplicaSet/wiki-service-df795ccc8
Containers:
wiki-service:
Container ID:   docker://1f378a6dce2b24133caf6f99934b8944606bfd77d4741f1b35fb6483302f1d2f
Image:          choerodon/wiki-service:0.9.1
Image ID:       docker-pullable://choerodon/wiki-service@sha256:cf18e7cb28a251b00007b7cc045746c317c2e212447afef0e95c978503654cf5
Port:           9990/TCP
State:          Running
Started:      Wed, 10 Oct 2018 16:47:54 +0800
Ready:          False
Restart Count:  0
Limits:
memory:  4Gi
Requests:
memory:   2Gi
Readiness:  exec [/bin/sh -c curl localhost:9991/health --fail && curl localhost:9990/v2/choerodon/api-docs --fail] delay=60s timeout=10s period=10s #success=1 #failure=3
Environment:
CHOERODON_EVENT_CONSUMER_KAFKA_BOOTSTRAP_SERVERS:  kafka-0.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-1.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-2.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092
EUREKA_CLIENT_SERVICEURL_DEFAULTZONE:              http://register-server.choerodon-devops-prod:8000/eureka/
EUREKA_DEFAULT_ZONE:                               http://register-server.choerodon-devops-prod:8000/eureka/
SPRING_CLOUD_CONFIG_ENABLED:                       true
SPRING_CLOUD_CONFIG_URI:                           http://config-server.choerodon-devops-prod:8010/
SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS:          kafka-0.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-1.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-2.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092
SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES:         zookeeper-0.zookeeper-headless.choerodon-devops-prod.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.choerodon-devops-prod.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.choerodon-devops-prod.svc.cluster.local:2181
SPRING_DATASOURCE_PASSWORD:                        nipu1y
SPRING_DATASOURCE_URL:                             jdbc:mysql://choerodon-mysql:3306/wiki_service?useUnicode=true&  characterEncoding=utf-8&useSSL=false
SPRING_DATASOURCE_USERNAME:                        choerodon
SPRING_KAFKA_BOOTSTRAP_SERVERS:                    kafka-0.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-1.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-2.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092
SPRING_KAFKA_PRODUCER_VALUE_SERIALIZER:            org.apache.kafka.common.serialization.ByteArraySerializer
WIKI_CLIENT:                                       xwiki
WIKI_DEFAULT_GROUP:                                XWikiAllGroup
WIKI_TOKEN:                                        Choerodon
WIKI_URL:                                          http://wiki.example.choerodon.io
Mounts:
/Charts from data (rw)
/var/run/secrets/kubernetes.io/serviceaccount from default-token-brbgz (ro)
Conditions:
Type           Status
Initialized    True
Ready          False
PodScheduled   True
Volumes:
data:
Type:    EmptyDir (a temporary directory that shares a pod’s lifetime)
Medium:
default-token-brbgz:
Type:        Secret (a volume populated by a Secret)
SecretName:  default-token-brbgz
Optional:    false
QoS Class:       Burstable
Node-Selectors:  
Tolerations:     
Events:
Type     Reason     Age                  From            MessageWarning  Unhealthy  1m (x6378 over 17h)  kubelet, node4  Readiness probe failed:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Dload  Upload   Total   Spent    Left  Speed
0     0    0     0    0     0      0      0 --:–:-- --:–:-- --:–:--     0curl: (7) Failed to connect to localhost port 9991: Connection refused
最后一行显示：
0curl: (7) Failed to connect to localhost port 9991: Connection refused您好，9991端口时wiki-service的健康检查端口。你可以进入wiki-service 的pod里面，使用curl 127.0.0.1:9991/health
看是否可以访问。哦，谢谢！我已经把环境初始化了，我准备安装choerodon 0.8版本，不知道安装wiki是否能成功？我主要是想用里面的知识库功能哈！我们现在已经发布了Choerodon 0.10 版本，你可以体验下。官网服务部署文档好的，谢谢！我先安装0.8版本试试，不行的话再安装0.10版本你好！我已一键部署完choerodon，安装xwiki时，做到这步就失败了，不知道为什么？
helm install c7n/xwiki \Error: timed out waiting for the condition你的helm是什么版本?V2.8.2的版本这个一般是操作超时，前面的日志有其他的日志吗我是通过一键部署脚本安装的choerodon，安装完成后，我再单独安装的xwiki，按照你们提供的操作步骤做的，如下：
添加choerodon chart仓库
helm repo add c7n https://openchart.choerodon.com.cn/choerodon/c7n/
helm repo update
创建数据库
进入数据库kubectl get po -n choerodon-devops-prodkubectl exec -it [mysql pod name] -n choerodon-devops-prod bashmysql -uroot -p${MYSQL_ROOT_PASSWORD}
创建choerodon所需数据库及用户并授权
部署完成后请注意保存用户名和密码。
CREATE USER IF NOT EXISTS ‘choerodon’@’%’ IDENTIFIED BY “password”;
CREATE DATABASE IF NOT EXISTS wiki_service DEFAULT CHARACTER SET utf8;
CREATE DATABASE IF NOT EXISTS xwiki DEFAULT CHARACTER SET utf8;
GRANT ALL PRIVILEGES ON wiki_service.* TO choerodon@’%’;
GRANT ALL PRIVILEGES ON xwiki.* TO choerodon@’%’;
FLUSH PRIVILEGES;
部署xwiki
创建数据卷
创建之前请在nfs服务器对应位置创建相应的目录。
helm install c7n/create-pv 
–set type=nfs 
–set pv.name=wiki-pv 
–set nfs.path=/u01/wiki 
–set nfs.server=nfs.example.com 
–set pvc.name=wiki-pvc 
–set size=50Gi 
–set accessModes={ReadWriteMany} 
–name wiki-pv --namespace=choerodon-devops-prod
部署xwiki
部署xwiki需要初始化一些数据，安装需要几分钟，请耐心等待,部署完成后需要根据指定的客户端到Choerodon添加对应的客户端。Choerodon创建客户端时不选择scope，请在创建完成后编辑Scope
helm install c7n/xwiki 
–set env.DB_USER=choerodon 
–set env.DB_PASSWORD=password 
–set env.DB_HOST=choerodon-mysql 
–set env.DB_DATABASE=xwiki 
–set env.OIDC_ENDPOINT_AUTHORIZATION=http://api.example.choerodon.io/oauth/oauth/authorize 
–set env.OIDC_ENDPOINT_TOKEN=http://api.example.choerodon.io/oauth/oauth/token 
–set env.OIDC_ENDPOINT_USERINFO=http://api.example.choerodon.io/iam/v1/users/self 
–set env.OIDC_ENDPOINT_LOGOUT=http://api.example.choerodon.io/oauth/logout 
–set env.OIDC_CLIENTID=wiki 
–set env.OIDC_SECRET=secret 
–set env.OIDC_WIKI_TOKEN=Choerodon 
–set persistence.enabled=true 
–set persistence.existingClaim=wiki-pvc 
–set service.enabled=true 
–set ingress.enabled=true 
–set “ingress.hosts[0]”=wiki.example.choerodon.io 
–name=xwiki 
–version=0.9.0 
–namespace=choerodon-devops-prod
最后一步就出错了，前面都没有问题的0.10的xwiki会换一种方式安装，建议您等一两天更新文档之后再试一下。0.10版本已经发布，您可以参考官网的知识管理部署文档—— 部署知识管理 ，安装时添加了几个环境变量，安装过程的内部逻辑也做了修改。哦，谢谢！我准备安装choerodon 0.8版本，不知道安装wiki是否能成功？我主要是想用里面的知识库功能哈！好的 现在应该安装0.10，0.8到0.10之间修复了很多bug并新增了很多特性哦，我先安装0.8版本试试吧，不行再安装0.10版本，我只需要知识库管理功能Choerodon平台版本：0.10.0运行环境：一键安装问题描述：
0.9升级0.10之后，之前的容器不能用了：devops服务报错：
执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作升级后若没有操作过实例直接删除会造成这个问题，我们正在修复，今晚会发布新的修复版本，十分抱歉额，好的。但是新创建的 环境就是没问题的。还有一个bug，麻烦处理一下：多个环境 往前拖动 换次序，会出现黑白（显示没有环境）
比如：
内部实验往阿里云环境拖动
该问题已经修复，请更新devops服务至0.10.2http://choerodon.io/zh/docs/installation-configuration/update/0.9-to-0.10/
请按照文档升级devops-service服务至0.10.2,然后任意操作一个实例。Choerodon平台版本：0.10.0运行环境(如localhost或k8s)：一键安装遇到问题时的前置条件：
0.10版本开发流水线-代码仓库跳转地址bug问题描述：
开发流水线-代码仓库跳转地址bug
请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：在Choerodon组织的租户设置中的客户端修改会报错，导致修改不成功。原因分析：
开发流水线-代码仓库跳转地址bug。
仓库地址：后面多了一个.git.git疑问：提出您对于遇到和解决该问题时的疑问谢谢反馈，我们尽快修复发布请问这个前后端均为0.10.0之后的版本么？我是从0.9一键安装
http://choerodon.io/zh/docs/installation-configuration/update/0.9-to-0.10/
按上面文档手动升级到0.10可以麻烦您截图一下这个请求的返回数据么？
这个问题应该是已经处理了，如果前后端都升级到0.10.0就应该不会出现这样的问题，需要定位一下原因
Choerodon平台版本：0.10.0运行环境：一键安装问题描述：
API测试中的{project_id}怎么查？如：管理页面怎么进入，进行系统配置执行的操作：
如:已经搭建完毕,成功登陆后并没有看到系统配置的入口报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作也可以在api测试中找查询项目的接口来获取project_id当然，也可以在iam_service数据库的fd_project表中找到所有项目的idRESOURCES:
==> v1/PersistentVolume
NAME                CAPACITY  ACCESS MODES  RECLAIM POLICY  STATUS     CLAIM  STORAGECLASS  REASON  AGE
choerodon-mysql-pv  3Gi       RWO           Retain          Available  0s==> v1/PersistentVolumeClaim
NAME                 STATUS   VOLUME              CAPACITY  ACCESS MODES  STORAGECLASS  AGE
choerodon-mysql-pvc  Pending  choerodon-mysql-pv  0         0sNAME:   choerodon-mysql
LAST DEPLOYED: Sat Oct 13 20:52:42 2018
NAMESPACE: choerodon-devops-prod
STATUS: DEPLOYEDRESOURCES:
==> v1beta1/Deployment
NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
choerodon-mysql  1        1        1           0          0s==> v1/Pod(related)
NAME                              READY  STATUS             RESTARTS  AGE
choerodon-mysql-5958d6f486-nrkkj  0/1    ContainerCreating  0         0s Success of install Mysql for Choerodon.[Step 8]: checking Mysql ready …
➜ choerodon-mysql not ready,sleep 5s,check it.
➜ choerodon-mysql not ready,sleep 5s,check it.
➜ choerodon-mysql not ready,sleep 5s,check it.
➜ choerodon-mysql not ready,sleep 5s,check it.
 Mysql is ready.[Step 9]: create database for Choerodon …
Error: Job failed: DeadlineExceeded
 create database failed文档地址:环境信息(如:节点信息):报错日志:
我这边查看node节点上的运行的镜像发现是
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
是不是我集群dns有问题导致的疑问:提出您对于遇到和解决该问题时的疑问我集群dns有问题导致的您的集群DNS有什么问题呢Name:           create-choerodon-databases-76b9t
Namespace:      choerodon-devops-prod
Node:           node4/172.16.160.12
Start Time:     Sat, 13 Oct 2018 21:22:32 +0800
Labels:         controller-uid=0ab659bd-ceeb-11e8-b022-00163e08f441
job-name=create-choerodon-databases
Annotations:    kubernetes.io/created-by={“kind”:“SerializedReference”,“apiVersion”:“v1”,“reference”:{“kind”:“Job”,“namespace”:“choerodon-devops-prod”,“name”:“create-choerodon-databases”,“uid”:"0ab659bd-ceeb-11e8-b02…
Status:         Pending
IP:             10.233.67.6
Created By:     Job/create-choerodon-databases
Controlled By:  Job/create-choerodon-databases
Init Containers:
mysqlcheck:
Container ID:  docker://e5c4b9e4bdcc2b62516e99707f60b147d0a112827d2bcb4a6c621e64ce7a9264
Image:         registry.saas.hand-china.com/tools/mysql-client:latest
Image ID:      docker-pullable://registry.saas.hand-china.com/tools/mysql-client@sha256:71bd51c96fc910933ebfb34676dee92a472427767138382884e7237aae8bd914
Port:          
Command:
sh
-c
while ! mysqlcheck --host=${MYSQL_HOST} --port=${MYSQL_PORT} --user=${MYSQL_USER} --password=${MYSQL_PASS} information_schema; do sleep 1; done
State:          Running
Started:      Sat, 13 Oct 2018 21:22:33 +0800
Ready:          False
Restart Count:  0
Environment:
MYSQL_HOST:  choerodon-mysql
MYSQL_PASS:  password
MYSQL_PORT:  3306
MYSQL_USER:  root
SQL_SCRIPT:  CREATE DATABASE IF NOT EXISTS devops_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS gitlab_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS iam_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS manager_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS agile_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS test_manager_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS asgard_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS notify_service DEFAULT CHARACTER SET utf8;           GRANT ALL PRIVILEGES ON devops_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON gitlab_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON iam_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON manager_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON agile_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON test_manager_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON asgard_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON notify_service.* TO choerodon@’%’;           FLUSH PRIVILEGES;
Mounts:
/var/run/secrets/kubernetes.io/serviceaccount from default-token-zmxqz (ro)
Containers:
create-choerodon-databases:
Container ID:
Image:         registry.saas.hand-china.com/tools/mysql-client:latest
Image ID:
Port:          
Command:
/bin/sh
-c
echo ${SQL_SCRIPT} | mysql -u${MYSQL_USER} -p${MYSQL_PASS} -h${MYSQL_HOST} -P${MYSQL_PORT}
State:          Waiting
Reason:       PodInitializing
Ready:          False
Restart Count:  0
Environment:
MYSQL_HOST:  choerodon-mysql
MYSQL_PASS:  password
MYSQL_PORT:  3306
MYSQL_USER:  root
SQL_SCRIPT:  CREATE DATABASE IF NOT EXISTS devops_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS gitlab_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS iam_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS manager_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS agile_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS test_manager_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS asgard_service DEFAULT CHARACTER SET utf8;           CREATE DATABASE IF NOT EXISTS notify_service DEFAULT CHARACTER SET utf8;           GRANT ALL PRIVILEGES ON devops_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON gitlab_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON iam_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON manager_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON agile_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON test_manager_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON asgard_service.* TO choerodon@’%’;           GRANT ALL PRIVILEGES ON notify_service.* TO choerodon@’%’;           FLUSH PRIVILEGES;
Mounts:
/var/run/secrets/kubernetes.io/serviceaccount from default-token-zmxqz (ro)
Conditions:
Type           Status
Initialized    False
Ready          False
PodScheduled   True
Volumes:
default-token-zmxqz:
Type:        Secret (a volume populated by a Secret)
SecretName:  default-token-zmxqz
Optional:    false
QoS Class:       BestEffort
Node-Selectors:  
Tolerations:     
Events:
Type    Reason                 Age   From               MessageNormal  Scheduled              3m    default-scheduler  Successfully assigned create-choerodon-databases-76b9t to node4
Normal  SuccessfulMountVolume  3m    kubelet, node4     MountVolume.SetUp succeeded for volume “default-token-zmxqz”
Normal  Pulling                3m    kubelet, node4     pulling image “registry.saas.hand-china.com/tools/mysql-client:latest”
Normal  Pulled                 3m    kubelet, node4     Successfully pulled image “registry.saas.hand-china.com/tools/mysql-client:latest”
Normal  Created                3m    kubelet, node4     Created container
Normal  Started                3m    kubelet, node4     Started container在这执行出现问题了[Step 9]: create database for Choerodon …
Error: Job failed: DeadlineExceeded
 create database failed[root@node1 tmp]# kubectl  get pod -n choerodon-devops-prod
NAME                               READY     STATUS     RESTARTS   AGE
choerodon-mysql-5958d6f486-4llkm   1/1       Running    0          1m
create-choerodon-databases-76b9t   0/1       Init:0/1   0          1m  创建数据库一直是这个状态 我看node4节点上运行的registry.saas.hand-china.com/tools/mysql-client:latest 看日志 mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
mysqlcheck: Got error: 2005: Unknown MySQL server host ‘choerodon-mysql’ (-2) when trying to connect
在报这个错误    等一段时间就退出了请耐心等待0.10版本发布，0.10版本将解决此问题，谢谢。0.10 版本安装 部署Choerodon
[Step 6]: install nfs client provisioner …
Release “nfs-client-provisioner” has been upgraded. Happy Helming!
LAST DEPLOYED: Sun Oct 14 22:06:45 2018
NAMESPACE: c7n-system
STATUS: DEPLOYEDRESOURCES:
==> v1/Role
NAME                         AGE
nfs-client-provisioner-role  3m==> v1/RoleBinding
NAME                       AGE
nfs-client-provisioner-rb  3m==> v1beta1/Deployment
NAME                    DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
nfs-client-provisioner  1        1        1           0          3m==> v1/Pod(related)
NAME                                     READY  STATUS             RESTARTS  AGE
nfs-client-provisioner-7985bb4f96-6mk29  0/1    ContainerCreating  0         3m==> v1/StorageClass
NAME             PROVISIONER                          AGE
nfs-provisioner  choerodon.io/nfs-client-provisioner  3m==> v1/ServiceAccount
NAME                       SECRETS  AGE
nfs-client-provisioner-sa  1        3m==> v1/ClusterRole
NAME                       AGE
nfs-client-provisioner-cr  3m==> v1/ClusterRoleBinding
NAME                        AGE
nfs-client-provisioner-crb  3m Success of nfs client provisioner.
➜ nfs-client-provisioner not ready,sleep 5s,check it.
➜ nfs-client-provisioner not ready,sleep 5s,check it.
➜ nfs-client-provisioner not ready,sleep 5s,check it.
➜ nfs-client-provisioner not ready,sleep 5s,check it.你好，  是一直在check吗？    nfs相关信息配置是怎么样的呢？好像很多人卡在了这一步，有的人没有这个问题，这个硬件资源不足的可能性有吗，看起来这一步就是在进行一些数据库创建脚本的执行0.10版本已发布，请按0.10版本进行安装，以上问题都已解决，谢谢运行完ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml的结果
系统内存占用情况执行完sh choerodon-install.sh values.sh的结果
[Step 8]: checking Mysql ready …
 Mysql is ready.[Step 9]: create database for Choerodon …
[debug] Created tunnel using local port: ‘36260’[debug] SERVER: “127.0.0.1:36260”[debug] Original chart version: “”
[debug] Fetched c7n/create-mysql-db to /root/.helm/cache/archive/create-mysql-db-0.1.0.tgz[debug] CHART PATH: /root/.helm/cache/archive/create-mysql-db-0.1.0.tgzError: Job failed: DeadlineExceeded
 create database failed请大神们帮忙分析一下问题出在哪里万分感谢一开始以为硬件资源不足后面把机器加到了3台还是这个样子请按最新安装文档进行安装哈，谢谢http://choerodon.io/zh/docs/installation-configuration/steps/helm/咦，这个好像跟之前的有点区别哦，是什么时候更新上来的亲  昨天发布的哈好勤奋的鱼鱼 我先试试执行完成后是这个样子的，看起来比较正常kubectl create serviceaccount --namespace kube-system helm
Error from server (AlreadyExists): serviceaccounts “helm-tiller” already exists
[root@node4 vagrant]# kubectl create clusterrolebinding helm-tiller-cluster-rule
clusterrolebinding “helm-tiller-cluster-rule” created[root@node4 vagrant]# helm init \$HELM_HOME has been configured at /root/.helm.
Warning: Tiller is already installed in the cluster.
(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)
Happy Helming!
[root@node4 vagrant]#
[root@node4 vagrant]# helm version
Client: &version.Version{SemVer:“v2.8.2”, GitCommit:“a80231648a1473929271764b920a8e346f6de844”, GitTreeState:“clean”}
Server: &version.Version{SemVer:“v2.8.2”, GitCommit:“a80231648a1473929271764b920a8e346f6de844”, GitTreeState:“clean”}看起来这次的更新还比较多，全部重新来一遍试试嗯  确实更新比较多哈只要helm version命令不报错就可以往下执行了哈哦，那往下再走一下，报错再重新来过执行到这里的时候我调查了一下后端在跑的东东 Success of install Mysql for Choerodon.[Step 8]: checking Mysql ready …
 Mysql is ready.[Step 9]: create database for Choerodon …
[debug] Created tunnel using local port: ‘36200’[debug] SERVER: “127.0.0.1:36200”[debug] Original chart version: “”
[debug] Fetched c7n/create-mysql-db to /root/.helm/cache/archive/create-mysql-db-0.1.0.tgz[debug] CHART PATH: /root/.helm/cache/archive/create-mysql-db-0.1.0.tgzkubectl get pod -n choerodon-devops-prod
NAME                               READY     STATUS             RESTARTS   AGE
choerodon-mysql-66446b856b-wnklm   0/1       CrashLoopBackOff   5          4m
create-choerodon-databases-7x2m9   0/1       Init:0/1           0          4mkubectl describe po -n choerodon-devops-prod choerodon-mysql-66446b856b-wnklm
Name:           choerodon-mysql-66446b856b-wnklm
Namespace:      choerodon-devops-prod
Node:           node2/192.168.43.12
Start Time:     Mon, 15 Oct 2018 17:02:17 +0800
Labels:         choerodon.io/infra=mysql
choerodon.io/release=choerodon-mysql
pod-template-hash=2200264126
Annotations:    choerodon.io/metrics-group=mysql
choerodon.io/metrics-path=/metrics
kubernetes.io/created-by={“kind”:“SerializedReference”,“apiVersion”:“v1”,“reference”:{“kind”:“ReplicaSet”,“namespace”:“choerodon-devops-prod”,“name”:“choerodon-mysql-66446b856b”,“uid”:"041d2b45-d059-1…
Status:         Running
IP:             10.233.65.2
Created By:     ReplicaSet/choerodon-mysql-66446b856b
Controlled By:  ReplicaSet/choerodon-mysql-66446b856b
Containers:
choerodon-mysql:
Container ID:   docker://bba921a664ed625c76006325e6a0ab1936b55066a9e3632bd2d050dd6cbb0249
Image:          mysql:5.7.23
Image ID:       docker-pullable://mysql@sha256:1d8f471c7e2929ee1e2bfbc1d16fc8afccd2e070afed24805487e726ce601a6d
Port:           3306/TCP
State:          Waiting
Reason:       CrashLoopBackOff
Last State:     Terminated
Reason:       Error
Exit Code:    1
Started:      Mon, 15 Oct 2018 17:04:25 +0800
Finished:     Mon, 15 Oct 2018 17:04:27 +0800
Ready:          False
Restart Count:  4
Liveness:       tcp-socket :3306 delay=60s timeout=5s period=10s #success=1 #failure=3
Environment:
MYSQL_ROOT_PASSWORD:  root
TZ:                   Asia/Shanghai
open:                 map[MYSQL_PASSWORD:password MYSQL_ROOT_PASSWORD:password MYSQL_USER:choerodon]
Mounts:
/var/lib/mysql from mysql (rw)
/var/run/secrets/kubernetes.io/serviceaccount from default-token-cczx9 (ro)
Conditions:
Type           Status
Initialized    True
Ready          False
PodScheduled   True
Volumes:
mysql:
Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
ClaimName:  choerodon-mysql-pvc
ReadOnly:   false
default-token-cczx9:
Type:        Secret (a volume populated by a Secret)
SecretName:  default-token-cczx9
Optional:    false
QoS Class:       BestEffort
Node-Selectors:  
Tolerations:     
Events:
Type     Reason                 Age              From               MessageNormal   Scheduled              3m               default-scheduler  Successfully assigned choerodon-mysql-66446b856b-wnklm to node2
Normal   SuccessfulMountVolume  3m               kubelet, node2     MountVolume.SetUp succeeded for volume “default-token-cczx9”
Normal   SuccessfulMountVolume  3m               kubelet, node2     MountVolume.SetUp succeeded for volume “choerodon-mysql-pv”
Normal   Pulling                3m               kubelet, node2     pulling image “mysql:5.7.23”
Normal   Pulled                 3m               kubelet, node2     Successfully pulled image “mysql:5.7.23”
Normal   Pulled                 2m (x3 over 2m)  kubelet, node2     Container image “mysql:5.7.23” already present on machine
Normal   Created                2m (x4 over 3m)  kubelet, node2     Created container
Normal   Started                2m (x4 over 3m)  kubelet, node2     Started container
Warning  BackOff                2m (x5 over 2m)  kubelet, node2     Back-off restarting failed container
Warning  FailedSync             2m (x5 over 2m)  kubelet, node2     Error syncing pod先分析一下上面的情况是正常的不看起来还是会Error: Job failed: DeadlineExceeded
不过这次执行的过程比较久，一会执行结果出来再贴上来有点差异，结果是这样的 Success of install Mysql for Choerodon.[Step 8]: checking Mysql ready …
 Mysql is ready.[Step 9]: create database for Choerodon …
[debug] Created tunnel using local port: ‘36200’[debug] SERVER: “127.0.0.1:36200”[debug] Original chart version: “”
[debug] Fetched c7n/create-mysql-db to /root/.helm/cache/archive/create-mysql-db[debug] CHART PATH: /root/.helm/cache/archive/create-mysql-db-0.1.0.tgzError: timed out waiting for the condition
 create database failed这次的文档更新不错哦， 加了选项的角标，更清晰了报完错之后就一直是这个样子了kubectl get pod -n choerodon-devops-prod
NAME                               READY     STATUS             RESTARTS   AGE
choerodon-mysql-66446b856b-wnklm   0/1       CrashLoopBackOff   6          8m
create-choerodon-databases-7x2m9   0/1       Init:0/1           0          8m看起来kube-system的运行都正常的请按0.10版本进行安装，不要再参照0.9版本安装，谢谢OK, 我先重新走一遍，感谢您的宝贵时间Choerodon平台版本: 0.10.0遇到问题的执行步骤: web打开敏捷管理文档地址:环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请再次执行敏捷服务升级命令即可helm upgrade agile-service c7n/agile-service 
–set env.open.JAVA_OPTS="-Xms256M -Xmx512M" 
-f <(helm get values agile-service) 
–set env.open.SPRING_REDIS_HOST=c7n-redis.c7n-system.svc 
–set env.open.SPRING_REDIS_DATABASE=4 
–version 0.10.1
在执行这个？对 是的执行了，还是一样报错。。这样试一试可以了。谢谢Choerodon平台版本: 0.10.0遇到问题的执行步骤:
一键安装的时候 重复出现
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):
CPU： 12核
内存： 96 GB
操作系统： CentOS 7.4 64位报错日志:
…
 Success create database for Choerodon.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.[Step 15]: install kafka …
…
…
…
[Step 18]: install Choerodon manager service …
Error: Job failed: BackoffLimitExceeded
 Success of install Choerodon manager service.
➜ config-server not ready,sleep 5s,check it.
➜ config-server not ready,sleep 5s,check it.
 Choerodon config server is ready.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “manager-service” not found
➜ manager-service not ready,sleep 5s,check it.原因分析:14步的安装zookeeper 重试多次之后 未显示安装成功就跳过了 不知道是否成功
后面的问题跟这个有无关联？疑问:1.重试安装的时候 执行helm del --purge  kafka minio minio-pvc register-server zookeeper;  卡住好久不动
2. 安装好几天了 一直都没有成功，系统重置，各种尝试多少次也不行，到如今都没有体验下系统，这个门槛实在是有点高我们现在遇到出现你这种情况是因为服务器网络访问公网拉镜像太慢导致的哈，请在每个节点执行下面语句先把镜像拉好又卡到：
 Success create database for Choerodon.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.执行helm del --purge kafka minio minio-pvc register-server zookeeper; 卡住好久不动请问你这一步删除是不是他还没有执行完你就取消掉了？   他不是卡住了   是在删除中，删除的快慢跟服务器性能有关系。我等了好久没删除完成 我就停了 我现在还有修复的办法吗？ 还是需要重置系统？请执行下面语句   一定等他删除完已经清除干净了  又卡到：
 Success create database for Choerodon.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.
➜ zookeeper not ready,sleep 5s,check it.请问这一步操作了吗？http://choerodon.io/zh/docs/installation-configuration/steps/nfs/第一次执行了 安装失败重新安装的时候 没有再执行经排查是服务器nfs挂载出现问题，umount掉就可以了。Choerodon平台版本: 0.10.0遇到问题的执行步骤: manager-service 升级报错文档地址: http://choerodon.io/zh/docs/installation-configuration/update/0.9-to-0.10/环境信息(如:节点信息):报错日志:
Error: UPGRADE FAILED: jobs.batch “choerodon-manager-service-init-db” already exists原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请删除所有job即可。什么意思？还在使用如何删？请执行以下语句进行删除kubectl delete job -n choerodon-devops-prod --all额，会不会影响现在使用?不会有任何影响的已经执行过 kubectl delete job -n xxx --all又报错:Error: UPGRADE FAILED: Job failed: BackoffLimitExceeded我们现在遇到出现你这种情况是因为服务器网络访问公网拉镜像太慢导致的哈，请在每个节点执行下面语句先把镜像拉好Choerodon平台版本: 0.9.0遇到问题的执行步骤: 升级asgard service文档地址: http://choerodon.io/zh/docs/installation-configuration/update/0.9-to-0.10/环境信息(如:节点信息): k8s报错日志:
Error: UPGRADE FAILED: Job failed: BackoffLimitExceeded原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问HI 执行升级命令后会启动一个新的pod, 粘贴一下那个pod的日志。已经好了。
库写错了manager_service 写成 asgard_service了。Choerodon平台版本: 0.9.0遇到问题的执行步骤:一键安装文档地址:无环境信息(如:节点信息):
报错日志:
choerodon-gitlab-service报错：
2018-10-12 14:06:56.433  WARN [gitlab-service,] 1 — [ XNIO-2 task-32] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.consumer.FailedMsgEventStoreFeign’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-10-12 14:06:56.434  WARN [gitlab-service,] 1 — [ XNIO-2 task-32] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.producer.execute.EventStoreClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-10-12 14:06:56.440 ERROR [gitlab-service,] 1 — [ XNIO-2 task-32] o.s.web.servlet.DispatcherServlet        : Context initialization failedorg.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘org.springframework.boot.autoconfigure.web.MultipartAutoConfiguration’: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘spring.http.multipart-org.springframework.boot.autoconfigure.web.MultipartProperties’: Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5a4aa2f2 has not been refreshed yet
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:749) ~[spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:189) ~[spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1201) ~[spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，无法创建目录具体指的是什么，这个报错是创建环境的时候报的吗Choerodon平台版本: 0.8.0遇到问题的执行步骤:
严格按照文档步骤在阿里云中安装失败，重复实验多次相同错误文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/#部署-1环境信息(如:节点信息):
CPU： 12核
内存： 96 GB
实例类型： I/O优化
操作系统： CentOS 7.4 64位
公网IP： 118.190.147.71
私有IP： 172.31.203.100报错日志:
[root@choerodon-master kubeadm-ansible]# kubectl get po -n kube-system
NAME                                        READY     STATUS    RESTARTS   AGE
default-http-backend-6dd4d5b7c9-sf9d4       0/1       Pending   0          11m
heapster-746d67c7b9-7dzkw                   0/1       Pending   0          11m
kube-apiserver-choerodon-master             0/1       Pending   0          2s
kube-controller-manager-choerodon-master    0/1       Pending   0          2s
kube-dns-79d99555df-hv7nc                   0/3       Pending   0          11m
kube-lego-6f45757db7-mc9jz                  0/1       Pending   0          11m
kube-proxy-6w2p6                            1/1       Running   0          11m
kube-scheduler-choerodon-master             0/1       Pending   0          2s
kubernetes-dashboard-dc8fcdbc5-d66d9        0/1       Pending   0          11m
nginx-ingress-controller-5d77d4945d-sjx24   0/1       Pending   0          11m[root@choerodon-master kubeadm-ansible]# kubectl describe po -n kube-system default-http-backend-6dd4d5b7c9-sf9d4
Name:           default-http-backend-6dd4d5b7c9-sf9d4
Namespace:      kube-system
Node:           
Labels:         app=default-http-backend
pod-template-hash=2880816375
Annotations:    kubernetes.io/created-by={“kind”:“SerializedReference”,“apiVersion”:“v1”,“reference”:{“kind”:“ReplicaSet”,“namespace”:“kube-system”,“name”:“default-http-backend-6dd4d5b7c9”,“uid”:"835708c1-cede-11e8-a…
Status:         Pending
IP:
Created By:     ReplicaSet/default-http-backend-6dd4d5b7c9
Controlled By:  ReplicaSet/default-http-backend-6dd4d5b7c9
Containers:
default-http-backend:
Image:  registry.cn-hangzhou.aliyuncs.com/choerodon-tools/defaultbackend:1.4
Port:   8080/TCP
Limits:
cpu:     10m
memory:  20Mi
Requests:
cpu:        10m
memory:     20Mi
Liveness:     http-get http://:8080/healthz delay=30s timeout=5s period=10s #success=1 #failure=3
Environment:  
Mounts:
/var/run/secrets/kubernetes.io/serviceaccount from default-token-j2nrg (ro)
Conditions:
Type           Status
PodScheduled   False
Volumes:
default-token-j2nrg:
Type:        Secret (a volume populated by a Secret)
SecretName:  default-token-j2nrg
Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  
Tolerations:     node-role.kubernetes.io/master:NoSchedule
Events:
Type     Reason            Age                From               Message[root@choerodon-master kubeadm-ansible]# kubectl describe po -n kube-system kube-dns-79d99555df-hv7nc
Name:           kube-dns-79d99555df-hv7nc
Namespace:      kube-system
Node:           
Labels:         k8s-app=kube-dns
pod-template-hash=3585511189
Annotations:    kubernetes.io/created-by={“kind”:“SerializedReference”,“apiVersion”:“v1”,“reference”:{“kind”:“ReplicaSet”,“namespace”:“kube-system”,“name”:“kube-dns-79d99555df”,“uid”:"83570e41-cede-11e8-aaa2-00163e08…
Status:         Pending
IP:
Created By:     ReplicaSet/kube-dns-79d99555df
Controlled By:  ReplicaSet/kube-dns-79d99555df
Containers:
kubedns:
Image:  registry.cn-hangzhou.aliyuncs.com/choerodon-tools/k8s-dns-kube-dns-amd64:1.14.5
Ports:  10053/UDP, 10053/TCP, 10055/TCP
Args:
–domain=cluster.local.
–dns-port=10053
–config-dir=/kube-dns-config
–v=2
Limits:
memory:  170Mi
Requests:
cpu:      100m
memory:   70Mi
Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
Environment:
PROMETHEUS_PORT:  10055
Mounts:
/kube-dns-config from kube-dns-config (rw)
/var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-dg46j (ro)
dnsmasq:
Image:  registry.cn-hangzhou.aliyuncs.com/choerodon-tools/k8s-dns-dnsmasq-nanny-amd64:1.14.5
Ports:  53/UDP, 53/TCP
Args:
-v=2
-logtostderr
-configDir=/etc/k8s/dns/dnsmasq-nanny
-restartDnsmasq=true
–
-k
–cache-size=1000
–log-facility=-
–server=/cluster.local/127.0.0.1#10053
–server=/in-addr.arpa/127.0.0.1#10053
–server=/ip6.arpa/127.0.0.1#10053
Requests:
cpu:        150m
memory:     20Mi
Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
Environment:  
Mounts:
/etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
/var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-dg46j (ro)
sidecar:
Image:  registry.cn-hangzhou.aliyuncs.com/choerodon-tools/k8s-dns-sidecar-amd64:1.14.5
Port:   10054/TCP
Args:
–v=2
–logtostderr
–probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
–probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
Requests:
cpu:        10m
memory:     20Mi
Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
Environment:  
Mounts:
/var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-dg46j (ro)
Conditions:
Type           Status
PodScheduled   False
Volumes:
kube-dns-config:
Type:      ConfigMap (a volume populated by a ConfigMap)
Name:      kube-dns
Optional:  true
kube-dns-token-dg46j:
Type:        Secret (a volume populated by a Secret)
SecretName:  kube-dns-token-dg46j
Optional:    false
QoS Class:       Burstable
Node-Selectors:  
Tolerations:     CriticalAddonsOnly
node-role.kubernetes.io/master:NoSchedule
Events:
Type     Reason            Age                From               MessageWarning  FailedScheduling  4m (x37 over 14m)  default-scheduler  No nodes are available that match all of the predicates: NodeNotReady (1).[all]
choerodon-master ansible_host=172.31.203.100 ip=172.31.203.100 ansible_user=root ansible_ssh_pass=u9B!176rUwrE ansible_become=true[kube-master]
choerodon-master[etcd]
choerodon-master[kube-node]
choerodon-master原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:是不是我对文档的理解有误 请百忙之中给予指导，感谢！Hi, 很抱歉由于文档的疏忽对您造成困扰， 如果您使用阿里云主机，并且按照阿里云的配置方式配置kubernetes，在您的网络未配置完成前 node将处理node ready状态， 你继续执行后续步骤即可，执行完成后再检查pod状态。ok ，亲测有效！ 问题解决Choerodon平台版本：0.9.0运行环境：一键安装问题描述：部署流水线无法删除容器请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：使用了一下迭代管理，到冲刺发布以后就结束了。 之前还想着是需求和feature相关之类的。迭代发布会和release分支关联在一起执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块或系统截图的形式展现)：提出您认为不合理的地方，帮助我们优化用户操作迭代和release版本发布是两码事，不是每个迭代都要发布的，可以多个迭代做一个版本的发布。每个迭代的发布可以是简单的用作演示测试版本，不涉及发布流程，所以设计时没有做关联。
截图中的报错是哪个版本？新版本已经优化了这种卡在处理中实例的问题，但是您截图中没有更详细一点的报错或者其他之类的信息，造成这种情况的原因比较多，一般是创建过程中出了问题，例如超时、部署应用的问题等等我的0.9.0 版本，部署没有成功，所以想删除一下。按了删除一直在处理中。但我去K8S上是已经删除的了。不知道这个会不会影响什么？没有什么影响，数据库删了就好，这个是程序的bug，在新的版本已经修复了，十分抱歉Choerodon平台版本: 0.8.0遇到问题的执行步骤: 执行docker-compose命令环境信息(如:节点信息):docker-compose.yaml但是redis已经启动了已经解决，原来是没有添加配置信息。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
一键部署环境信息(如:节点信息):
k8s 1.11报错日志:
[Step 24]: install Choerodon oauth server …
Error: Job failed: DeadlineExceeded
 Success of install Choerodon oauth server.[Step 25]: install Choerodon file service …
Error: a release named choerodon-file-service already exists.
Run: helm ls --all choerodon-file-service; to check the status of the release
Or run: helm del --purge choerodon-file-service; to delete it
 Success of install Choerodon file service.[Step 26]: install Choerodon devops service …
Error: failed to download “c7n/devops-service”
 Success of install Choerodon devops service.你好，我们建议使用的k8s版本为1.8.5，helm版本为2.8.2，其余版本未经测试，不建议使用，谢谢。k8s版本是我们已经在线上使用了。如果choerodon整个部署在1.8.5上。实际应用跑在1.11上
这样子会有问题吗？choerodon整个部署在1.8.5上，他所管理的实际应用理论上跑在任何版本上的k8s都没有问题。实质上有没有问题是和实际应用自身能否在该版本k8s上运行有关，与choerodon是没有关系的。明白了   那我把choerodon独立一个集群试试。 谢谢choerodon-notify-service-d8dff7b94-64ln7                0/1       CrashLoopBackOff   4          9m
choerodon-notify-service-init-config-6wqqd              0/1       Completed          0          1h
choerodon-notify-service-init-db-rl6sm                  0/1       Completed          0          1h
choerodon-oauth-server-86f8894b4f-jqkrs                 0/1       CrashLoopBackOff   5          13m
choerodon-oauth-server-init-config-82hcp                0/1       Completed          0          14mchoerodon-notify-service、choerodon-oauth-server
发现这两个服务一直OOMKILLED这大概率是内存不足的情况哈，请添加节点。官方建议整套下来多少资源。http://choerodon.io/zh/docs/installation-configuration/pre-install/3台master
3台node，232G+118G竟然跑不起来k8s切换到1.8版本还是有好几处这种错误Error: Job failed: DeadlineExceeded[Step 21]: install Choerodon iam service …
Error: Job failed: DeadlineExceeded
 Success of install Choerodon iam service.[Step 23]: install Choerodon notify service…
Error: Job failed: DeadlineExceeded
 Success of install Choerodon notify service.硬件资源是什么样子的，贴出来睇睇关于猪齿鱼，我们现在有一个定时任务。
猪齿鱼有没有提供不通过表单验证，获取token的方法呢？你好，我使用上述方式，进行了测试。但是会数组越界的异常。
密码需要采用加密后的形式才可以请问，keyStr是什么参数？例子中是没有定义的。可以参考这里


github.com


choerodon/oauth-server/blob/master/src/main/resources/public/static/captcha.js
function changeImg() {
    var imgSrc = $("#imgObj");
    var src = imgSrc.attr("src");
    imgSrc.attr("src", chgUrl(src));
}

function chgUrl(url) {
    var timestamp = (new Date()).valueOf();
    return 'public/captcha?code=' + timestamp;
}

var keyStr = "ABCDEFGHIJKLMNOP" + "QRSTUVWXYZabcdef" + "ghijklmnopqrstuv"
    + "wxyz0123456789+/" + "=";

$(function () {
    if (/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
        window.location.href = "/oauth/login?device=mobile";
    }
    document.onkeydown = function (event) {
        var e = event || window.event;


  This file has been truncated. show original





Choerodon平台版本: 0.9.0遇到问题的执行步骤: SaaS环境侧边栏和正文的分割线有问题。查看了敏捷、开发流水线、部署流水线等，发现都有这个问题。Google Chrome 版本 69.0.3497.100（正式版本） （64 位）初步判断是浏览器有缩放， 请crtl+0还原原尺寸Caused by: feign.FeignException: status 404 reading OauthFeignClient#clientAccess(String)
at feign.FeignException.errorStatus(FeignException.java:62)
at feign.codec.ErrorDecoder$Default.decode(ErrorDecoder.java:91)
at feign.SynchronousMethodHandler.executeAndDecode(SynchronousMethodHandler.java:138)
at feign.SynchronousMethodHandler.invoke(SynchronousMethodHandler.java:76)
at feign.hystrix.HystrixInvocationHandler$1.run(HystrixInvocationHandler.java:108)
at com.netflix.hystrix.HystrixCommand$2.call(HystrixCommand.java:302)
at com.netflix.hystrix.HystrixCommand$2.call(HystrixCommand.java:298)
at rx.internal.operators.OnSubscribeDefer.call(OnSubscribeDefer.java:46)
… 28 common frames omitted你好，你调用这个接口是要干什么的？oauth里面应该没有这个接口你好， 我想知道，如何设置一个特殊的永久的token供给一个业务代码使用。猪齿鱼，有办法设置token吗？求执教你好，猪齿鱼是基于oauth2 来做token授权鉴权的，目前可以设置client 的token 失效时限，来增大token的失效时间。但是对于oauth 的token不建议授权时间过长，如果业务场景中确实需要如此，建议在客户端来对token进行验证，如果不通过重新获取授权。你好，可以通过http的方式获取token吗？我们现在有一个定时任务，需要，不通过表单验证的方式获取token。Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：
猪齿鱼平台搭建以后，我们把一些中间件打成chart用猪齿鱼来发布。包括mysql，elk，skywalking,apollo等等。
于是在发布这些chart的时候，遇到了各种问题。其中有一个问题是应用虽然发布成功，实际上也能正常使用，但是猪齿鱼平台上实例里的那个圆环还是红的。所以我想问一下圆环颜色是根据什么来变化的，是通过choerodon-agent实时获取k8s集群里应用的状态吗？ 获取什么状态呢因为看起来都停正常这些中间件应用charts都是有requirement的，算比较复杂的charts，在部署过程中，有些service会漏掉，该怎么排查？能简单讲下部署过程吗？devops-service  choerodon-agent  gitops等都做了什么？1，圆环颜色是根据什么来变化的： 这个是根据这个chart中总的pod数来显示的，异常的pod显示为红色，正常的显示绿色。
正常的定义：通过k8s的健康检查2，主chart,及requirement导入的chart，在gitops执行时都会注入label,用来发现service,应该不会出现这种漏掉的情况，你这边是否可以提供一些例子，供我们测试验证这种漏掉的情况。3，gitops分为以下几个阶段:
1)修改环境配置库
2)Devops平台收到配置库修改通知，解释修改后的配置库内容，解释完成后，通知choerodon-agent
3)choerodon-agent收到通知后，拉取最新的配置库内容，将差异调用k8s api更新到环境中你好！
输入网址http://example.choerodon.io后回车，一片空白，没有任何显示你好，请根据这里教程进行排查   是否有哪一步操作错误http://choerodon.io/zh/docs/installation-configuration/steps/dns/我是按照那个设置的dns，验证也能成功！
[root@node2 ~]# kubectl exec -n choerodon-devops-prod $(kubectl get po -n choerodon-devops-prod -l choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host example.choerodon.io
example.choerodon.io has address 192.168.177.27
我的主机也能ping通example.choerodon.io;输入网址：http://example.choerodon.io回车后一片空白，但是猪齿鱼的图标是显示出来了的，如下：
不好意思，图片上传错误，应该是这张图片
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-front/
在分步部署的“整合前端”步骤:其中有个验证步骤，你们的验证输出和我的验证输出不一样，
你的验证输出是：我的验证输出是：不知道问题是不是出在这里，我的是一键部署的choerodon你好，在一键部署和分步部署中都有提示的哈。你好！登陆进去后报错！
你好，请根据这里教程  手工添加client后重试http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-front/你好！我把以下数据提交了还是不行，登陆后报同样的错误！
OAuth Error
error=“invalid_client”, error_description=“Bad client credentials”
下面的数据是否需要修改？
{
“accessTokenValidity”: 60,
“additionalInformation”: “”,
“authorizedGrantTypes”: “implicit,client_credentials,authorization_code,refresh_token”,
“autoApprove”: “default”,
“name”: “choerodon”,
“objectVersionNumber”: 0,
“organizationId”: 1,
“refreshTokenValidity”: 60,
“resourceIds”: “default”,
“scope”: “default”,
“secret”: “secret”,
“webServerRedirectUri”: “http://choerodon.example.choerodon.io”
}
还有就是：认证请使用用户名：admin，密码：admin
这个用户名和密码在哪里设置？还是本来就已经设置好了的?Choerodon平台版本: 0.9.6遇到问题的执行步骤:
部署完成后，无法在猪齿鱼的网络中发现Service，helm文件大致如下：
kind: Service
apiVersion: v1
metadata:name: {{ template “x-apollo-portal.fullname” . }}-portal
labels:
app: {{ template “chart.name” . }}-portal
chart: {{ include “chart.chart” . }}
release: {{ .Release.Name }}
heritage: {{ .Release.Service }}
spec:
ports:selector:
app: {{ template “chart.name” . }}-portal
release: {{ .Release.Name }}
type: {{ .Values.apolloportal.service.type }}sessionAffinity: ClientIP查看数据库后，发现如下问题，烦请说明出现原因
devops_env_resource 表中未出现任何有关 Service 的kind，我通过查看源代码后发现如下一段代码：
请问是否就是这里出现这个问题？我应该如何解决？文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问您好，现在平台只支持存储通过平台创建的网络，在chart包里面定义的网络由于没有带有平台网络的标签。所以被过滤掉了，我们会在下个版本支持存储chart里面创建的网络。
可是我其他的应用没有做其他的特殊设置，就可以发现网络了，而且你说chart包里面没有带平台网络的标签的意识是否是如下图的这几个标签：
我看过这几个标签，这都通过猪齿鱼平台创建应用的时候，自动加上的，但是我看过和其他同样是通过猪齿鱼部署的应用，标签完全一样，没有缺失，但是其他应用就可以正常进行网络的发现，chart里面service的模版是一模一样的。
按照正常推断，应该同样能够发现网络啊，我觉得你的回答可能存在问题。Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：
在创建用户时，忘记修改初始密码了，使用了页面默认的密码。但由于不知道默认密码，导致新用户就无法登陆了，在猪齿鱼官方文档中也没有找到默认密码的说明。建议：
1、文档中请写明默认密码。
2、加入密码找回策略
3、加入管理员可以重置密码功能你好1、新用户创建的默认密码为创建用户对应组织的密码策略中的默认密码，如果没有开启密码策略的话，采取系统默认密码，系统默认密码为abcd1234。文档中缺失是我们的遗漏，我们会补全这一部分的。2、最新版本的猪齿鱼已经添加了密码找回的功能，可以通过配置平台邮箱来根据用户邮箱找回密码。3、对于管理员重置密码的功能点，我们考虑到组织管理员可能会存在多个，如果可以修改组织内个人用户的密码，可能会引起用户信任问题，所以暂时不会添加对应的功能。你好！我的choerodon一键部署完毕，根据你们提供的操作步骤，我登陆了gitlab的主页，但是无法查询到gitlab的版本号，这是什么原因啊？你好，谢谢，我查到了Choerodon平台版本: 0.9.0背景
我们使用了阿里云的K8s容器服务，k8s的版本是1.11.2。
所以kube-lego没有预先安装。看了下它也只是支持到1.8。它的接任者是cert-manager。问题
1.在进行gitlab-ci的时候，报registry.xx.xxx/v2找不到。
这里为什么要用/v2,我看到harbor创建了两个ingress。registry.xx.xxx/v2  和registry.xx.xxx 都映射到了harbor-ui 80
关于背景：Choerodon安装文档中所介绍的证书申请方式是我们测试过的能够成功的方式，cert-manager将在下一个安装文档版本进行讲解，我们正在测试当中。问题1 、2
答：这两个问题实质上是一个问题。解决此报registry.xx.xxx/v2找不到问题请删除只有一个域名的ingress即可。由于harbor1.5.0在k8s1.9+版本会出现文件系统权限问题，故harbor官方提供了此解决方案。Choerodon平台版本：0.8.0运行环境：自主搭建使用sonarqube执行代码检查，在执行gitlab-ci时，sonarqube检查失败(出现bug)，但是为什么执行ci的这个job还是通过了。也没有发送邮件，请问这个需要做什么配置才能在ci时报错吗？sonarqube检查出现失败,执行sonarqube的那个job是不会失败的，当此时sonarqube出现bug之类的问题时，会额外出现一个job提示您，本次sonarqube有那些错误具体错误可进入平台,应用管理－>应用,点击要查看应用的代码质量，里面有sonarqube的具体分析使用官方的猪齿鱼平台，但是参考猪齿鱼文档自搭了sonarqube，为什么没有生成一个额外的错误job，请问需要在sonarqube上设置什么吗？

它只有两个job，没有生成external job你可以参照这里进行设置，在你自己搭建的sonarqube安装Gitlab插件，并配置相应token及url即可。



GitHub



gabrie-allaigre/sonar-gitlab-plugin
Add to each commit GitLab in a global commentary on the new anomalies added by this commit and add comment lines of modified files - gabrie-allaigre/sonar-gitlab-plugin





Add to each commit GitLab in a global commentary on the new anomalies added by this commit and add comment lines of modified files - gabrie-allaigre/sonar-gitlab-plugintoken用的是什么，是用户的token吗？我使用这个好像还是没有用
是gitlab用户的token哈，请注意你取得token所对应的用户在项目下的权限，建议为master。你好！ 请问choerodon的一键部署脚本是否有改动，昨天下午我部署成功了，今天重新部署就不行了，好奇怪！没有改动哦你好！
请问在一键部署的values.sh配置文件中，这个几个密码有关联性吗？我的密码按照以下设置是否正确？没有关联的哦，好的，谢谢！
我通过一键部署脚本搭建好了，发现有一个pod总是运行不正常
choerodon-devops-prod   choerodon-gitlab-595b5769d5-4m74q                        0/1       Running   1          12m
我查看了该pod的日志信息，日志报错如下，请问是什么原因啊？
==> /var/log/gitlab/gitaly/current <==
2018-10-09_09:48:53.47481 time=“2018-10-09T09:48:53Z” level=info msg=“dialing to target with scheme: “”” system=system
2018-10-09_09:48:53.47484 time=“2018-10-09T09:48:53Z” level=info msg=“ccResolverWrapper: sending new addresses to cc: [{/tmp/gitaly-ruby870658504/socket.0 0  }]” system=system
2018-10-09_09:48:53.47485 time=“2018-10-09T09:48:53Z” level=info msg=“ClientConn switching balancer to “pick_first”” system=system
2018-10-09_09:48:53.47499 time=“2018-10-09T09:48:53Z” level=info msg=“pickfirstBalancer: HandleSubConnStateChange: 0xc4204077e0, CONNECTING” system=system
2018-10-09_09:48:53.47528 time=“2018-10-09T09:48:53Z” level=info msg=“pickfirstBalancer: HandleSubConnStateChange: 0xc4204077e0, READY” system=system
2018-10-09_09:48:53.47764 time=“2018-10-09T09:48:53Z” level=info msg=“dialing to target with scheme: “”” system=system
2018-10-09_09:48:53.47782 time=“2018-10-09T09:48:53Z” level=info msg=“ccResolverWrapper: sending new addresses to cc: [{/tmp/gitaly-ruby870658504/socket.1 0  }]” system=system
2018-10-09_09:48:53.47783 time=“2018-10-09T09:48:53Z” level=info msg=“ClientConn switching balancer to “pick_first”” system=system
2018-10-09_09:48:53.47783 time=“2018-10-09T09:48:53Z” level=info msg=“pickfirstBalancer: HandleSubConnStateChange: 0xc4203ce280, CONNECTING” system=system
2018-10-09_09:48:53.47810 time=“2018-10-09T09:48:53Z” level=info msg=“pickfirstBalancer: HandleSubConnStateChange: 0xc4203ce280, READY” system=system==> /var/log/gitlab/nginx/gitlab_error.log <==
2018/10/09 17:49:01 [error] 509#0: *91 connect() to unix:/var/opt/gitlab/gitlab-workhorse/socket failed (111: Connection refused) while connecting to upstream, client: 10.233.66.1, server: gitlab.example.choerodon.io, request: “GET /help HTTP/1.1”, upstream: “http://unix:/var/opt/gitlab/gitlab-workhorse/socket:/help”, host: “10.233.66.66:80”==> /var/log/gitlab/nginx/gitlab_access.log <==
10.233.66.1 - - [09/Oct/2018:17:49:01 +0800] “GET /help HTTP/1.1” 502 2916 “” “kube-probe/1.8”请耐心等待，gitlab启动会消耗很长时间。执行过程报错
从日志了解到需要从这个registry 里面获取  docker image, 但是这个域 名应该是打不开的，大家有遇到这个问题吗， 如何解？谢谢你好，是不是报错的信息没有粘贴进来，并没有报错信息哈registry.saas.hand-china.com 这个镜像库是可以访问的哈
在官网文档中，如上图中的GET方式进行传输数据时, GET请求如何传输Map格式的数据呢？你好，这是我们文档的错误，很抱歉给你带来理解上的错误，我们会尽快修正，谢谢！下载了好久都下载不完， 用迅雷下载的速度都只有不到100K，本地网络下载国外资源都可以到10M的速度，现在卡在PULL MYSQL 5.7.22这里了
这个有办法解吗请参考这里进行设置，谢谢。https://www.docker-cn.com/registry-mirror记得在choerodon的安装过程 里面已经加上这个了我现在是先在服务器端pull回来，再save 出来，再到 container里面 load进去你的意思是你在阿里云服务器上pull了镜像然后save成tar包，之后你想通过迅雷下载到本地，结果你发现下载tar包很慢是吗？是啊， 从ALIYUN上下载不管是自已的服务器还是registry开头的那个网址都会很慢，大家在本地搭建的时候没有这么慢的吗？从你自己阿里云的服务器上下载tar包时很慢是因为你购买ECS时配置网络带宽就很小，所以下载很慢，请咨询阿里云客服如何后期调整网络带宽。
Choerodon文档中提供的镜像地址一部分为DockerHub上的官方镜像，由于众所周知的网络原因这一部分镜像下载比较缓慢，故通过https://www.docker-cn.com/registry-mirror文档进行设置，设置后网络将不会出现问题。另一部分registry开头的镜像使用的是阿里云提供的镜像仓库，故在国内是不会很慢的。很好，非常感谢！现在所有需要的文件都已经下载到本地，应该过程比较快了，我再试试，有问题再来找你。大家好！
谁有choerodon的一键部署脚本啊，官网的部署脚本老是部署失败能说下报什么错吗，我们正在优化安装脚本你好！现在通过一键部署脚本安装上了，这些（微服务开发框架部署，持续交付部署，敏捷管理部署，测试管理部署，知识管理部署，整合前端）需要单独安装吗？还是已经包含到一键部署脚本里面了？除了知识管理都包含了哦正在准备一个一键启动vagrant自动部署的脚本，网速太慢，所需资源无法快速下载回来，有需要留下联系方式，后面发出来哦，好的，谢谢！我再试试！电脑没有重启前，pod运行正常，电脑重启后，好多的pod都运行错误，这是为什么啊？主机重启之后 在改主机上的容器会分到其他主机 你需要等待POD重启完成在测试猪齿鱼时,发现一直报出这样的异常.使用的版本是0.9.6.麻烦问一下这什么问题,配置吗?但是服务是正常的.16:02:50.764 [XNIO-3 task-18] ERROR io.undertow.request - UT005023: Exception handling request to /v1/projects/25/app_instances/400/resources
org.springframework.web.util.NestedServletException: Request processing failed; nested exception is java.lang.NullPointerException
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:982)
at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.choerodon.resource.filter.JwtTokenFilter.doFilter(JwtTokenFilter.java:101)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:111)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:208)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:64)
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132)
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:336)
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException: null
at sun.reflect.GeneratedConstructorAccessor442.newInstance(Unknown Source)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:598)
at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735)
at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
at io.choerodon.devops.app.service.impl.DevopsEnvResourceServiceImpl.listResources(DevopsEnvResourceServiceImpl.java:51)
at io.choerodon.devops.api.controller.v1.ApplicationInstanceController.listResources(ApplicationInstanceController.java:294)
at sun.reflect.GeneratedMethodAccessor2082.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133)
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738)
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
… 78 common frames omitted
Caused by: java.lang.NullPointerException: nullio.choerodon.devops.app.service.impl.DevopsEnvResourceServiceImpl.listResources(DevopsEnvResourceServiceImpl.java:51)
at io.choerodon.devops.api.controller.v1.ApplicationInstanceController.listResources(ApplicationInstanceController.java:294)就看到这里npe了 应该是有其它原因导致的另 小建议一下 代码健壮性加强一下哈io.choerodon.devops.app.service.impl.DevopsEnvResourceServiceImpl.listResources(DevopsEnvResourceServiceImpl.java:51)
at io.choerodon.devops.api.controller.v1.ApplicationInstanceController.listResources(ApplicationInstanceController.java:294)感谢您的建议，我们会立刻查找这个NPE的原因并修复，我们会努力加强代码的健壮性！Choerodon平台版本: v0.6遇到问题:我使用的是分步安装，当部署到runner的时候，发现需要配置sonarqube的地址，而在文档中我并没有找到，大佬们，可以麻烦提供一下吗。文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/gitlab-runner/#部署runner如果你的程序无需sonarqube检查则无需配置这个地址sonarqube非Choerodon的必须组件，我们已暂时的将相关搭建文档移除以减少安装步骤，这里你无需设置这个变量即可。你好，请问如果使用官方提供的猪齿鱼平台。可以使用自己搭建的sonarqube进行代码检查吗？我们可能需要定制化代码检查是可以的哈，请在所需设置的项目下添加Gitlab CI Variables，变量名为SONAR_URL，值为你自己搭建的sonarqube地址。为什么使用自搭的sonarqube的时候，sonarqube代码检查在gitlab中不是external job，而是在mvn package这个job中显示（这样如果代码检查不通过，其他的job就无法通过了）。搭建的方式使用的是猪齿鱼文档搭建方式，请问需要在sonarqube上设置什么吗？有没有参考文档？
,
@xinghao .   你可以参照这里进行设置，在你自己搭建的sonarqube安装Gitlab插件，并配置相应token及url即可。



GitHub



gabrie-allaigre/sonar-gitlab-plugin
Add to each commit GitLab in a global commentary on the new anomalies added by this commit and add comment lines of modified files - gabrie-allaigre/sonar-gitlab-plugin





Add to each commit GitLab in a global commentary on the new anomalies added by this commit and add comment lines of modified files - gabrie-allaigre/sonar-gitlab-pluginChoerodon平台版本：猪齿鱼 SaaS https://choerodon.com.cn/运行环境：自主搭建问题描述：登陆猪齿鱼 SaaS，并导航至 部署流水线 > 域名，创建完域名之后，记录一直处于“操作中”，无法对其进行操作，提示“域名操作中请耐心等待”。K8S 集群内部已证实 Ingress 已经创建。你好这个bug我们已经修复了，您项目下的域名状态应该也已近恢复正常了。你好，刚刚我看了一下，我项目下的两个域名仍然在转圈圈。
抱歉，环境客户端是需要升级的，但是由于这边有个bug，没有提示升级环境客户端，需要先在k8s环境中将环境客户端删除，再按照界面上的指令，部署对应版本的环境客户端。升级完正常了，不过之前创建的网络也删不掉了。。。之前发生了很多次这种情况，导致环境、资源对象等等都删不掉，一直在那里转圈。。这个体验性并不是很好啊。不知道框架组是否有能改进一下呢？这里我们改进了，能删掉，在处理中的经过，这次会有一个修复机制，不会一直在处理的。Choerodon平台版本: 0.9.0疑问:提出您对于遇到和解决该问题时的疑问看到安装教程里与devops出现两个地址
devops.service.example.choerodon.io
devops.example.choerodon.io
这两个域名有啥区别？分别是分布部署和一键部署中的示例地址，本质上没有区别。OKChoerodon平台版本: 0.9.0遇到问题的执行步骤:
1、发现swagger中平滑升级的接口有2个 ，调用了 [GET][/v1/upgrade] 这个。查询devops-service的devops-check-log表时发现有报错信息。之后查看平台"Choerodon"的事务实例时也发现有报错。文档地址:
http://choerodon.io/zh/docs/installation-configuration/update/0.8-to-0.9/环境信息(如:节点信息):
4节点  32G报错日志:
java.lang.NullPointerException
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:598)
at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677)
at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735)
at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
at io.choerodon.devops.app.service.impl.GitlabGroupMemberServiceImpl.createGitlabGroupMemberRole(GitlabGroupMemberServiceImpl.java:55)
at io.choerodon.devops.api.eventhandler.SagaHandler.handleGitlabGroupMemberEvent(SagaHandler.java:142)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.invoke(SagaMonitor.java:194)
at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.run(SagaMonitor.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
at io.choerodon.devops.app.service.impl.GitlabGroupMemberServiceImpl.operation(GitlabGroupMemberServiceImpl.java:175)
at io.choerodon.devops.app.service.impl.GitlabGroupMemberServiceImpl.lambda$createGitlabGroupMemberRole$1(GitlabGroupMemberServiceImpl.java:61)
at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)
at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)createGitlabGroupMemberRole你好，请问一下devops-service版本，还有点开一下错误的事物实例详情看看。devops-service的版本时 0.9.3
这个上面的报错就是点开 iam-update-memberRole 这个里面的报错信息应该是系统中存在个别iam用户在devops和gitlab这边不存在，导致这些用户用户角色同步的时候报错了。如果其他正常的用户角色应该是已经同步成功了，现在升级之后看一下你们项目的部署操作能否正常进行。Choerodon平台版本: 0.9.9遇到问题的执行步骤:
systemctl start nfs-server文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/nfs/环境信息(如:节点信息):报错日志:
原因分析:服务器IP为10.211.55.13
[root@node1 ~]# cat /etc/exports
/u01/choerodon/nfs 10.211.0.0/16(rw,sync,no_root_squash,no_all_squash)
[root@node1 ~]#你在node1上建了nfs然后在node1的同一个目录绑定了node1的nfs?我在k8s服务器部署nfs没有成功，我就继续部署了客户端？然后就报错了，请问怎样解决呢？你需要安装文档顺序操作， 成功之后再执行下一步我就是按照文档上的安装顺序来的，但是由于安装服务端出错了，我就继续往下做了，请问现在要怎样处理呢？你需要反向操作到搭建NFS服务端的地方请问有具体的命令和方法吗？这个需要根据你的实际情况，取消挂载之后重试哦，我们没法直接提供命令。NFS服务器端和客户端可以同时部署到K8s的那台服务器上吗？我的客户端并没有挂载成功。在同一服务器上就不需要进行 客户端挂载 了。nfs是多节点共享资源的网络文件系统。如果你只有一个节点就只需要在这份节点上搭建一下nfs服务端就可以了。更多nfs信息请查看这里： http://cn.linux.vbird.org/linux_server/0330nfs.phpChoerodon平台版本: 0.9.9遇到问题的执行步骤:
cd kubeadm-ansible
export ANSIBLE_HOST_KEY_CHECKING=False
ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml -K文档地址:环境信息(如:节点信息):报错日志:
疑问:为什么每次执行都报错?麻烦把前后的日志都贴一下看报错信息推断应该是你集群有残留的k8s配置及其相关文件，建议你先重置集群删除相关文件，然后再进行部署，重置命令如下，谢谢。可以不用每次都执行这个命令吗？能有什么方法让他不要每次都出现问题吗？你安装k8s的时候一直是那几台机子  还是每次都是全新的机子？就是同一台机器，只是我每次都会吧虚拟机直接关机了，这样会导致我每次都要安装helm，很麻烦。首先我们梳理一下问题你是因为每次再启动虚拟机不能执行helm命令然后得到-bash: helm: 未找到命令这样的报错信息是吗？而后你就再去执行安装k8s集群的命令是吗？我是用安装在本地的方式来安装k8集群的。
安装网址为http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/的 测试环境模式。
现在问题是：
我每次启动center os之后，执行如下命令
cd kubeadm-ansible
export ANSIBLE_HOST_KEY_CHECKING=False
ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml -K
都会报错，每次都需要先执行ansible-playbook -i inventory/hosts reset.yml -K
然后再执行
cd kubeadm-ansible
export ANSIBLE_HOST_KEY_CHECKING=False
ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml -K
才能成功为什么你每次虚拟机启动都要去执行安装命令呢？    你每次是执行的关机，不是销毁虚拟机，所以你第一次搭建了k8s后，后面直接开机等待服务启动后就可以使用了，不需要每次开机还去部署，这也就是你所谓的“每次执行报错”的原因。每次重新开机，k8s服务器会自动启动？对的，会自动启动的。好的，谢谢你helm无法执行的问题是因为你PATH环境变量里面没有包含/usr/local/bin目录导致的，你可以执行下面命令修复我用的是0.6版本的猪齿鱼。。但我现在想迁移到oracle去，测了几个oracle依赖，发现maven都错首先从pom.xml里面找到依赖的版本号，再从 https://mvnrepository.com/ 查找有没有相应的依赖我是这么干的，了解一下。你可以使用choerodon-starter-parent:0.6.3.RELEASE 其中choerodon-starter-mybatis-mapper 已经添加了ojdbc7 的依赖或者说有人干过这个事情了吗,希望能一起讨论下.小白用户哈, 不要嫌弃如果您的Mac有32G以上内存可以试一下哦那我先存够五万之后再开始Choerodon是基于微服务开发的，并通过K8s做容器编排和管理，对内存的需求量比较大，所以，Mac应该不太可以。可以使用阿里云做测试，短期费用还到可以接受。哈哈，太奢侈了。专门用Mac安装猪齿鱼。只有这个环境，推荐的标准配置是什么样子的，可以考虑准备一套环境来跑PAAS了。感谢GANGAN大大！http://choerodon.io/zh/docs/installation-configuration/pre-install/GANGAN大大威武啊， 太NB 了如果用于测试或者跑流程熟悉的话是否可以再精简些配置根据我们测试经验，至少也需要3台 4核心 16G内存。好像是只能在WINDOWS下对不，MAC下面的实践有没这个和平台无关，准确的说只和是不是能够部署K8s有关系。猪齿鱼是运行在K8s上的。明白， 我先来做个MAC的实践友情提示~系统资源不足的话，例如内存等，可能导致各种各样的错误。不管是猪齿鱼，其他系统也都是这样子的。哈哈哈， 明白，先准备好环境了官网只有 Windows 的嗯,我试试看, Mac 哇,会不会掉坑里然后呢,等了2天没看到你更新啊Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，能否描述详细些？是指k8s 容器互相调用的问题？还是微服务调用出现了问题？还是说想了解下choeroodn 服务之间调用的原理？主要想了解choeroodn 服务间的关系
目前是通过helm 部署的choeroodn 所有组件，期间主要卡在微服务开发框架这块儿，文档只是简单写了一些安装命令，组件之间的关系，以及他们主要的作用都没有描述。在manager 和notify这俩服务部署都失败后 ，后续服务也没法部署成功。你好，有关于所有服务的描述在github上可以看到，文档中不全的部分我们会尽快补上The open source PaaS for Kubernetes. Contribute to choerodon/choerodon development by creating an account on GitHub.其中manager 和notify 部署失败有没有更具体的错误信息？@vinkdong描述一下具体的错误信息吧2018-09-26 17:50:59.780  WARN [config-server,] 1 — [           main] s.c.a.AnnotationConfigApplicationContext : Exception thrown from ApplicationListener handling ContextClosedEventorg.springframework.beans.factory.BeanCreationNotAllowedException: Error creating bean with name ‘eurekaAutoServiceRegistration’: Singleton bean creation not allowed while singletons of thi
s factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!)
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:216) [spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:308) ~[spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) ~[spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1080) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.event.ApplicationListenerMethodAdapter.getTargetBean(ApplicationListenerMethodAdapter.java:283) ~[spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.event.ApplicationListenerMethodAdapter.doInvoke(ApplicationListenerMethodAdapter.java:253) ~[spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.event.ApplicationListenerMethodAdapter.processEvent(ApplicationListenerMethodAdapter.java:177) ~[spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.event.ApplicationListenerMethodAdapter.onApplicationEvent(ApplicationListenerMethodAdapter.java:140) ~[spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.event.SimpleApplicationEventMulticaster.doInvokeListener(SimpleApplicationEventMulticaster.java:172) ~[spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:165) ~[spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:139) ~[spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:393) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:399) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:347) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:991) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:958) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.cloud.stream.binder.DefaultBinderFactory.destroy(DefaultBinderFactory.java:84) [spring-cloud-stream-1.2.2.RELEASE.jar!/:1.2.2.RELEASE]
at org.springframework.beans.factory.support.DisposableBeanAdapter.destroy(DisposableBeanAdapter.java:272) [spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroyBean(DefaultSingletonBeanRegistry.java:583) [spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingleton(DefaultSingletonBeanRegistry.java:555) [spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingleton(DefaultListableBeanFactory.java:961) [spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.destroySingletons(DefaultSingletonBeanRegistry.java:516) [spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.destroySingletons(DefaultListableBeanFactory.java:968) [spring-beans-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.destroyBeans(AbstractApplicationContext.java:1032) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:556) [spring-context-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:122) [spring-boot-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:693) [spring-boot-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:360) [spring-boot-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:303) [spring-boot-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:1118) [spring-boot-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.boot.SpringApplication.run(SpringApplication.java:1107) [spring-boot-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at io.choerodon.config.ConfigServerApplication.main(ConfigServerApplication.java:22) [classes!/:0.8.0.RELEASE]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_121]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_121]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_121]config server服务首先抛出这个异常，请问可以定位到问题吗？请问下使用的版本是哪个我就是安装官方安装文档部署的，版本0.9.0
如下 http://choerodon.io/zh/docs/installation-configuration/install-list/是这样的,之前很多问题我都已经解决了。目前定位到最主要的问题是由时区不匹配导致的，我使用你们官网部署文档的helm方式部署出来的mysql、zk 、kafka等基础服务他们系统使用的时区是中部时区，而微服务中的各个服务组件部署出来的容器系统时区又是东八区的，一些对时间不敏感的服务部署后可以正常运行，卡在了notify服务没法正常运行。后续服务也会依赖他，没法进行后续安装。请问你们helm安装的这些各类基础镜像时区问题是否有统一的基础镜像，配置相同的时区。或者有其他处理办法吗？正常情况下时区不一致也不会影响正常运行，你的服务器时间是否一致，能给下notify的错误日志吗pod启动了但是不是就绪的状态，说明内部服务异常，直接http请求也是500错误码。
另外mysql日志中 报错为Got an error reading communication packets能贴一下这个服务的日志吗能否贴一下 asgard日志，执行下这个命令kubectl logs -f asgard-service-fd48bfd76-296mr -n choerodon-devops-prod这里的日志没有什么异常， 后边没有其他的日志吗这个pod不断的在重启，就这么多日志
目前mysql的时区问题也解决了，还是这个notify应用的问题，启动服务就是异常的asgard-service 起来了吗后续服务启动都有问题，后续服务应该依赖前面服务的吧文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/疑问:微服务开发框架的服务有没有功能介绍文档？目前对于各个服务在github上有一个简单的介绍



GitHub



choerodon/choerodon-framework
Choerodon Microservices Framework. Contribute to choerodon/choerodon-framework development by creating an account on GitHub.





Choerodon Microservices Framework. Contribute to choerodon/choerodon-framework development by creating an account on GitHub.我们会逐步在官网上添加对应服务的介绍和使用说明
http://choerodon.io/zh/docs/development-guide/backend/framework/嗯嗯， 谢谢。
其实我主要想了解 asgard， notify， file这三个的功能， 好像github上面也没有或者不清楚我们的asgard和notify功能正在开发，后续会将文档更新到官网上。
asgard：主要包含两部分， 一部分是基于saga的数据一致性支持；另一部分是基于quartz的定时任务支持，包含简单定时任务和cron定时任务，将于0.10.0版本发布。
notify：通知服务。目前包含邮件通知和站内信通知，邮件通知基于javamail，其他服务可以自定义邮件模版，feign调用notify服务接口发送邮件；站内信通知基于websocket长连接和redis的pub/sub，通过feign调用notify服务接口发送站内信通知，用户登录后可以通过websocket实时收到消息通知。
file: 基于mino的文件服务，其他服务可以调用file-service接口将文件上传到mino，并生成下载链接嗯嗯， 感谢您的回复Choerodon平台版本: 0.6.0遇到问题的执行步骤:
第四步：一键部署Choerodon文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:choerodon-mysql not ready,sleep 5s,check it.    这句会一直出来，/u01目录里面并没有安装Mysql，/u01是NFS目录[root@node1 ~]# cd /u01
[root@node1 u01]# ll
总用量 60
drwxr-xr-x. 2 root root     6 9月  14 17:50 aaa
-rw-r–r--. 1 root root 60856 9月  14 17:52 choerodon-install.sh
[root@node1 u01]#
[root@node1 u01]#
[root@node1 u01]#
[root@node1 u01]#
[root@node1 u01]#
[root@node1 u01]#你在nfs中创建对应的目录了吗需要创建什么目录？  我可以参考教程的哪部分？values 中定义的目录你需要在nfs中创建对应的目录把目录全部建立了也不行，目录都是在NFS客户端建立的[root@node1 io-choerodon]# ll
总用量 0
drwxr-xr-x. 2 root root  6 9月  17 17:02 chartmuseum
drwxr-xr-x. 2 root root  6 9月  17 17:02 gitlab
drwxr-xr-x. 5 root root 48 9月  17 17:02 harbor
drwxr-xr-x. 3 root root 15 9月  17 17:01 kafka
drwxr-xr-x. 2 root root  6 9月  17 17:02 minio
drwxr-xr-x. 2 root root  6 9月  17 17:01 mysql
drwxr-xr-x. 3 root root 15 9月  17 17:01 zookeeper
[root@node1 io-choerodon]# pwd
/u01/io-choerodon
[root@node1 io-choerodon]#执行下面命令 看下[root@node1 io-choerodon]# kubectl get po -n choerodon-devops-prod
NAME                              READY     STATUS              RESTARTS   AGE
choerodon-mysql-dcb9b88cd-wlgqv   0/1       ContainerCreating   0          1h
dnsmasq-5f5777c999-lznnl          1/1       Running             0          18d顺便说一下，自建的mysql目录是空的看一下pod的状态你的nfs服务器域名是 nfs.example.choerodon.io 吗？nfs.example.choerodon.ioapi.example.choerodon.io
nfs.example.choerodon.io
等等，这些都没有配置，本机部署了example.choerodon.io这个通过其他主机能够访问 nfs.example.choerodon.io 吗，是否配置了域名解析？我把NFS这个域名换成IP了，也不行
PS，我安装的是单机版k8s，NFS在另外一台服务器上根据刚才的步骤 再贴一次 kubectl describe po xx 的 结果改了IP之后安装成功了，刚刚安装等待比较久。
现在走到这一步了这个问题是算完成了？1.执行 kubectl describe get job -n choerodon-devops-prod 你应该能看到一个 create-database-xx 的job
2.执行 kubectl get job "create-database-xx" -n choerodon-devops-prod -o json | jq 'del(.spec.selector)' | jq 'del(.spec.template.metadata.labels)' | kubectl replace --force -f -  如果提示jq不存在先安装jq
3.kubectl describe get pod -n choerodon-devops-prod -w 查看是否有新启动的pod
4.如果有追踪下它的日志[root@node1 ~]# kubectl describe get job -n choerodon-devops-prod
the server doesn’t have a resource type “get”sorry, 没有 describe,执行这句请问一下，我用自动部署，走到了第14步，如果失败了，是不是之前的全部步骤都需要回滚？  譬如mysql，需要自己手动删除，还是怎样处理？Choerodon平台版本：0.9.9运行环境：自主搭建问题描述：本地运行从服务器上获取的后端微服务模板程序的时候报错执行的操作：
按照http://choerodon.io/zh/docs/development-guide/backend/develop-env/install_windows/ 搭建了开发环境
本地运行从服务器上获取的后端微服务模板程序的时候报错报错信息(请尽量使用代码块的形式展现)：
2018-09-27 20:20:30.887  WARN [dd-microservice-test,] 4745 — [ad | producer-1] org.apache.kafka.clients.NetworkClient   : Error while fetching metadata with correlation id 0 : {springCloudBus=LEADER_NOT_AVAILABLE}请问是使用这个模板吗？



GitHub



choerodon/choerodon-microservice-template
This is a choerodon microservice template. Contribute to choerodon/choerodon-microservice-template development by creating an account on GitHub.





This is a choerodon microservice template. Contribute to choerodon/choerodon-microservice-template development by creating an account on GitHub.本地开发需要配置可以在resources 文件夹下创建application-default.yml 文件，然后添加这些配置是的Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
猪齿鱼微服务架构支持单点登录吗？怎么配置单点登录？你好，可以配置choerodon.oauth.enabled-single-login: true 使单点登录生效好的，oauth服务有实现是吗？嗯嗯，对的凡哥，能具体说说实现吗？或者有文档例子之类的吗？choerodon 支持的是自身平台的单点登录，通过http session ，设置登录choerodon 的用户只允许存在一个session，保证单点登录。嗯，猪齿鱼单点登录的服务器可以，猪齿鱼做单点登录的客户端可以吗？暂时不支持第二种方式你好，第二种还打算做吗？我们现在自己做SSO，遇到些问题？你好，能否具体描述下你们做SSO遇到的问题？我们可以一起分析下哈1.sso登录与正常登录怎么切换，目前用@ConditionalOnProperty，但是在依赖注入时报空指针
2.sso登录怎么与springsecurity、oauth2集成，目前重定向界面后，还会跳到登录界面；
3.sso怎么做登出。1、 如果两种登录共存，需要有单独的登录入口，以便跳转到cas对应的登录界面
2、需要添加额外的filter 对cas的域名进行验证，执行重定向的逻辑
3、登出时主动触发cas/logout，清空sessionChoerodon平台版本：0.9.0 公司平台运行环境：k8s 1.8.5问题描述：k8s未知原因down之后重启执行
systemctl restart kubelet
systemctl restart docker
之后部分pod没有起来如：管理页面怎么进入，进行系统配置执行的操作：systemctl restart kubelet
systemctl restart docker报错信息：kube-flannel event信息报错找不到挂载卷，如下图
查看configMap配置如下图
kube-flannel-cfg yaml文件如下图
疑问：@vinkdong先把kube-system 里面的pod都删除试试删掉之后， 好像会重建zookeeper和kafka也异常， 要怎么处理？
删除之后会自动新建新的， 你是否对磁盘进行过清理操作？ 看下kafak的日志kafka好像是起来了， 但是这个svc是没有clusterIP的么？
是的Choerodon平台版本: 0.9.0疑问:
这个是用来给agent管理实例的一个自定义k8s对象。现在gitops库中存实例对应成一个c7nhelmrelease文件当模板吗？那这个自定义的镜像有啥用呢？这个文件创实例平台会给自动创建生成到gitops库，自定义镜像可以写在values里边，部署的时候可以替换进去。这个文件其实就是代表了一个helm chart应用和values。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
manager-service安装的helm脚本中的参数
preJob.preInitDB.mysql.host这个参数是部署的mysql的svc的名字吗？文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/是的执行安装报错Error: Job failed: BackoffLimitExceeded
是什么原因？再执行一次命令，然后看下pod状态kubectl get po -n uat在执行 helm install 之后 另起一个窗口 执行稍微等待一会你应该能够看到一个名字像xxx-init-xx的pod好像又成功了这个一般是拉取镜像超时导致的现在部署iam又出现这个问， 我看数据库里面初始化了一部分数据
应该是同样的问题，一般情况下您重试一下应该就好了我查看数据库初始化pod的日志有报锁的错误
查看下databasechangeloglock 这张表怎么看？清一下然后重新部署就好了嗯嗯， 测试有效果， 谢谢一般可能是因为上一次job没有结束就被杀掉了，或者同时部署了两个，清一下就好了。数据库初始化正常结束了， 但是helm还是报错了
Error: Job failed: DeadlineExceeded
应该是服务器性能差job执行超时了自动配置完成了
python3 ../../choerodon-front/node_modules/choerodon-front-boot/structure/configAuto.py iam无法执行下面这句，看起来是python需要有2和3 2个版本同时存在，系统是MACOSpython ../../choerodon-front/node_modules/choerodon-front-boot/structure/sql.py报错如下又通过安装sql.py里面所需的组件，最后显示安装完成，还是报出无法找到模块yaml请大大帮忙看看，感谢补充一下环境信息：你好，麻烦提供下如下信息：同时安装了python2 和python3？ 如果同时都安装的话，建议将python3卸载掉重新试下。菜单初始化的脚本不支持python3但是我看到autoConfig.py里面定义的是3的版本，所以就去装了个3的版本#!/usr/bin/python3格式排得不太好，请见谅一下哈。这个是我们代码的问题，我们修改下，感谢反馈太帅了，这么快的响应如果我使用刚才安装的python2 则是另一个错误出现了这里就只有一个小问题，我修复过程如下：临时修改数据库字段名 FD_LEVEL 为 LEVEL
执行脚本
再将字段名改回为 FD_LEVEL因为master 分支的代码是我们最新的代码，我们数据库有做过修改。建议使用tag 对应的代码OK 谢谢你为何要执行
npm install --save /Volumes/development/workspace/paas/framework/choerodon-framework/choerodon-front正常的启动总前端的步骤是这样的：注意：
如果子模块中比如iam中npm install过的话，要把node_moudles目录删除好的，是我执行的地方不正确，谢谢你哈Choerodon平台版本: 0.9.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:
之前有显示部署成功，但是域名访问sonar后，就提示如下错误：
java.lang.IllegalStateException: Fail to unzip plugin [csharp] /opt/sonarqube/extensions/plugins/sonar-csharp-plugin-7.0.1.4822.jar to /opt/sonarqube/temp/ce-exploded-plugins/csharp
2018/9/27 下午5:15:24 	at org.sonar.ce.container.CePluginJarExploder.explode(CePluginJarExploder.java:56)
2018/9/27 下午5:15:24 	at org.sonar.core.platform.PluginLoader.defineClassloaders(PluginLoader.java:92)
2018/9/27 下午5:15:24 	at org.sonar.core.platform.PluginLoader.load(PluginLoader.java:72)
2018/9/27 下午5:15:24 	at org.sonar.ce.container.CePluginRepository.start(CePluginRepository.java:71)
2018/9/27 下午5:15:24 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2018/9/27 下午5:15:24 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2018/9/27 下午5:15:24 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2018/9/27 下午5:15:24 	at java.lang.reflect.Method.invoke(Method.java:498)
2018/9/27 下午5:15:24 	at org.picocontainer.lifecycle.ReflectionLifecycleStrategy.invokeMethod(ReflectionLifecycleStrategy.java:110)
2018/9/27 下午5:15:24 	at org.picocontainer.lifecycle.ReflectionLifecycleStrategy.start(ReflectionLifecycleStrategy.java:89)
2018/9/27 下午5:15:24 	at org.picocontainer.injectors.AbstractInjectionFactory$LifecycleAdapter.start(AbstractInjectionFactory.java:84)原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问这个问题我们没法重新， 检查下您的磁盘是否充足？充足。镜像卷是10G，nfs的200G，node机器还有20多G。查了一下容器空间等，都比较充足。
跟postgres的版本有没有关系。
或者说，是不是可以重新下一个sonar的镜像版本，会有问题吗。这个镜像我们使用是没问题的，你可以试下在主机上docker run 看正不正常Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：
我们使用了一段时间的猪齿鱼平台，开发了一些项目。随着项目数量的增加，自动化测试的必要性就体现出来了。因此我们想咨询一下猪齿鱼平台是否有计划加入自动化测试功能，如果会加入的话，目前有大概的方案会使用哪些自动化测试工具？您好，自动化测试的功能已经在设计并纳入迭代计划了。第一版准备支持的框架是我们自己已经在使用的 mocha+chai 的api测试框架。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
部署gitlab，使用外部mysql，事先创建好数据库和用户名密码文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问外部数据库中的表全部删除，gitlab挂载出来的数据全部删除，再重启gitlab即可。删掉表，删除gitlab/data下数据，但是k8s上报CreateContainerConfigError: failed to prepare subPath for volumeMount “gitlab-data” of container “gitlab”。
这种是不是要重新执行helm install重新部署了一次，现在报这个错误
ActiveRecord::StatementInvalid: Mysql2::Error: Specified key was too long; max key length is 767 bytes: CREATE UNIQUE INDEX index_lfs_file_locks_on_project_id_and_path USING btree ON lfs_file_locks (project_id, path)Specified key was too long;您的数据库是5.6以下的吗是5.6.27版本5.6及以前版本的mysql可能存在问题哦，你需要去配置一些东西


stackoverflow.com






#1071 - Specified key was too long; max key length is 767 bytes


mysql, byte, varchar, mysql-error-1071


  asked by
  
  
    Steven
  
  on 03:18AM - 29 Nov 09






Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：虽然说不安全也有可能。但不觉得是一种常规操作。如果我不想修改master的22端口，能不能提供其他的解决方案？
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/由于使用ssh方式克隆代码需要使用22端口，但是sshd服务占用了22端口，所以需要将原sshd修改到其他端口，你可以选择任意节点操作，并将gitlab的域名映射到这个节点。目前暂不支持使用其他端口。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
在组织管理页面创建模板时，不成功。
页面有创建记录：
saga不存在，表示asgard-service没有扫到该saga, 尝试解决方式是:2.重启后创建模板如果还是找不到saga,则为搭建的kafka的问题,此时检查kafka是否搭建成功，
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/kafka/kafka是搭建成功的。

重启devops－service与asgard-service，还是找不到saga
可以用管理员账号登录平台，然后点击管理，查看平台中已有的saga然后查看创建模板的saga是否存在只能查到iam的事务
重启下其他服务比如devops，看下asgard-service日志，有没有收到devops的启动消息。如果没有，看下go-register注册中心日志，是否正常发送服务启动的消息到kafka中注册中心的日志，devops－service注册上了，以下是信息。帮忙查看下有没有异常
asgard类？asgard没有收到服务启动的日志吗？这样的，Choerodon平台版本：0.9.0 （SaaS）运行环境：自主搭建问题描述：
我想自己的组织的应用市场发布了一个应用，然后尝试在组织下的项目中部署它，结果失败了：
我根据它的提示，尝试去 K8S master节点中执行:之后重新执行应用部署仍然失败。应用发布的范围是本组织：
您好，请问在自己组织下,应用所在的项目下可以部署成功吗？之前是可以的，今天下午我尝试重新再项目下部署，也是失败的，错误一样。您是自己搭建的平台吗?公司的 SaaS， K8S 是我自己搭建的我是根据 http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/helm/ 进行 Helm/Tiller 的安装部署的。
目前本地有这几个Chart Repo：
oracle 的那个是我发现部署错误之后，尝试添加的，不过也没什么效果。http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/chartmuseum/
分步搭建的时候 chartmuseum的ingress地址是什么我并未执行这个步骤。SaaS 我指的是 https://choerodon.com.cn/上面没看到，不好意思哦，刚刚查看了charts仓库，不好意思，由于公司saas chartmuseum要定期备份,所以公司saas 的chartmuseum会定期清除测试的chart包，所以之前可以部署，现在部署不了是因为chart包被清理掉了。如果想重新部署可以重启跑ci生成版本重新跑了一下 Pipeline 已经正常了。多谢。Choerodon平台版本：0.9.9运行环境：自主搭建问题描述：应用部署的时候报错
Successfully assigned dd-microservice-test-c865d-init-db-r7m7h to node1
MountVolume.SetUp succeeded for volume “tools-jar”
MountVolume.SetUp succeeded for volume “default-token-zdl9l”
pulling image “registry.cn-hangzhou.aliyuncs.com/choerodon-tools/dbtool:0.5.2”
Successfully pulled image “registry.cn-hangzhou.aliyuncs.com/choerodon-tools/dbtool:0.5.2”
Created container
Started container
pulling image “registry.choerodon.com.cn/ora-cddc/dd-microservice-test:2018.9.26-143433-master”failed to get container status {"" “”}: rpc error: code = OutOfRange desc = EOF最有一句也在日志上的吗，然后麻烦实例界面有红色感叹号内容贴一下出来吧。首先这个就是受限于集群资源或其他原因Job执行超时了,可以在环境连接正常状态，点击修改配置，重新部署，也可以通过提高job执行超时时间，重新生成版本，降低超时的可能性。好的，谢谢一键部署猪齿鱼很难一次成功，基本上都要来回部署好几次才能安装成功。如果部署到后面第20几步了，但是有个service没有部署成功，此时只能删除所有service
重新再部署一次。这样在删除和重新部署的过程中非常消耗时间，请问有什么更好的解决办法吗？感谢反馈 我们正在加紧优化一键部署，以避免重复安装和清除数据好的谢谢，目前的解决办法还是只能删除所有的service 和数据重新部署是吗？您部署到哪一步失败的manager-service 失败过，iam-service 也失败过最近这一次 是iam-service
在step 21的时候显示Job failed: DeadlineExceeded 但是却也输出Success of install Choerodon iam service，
然后到step 24就会报错下面错误
这种一般是拉镜像时间过长导致的，目前你只能先删除数据了。我们会尽快更新一键安装你好，就在刚才我第二次安装还是跟上面一样的这个问题。
在执行过程中，查看choerodon-iam-service-init-db-tgnj7这个pod的日志，
另外还有一个问题，那个job的执行时间限制必须为120秒吗，不能再长一点吗？目前无法直接修改这个值，我们会在下一个版本提高默认值，安装微服务一般包括 init-db和init-config 你看一下init-config是否正常跑完init-config 没怎么注意过，不过一般看到的都是init-db pod出问题。后面再装一次这个iam-service又好了。
我有一个猜想，就是每次pod被分配到不同的node上安装，
如果这个node之前pull过这个pod安装需要的镜像那么可能安装速度快一些，service安装就不会失败；
如果这个node没有pull过该镜像，那么加上拉取镜像耗费的时间，就很有可能service安装失败。我这种猜想存在吗？如果存在的话，有没有可能在安装猪齿鱼之前先把需要的镜像全部拉取下来？这样避免在安装的时候老是出现安装失败！是这样的。在所有主机拉取所有镜像有一点麻烦，我们在考虑如何规避这个问题不好意思，又遇到一个问题，网上找不到解决办法，请教一下。choerodon-gitlab not ready
pod日志：gitlab 还在启动过程中， 你可以看下gitlab的日志，这个日志是正常。你好，我这边一直显示 choerodon-gitlab not ready,sleep 5s,check it
下面是查看pod日志：
这算是正常，需要继续等待吗gitlab的日志只到这里吗对，这是刚才我执行kubectl logs choerodon-gitlab-64c4889cf8-rmgbj -n choerodon-devops-uat
输出的日志，最下面的部分你可以先把这个pod删掉 ，再追踪下新启动的pod的日志好 ，不过我刚才停止安装，又重新来一遍了，可能需要等一点时间才到这里.你好，我这边跟踪了一下，删除gitlab 的pod之后，新建的POD 日志输出还是卡在这里
也许是数据库访问不了， 你看下mysql的容器是否正常你好，我执行命令  kubectl logs  choerodon-mysql-dcb9b88cd-rlrrd -n choerodon-devops-uat -f  查看mysql的日志，输出下面的内容，这有什么问题吗？我这边，找到了，多语言的几个内置类。但是不确定前后台，交互数据怎么写。
求大佬，给说下。谢谢谢。role表多语言字段是name，建立对应多语言表，如下
excel初始化数据：
使用的是spring message的多语言，建好messages文件夹然后把code对应语言就行了
目前后端多语言不是很完善，我们还在持续优化中前端多语言前端多语言分为界面多语言和数据多语言，数据多语言主要是后端api提供的，这里主要介绍界面多语言。上图是iam模块多语言文件所在的目录：/iam/src/app/iam/locale其中dashboard是工程中所要开发的仪表盘卡片所对应的多语言目录，该目录需要在config.js中进行配置，如下所示：以上代码出自 /iam/src/app/iam/containers/IAMIndex.js通过 AppState.currentLanguage 获取当前用户设置的语言代码，使用 asyncLocaleProvider 创建多语言供应组件。那如果要在界面上，进行编辑多语言的话。前后台数据交互怎么操作呢？
这里给的例子，数据来源是初始化的。那么如果界面上要传送多语言的话，DTO要加入附加字段来存储多语言的数据吗？初始化的时候是怎么让数据，可以直接保存到多语言表的呢？求解答。
谢谢谢谢。可以加你，微信或QQ。详细说下吗？不需要多余字段啊，就name一个字段就行了，我这个登陆用户是中文用户，为什么要返回一个英文的name和中文name呢，肯定是后端sql查询的时候就筛选过了，根据登陆用户的语言关联查询，然后放到name字段里啊我指的是，当数据在前端界面进行编辑的时候。比如，我有一个多语言的编辑页面，来配置，多语言的内容。那么我的接口上，怎么来传输多语言的数据呢？比如name字段，前台写了，“张三”和“zhangsan”，一个是中文，一个是英文。那么接口怎么来拿到这个数据呢？你这里给的例子主要是查询。查询的话，可以通过lang = “zh_CN”来处理，但是，插入的时候呢？插入的DTO怎么才能一次性，接受到 “张三”，“zhangsan”。就好比，初始化的时候，其实，是中文和英文。你好，猪齿鱼目前只是在数据库查询和页面分别做了多语言适配，对于接口数据的多语言处理还有所欠缺。目前可以通过手动的set将数据更新至多语言表。我们会完善开发手册中对于多语言的说明，后续我们会对该功能进行迭代。Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：新pull下来的项目，使用docker初始化数据库报错。
错误信息为：Error response from daemon: No such container: mydb
Error: failed to start containers: mydb请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：在Choerodon组织的租户设置中的客户端修改会报错，导致修改不成功。修改的数据：报错信息(请尽量使用代码块的形式展现)：
Network:原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问你好，docker ps 查看一下正在运行的容器Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
猪齿鱼有类似HAP接口管理功能吗？支持soap接口调用。你好，猪齿鱼有提供接口管理功能，目前仅限于查询和页面测试。暂时没有提供页面支持soap接口配置。报错信息如下，求解决方法看下你的入口函数是不是没加@EnableChoerodonResourceServer注解
刚才去查看了，入口函数，是含有你说的这个@ EnableChoerodonResourceServer这个注解的。
附图为，fegin的代码。看报错，是没有token导致的。这里，想要知道，token从哪里可以单独的获取到。你这个调用是怎么调用的，postman还是swagger-ui，有没有登陆啊使用的是swagger。登陆过的，在swagger，调用非fegin的url是没有问题的。使用的是swagger。登陆过的，在swagger，调用非fegin的url是没有问题的。看看有没有加这个依赖K8S 版本: v1.8.5kubeadm-ansible 版本： master分支最新脚本遇到问题的执行步骤:
使用 kubeadm-ansible 部署一个 3节点的 K8S 集群，所有的工作节点的状态全部为 NotReady，节点的信息及日志请参见 Github 上的对应的 Issue。Github Issue 引用: https://github.com/choerodon/kubeadm-ansible/issues/14@vinkdong 我们也可以在这里讨论，如果方便的话。@eliu 你用的是自己的虚拟机吗算是吧，是广州开发中心的三台服务器，彼此可以互相访问。在master节点看下有返回结果吗我发现 /opt/cni/bin/portmap 仅在主节点有，其他工作节点还是没有这个文件curl https://k8s-node01:6443 -k[root@k8s-master bin] ○ curl https://k8s-node01:6443 -k
curl: (7) Failed connect to k8s-node01:6443; 拒绝连接
[root@k8s-master bin] ○我们再检查下 这个应该每个节点都有的您的master节点同时也是node吗是的。好的 我们先测试一下请在每个节点执行下面命令  看看是否正常执行过了，还是不正常的。@vinkdong 我发现 cluster.yml 里面仅在 kube-master 下才会执行 addons/flannel而 portmap 的安装是在 addons/flannel/tasks/main.yml 下的。。这个问题已修正。请在所有节点都执行一下netstat -luntp，查看一下端口号开启情况netstat -luntp我重新又部署了一次，我在ansible的执行日志中始终只能看见 portmap 在 kube-master 这个hosts 上执行过。 但我不确定会不会和我的 NotReady 有关。另外，请问您说的 问题已修复指的是哪个问题呢？ @TimeBye修正的代码刚刚才合并进去哈，建议你再reset一下集群，然后拉取一下master分支最新代码进行部署试一试。多谢，刚刚重新部署，portmap 在所有节点都已经有了。不过问题仍然未解决~Choerodon平台版本：0.9.0运行环境：pass问题描述：
使用知识管理模块编辑文章时，点击保存后之前编辑好的部分格式出现错乱请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。
执行的操作：报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作不好意思，这个出现的问题应该是因为页面使用的是markdown编辑语法。
目前markdown的语法编辑模式会出现列表解析错乱的问题，我们会尽快修复它。 
您现在可以先切换到 XWiki 2.1 的语法，使用富文本编辑器工具栏的列表工具。Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：我用猪齿鱼去部署agile-service的时候，在 2选择环境及修改配置信息这一步修改了配置
把redis这一块注释掉了。
这个是为什么，改怎么解决？除了这个例子，之前也遇到过类似这种情况你好，平台现在部署时Values替换逻辑时这样的，以你应用版本中values文件中的为基础，用修改的部分去替换掉版本中的部分，所以你注释掉又会回来。该情况我们已经记录，会在后续版本进行修复优化，谢谢反馈谢谢请教下保存在gitlab那个gitops库里的环境变量是用在哪里的？那些环境变量是实际部署应用版本chart时，用来 override chart默认values文件的，是用户修针对版本中默认的values增加或者修改的值。Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：自主搭建问题描述：
部署完应用后发现swagger中没有出现该应用，之后使用swagger中manager-service的手动刷新权限接口进行手动刷新，返回200，但查看swagger中还是没有该应用的信息。查看iam-service中有大量日志出现。这个情况经常会出现，有事重新部署几次就好了。但没有找到具体原因，是代码有问题？部署完应用后发现swagger中没有出现该应用，之后使用swagger中manager-service的手动刷新权限接口进行手动刷新，返回200swagger里面没有对应应用看看route表里面有没有对应的路由，iam-service的日志是因为order-service上@Permission注解的roles你写了admin和managerAdmin但是role表中却没有对应的角色，只是Log出来了，不影响其他的插入查看了mgmt_route表，的确是没有order-service对应的路由。重新部署了，发现还是没有写入。查看对应的init-conf的pod的日志，都正常。
的确是没有order-service对应的路这个自动添加路由可能有点问题，你先看下你的服务里面有没有下面的代码，插入自定义路由的：这个代码是有的，那我手动添加试试随着企业业务创新和应用复杂度的升高，传统的“瀑布式开发模型”面临着需求变更、过度开发、适应性不强等诸多问题，亟待改善。不仅如此，企业内部程序复杂，业务发展快，开发效率也逐渐变得愈发重要。本次直播将介绍Choerodon猪齿鱼如何助力华润置地实现中台化转型，基于真实案例和实践经验，讲解Choerodon猪齿鱼如何帮助企业利用微服务和容器技术构建中台架构体系，打造以Choerodon猪齿鱼为核心的敏捷研发体系，聚焦业务，快速迭代，持续交付。华润置地有限公司是财富500强企业华润集团旗下的地产业务旗舰，是中国内地最具实力的综合型地产发展商之一，主营业务包括房地产开发、商业地产开发及运营、物业服务等。华润置地一直重视企业的信息化建设，从最早期的采用ERP套件，到后面自主研发的一系列“烟囱式”应用，应用之间相互独立，系统功能重合，架构各异，伸缩扩展能力有限，服务器及人力资源浪费严重，产品交付周期长，运维工作繁重。引入Choerodon猪齿鱼后，统一开发框架和平台，新的系统尽量采用微服务方式开发，基于敏捷迭代研发的思想，一般几周便可快速上线系统，部署周期从数周减少到几分钟，应用交付的效率提高了数十倍，容器平台由专门团队运维，项目组只需要专注于业务需求和交付，极大的降低了日常的运维成本，产品在设计、开发、运维等各个阶段均有改善。Choerodon猪齿鱼 是一个开源企业级服务平台，基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理，并提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，来帮助企业聚焦于业务，加速数字化转型。本次直播主要介绍华润置地中台转型背景及落地过程。主要内容包括：华润置地中台转型背景和架构体系介绍Choerodon在华润置地的部署架构及业务架构转型过程Choerodon猪齿鱼架构师  叶德华10 月 15 日（周一）下午  14:00IT大咖说，直播地址： http://www.itdks.com/liveevent/detail/16310大家也可以通过以下社区途径了解猪齿鱼的最新动态、产品特性，以及参与社区贡献：欢迎加入Choerodon猪齿鱼社区，共同为企业数字化服务打造一个开放的生态平台。Choerodon平台版本: 0.6.0遇到问题的执行步骤:根据文档-开发手册-前端开发手册-代码运行进行操作文档地址:http://choerodon.io/zh/docs/development-guide/front/basic-env/run/环境信息(如:节点信息):报错日志:疑问:
安装的是最新的Nodejs,但是在软件准备-安装Gulp时也出现了问题，localhost:9090打不开是打开网页出现500的错吗？
尝试在npm start之前在项目根目录执行
chmod -R 777 .
ci文件里是有这一步的，不执行可能会因为权限问题而error，但文档里没有提醒。PS：全局安装gulp命令前需要加sudu，npm start执行时用的是node_modules里的gulp，全局不装应该也没关系，可以照着文档往下做。多谢回复，抱歉确认晚了，加上了chmod语句之后再install就已经启起来了您好，0.9.0版本有部分缺陷，建议您部署一下最新的敏捷管理版本：0.9.5。运行日志中说明项目创建的时候没有成功消费消息导致敏捷管理中初始化项目信息失败，可能原因：asgard-service服务没有启动成功，可以看一下asgard-service的日志。
同时你可以查看一下敏捷服务的agile_project_info表是否有对应项目的数据，没有说明消息消费失败。看截图信息，可能是由于项目创建时初始化失败，导致没有相关的项目信息
由于系统为微服务多组件架构，项目创建后会产生相关的事件消费，会在敏捷模块中初始化相关的信息
截图中我们猜测是由于消息未消费或者消费失败，需要确认消息相关的服务是否正常（asgard-service服务），如果可以的话查看数据库agile-service数据库中的agile_project_info表中是否生成了对应项目的信息，若为空，则大概率是消息相关服务或者中间件配置错误导致的agile_project_info是空的。
你好，可以使用admin登录平台，通过菜单管理 -> 全局事务 -> 事务实例 查看系统中的事务。从下往上，找到失败的那一条点击详情，然后选择重试。在事务实例中没有看到错误。
event-store-service是之前版本的消息一致性服务，0.9.0我们更新为基于saga的asgard-service来保证事务一致性这个是一个完整的创建项目的事务，红色框出的部分为敏捷模块初始化的逻辑，麻烦检查一下另外敏捷模块0.9.5是目前最新的版本，最好使用我们已发布的最新版本，由于产品在不停的优化，当前还未发布一个非常稳定可靠的版本，所以麻烦尽量使用最新的版本，会减少很多问题的出现查看了刚才的create事务，没有任何的输出。
事务实例有没有输出没有啥意义，事务实例的输出就是最后的task实例输出。要看里面具体task的输出和执行情况你们这是saga都没有扫描进去，可以重启下敏捷这些服务，看下asgard有没有异常日志，asgard能不能正常接收kafka消息saga是什么，如何扫描。 asgard－service没有异常。
重启异常的服务，然后看asgard－service有没有异常，如果正常的话在saga的事务管理页面可以看到完整的调用
初始化的job显示是成功的。
现在的问题应该是你在启动其他服务时asgard－service没启动或者不正常所以导致saga事务一致性没有注册成功，把devops、agile、test、wiki这些业务模块重启一下把事务重新注册一下就好，正常的时候可以看到我之前截图的事务逻辑图，那样才是对的devops、agile、test、wiki这几个业务逻辑模块重启有顺序吗没有的Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：k8s
腾讯云遇到问题时的前置条件：
devops-service 中报错
7:50:20.292 [pool-3-thread-1] INFO  i.c.d.a.e.DevopsEventHandler - data: io.choerodon.devops.api.dto.GitlabProjectEventDTO@6a87c073
17:50:20.643 [pool-3-thread-1] WARN  i.c.e.consumer.handler.MsgHandler - message consume exception, msg : EventPayload{uuid=‘4a8882c82a7e48d9b14f27dda7357980’, businessType=‘OperationGitlabProject’, data=io.choerodon.devops.api.dto.GitlabProjectEventDTO@6a87c073}, cause java.lang.reflect.InvocationTargetException
java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at io.choerodon.event.consumer.handler.DefaultMsgHandlerImpl.execute(DefaultMsgHandlerImpl.java:81)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333)
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
at com.sun.proxy.$Proxy237.execute(Unknown Source)
at io.choerodon.event.consumer.factory.KafkaMessageConsumerFactory.lambda$receiveMsg$1(KafkaMessageConsumerFactory.java:110)
at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)
at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374)
at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
at java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:291)
at java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:731)
at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
at java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:401)
at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:734)
at java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:160)
at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:174)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)
at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:583)
at io.choerodon.event.consumer.factory.KafkaMessageConsumerFactory.receiveMsg(KafkaMessageConsumerFactory.java:94)
at io.choerodon.event.consumer.factory.KafkaMessageConsumerFactory.lambda$createConsumers$0(KafkaMessageConsumerFactory.java:82)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: io.choerodon.core.exception.CommonException: error.projecthook.create
at io.choerodon.devops.app.service.impl.ApplicationServiceImpl.operationApplication(ApplicationServiceImpl.java:327)
at io.choerodon.devops.api.eventhandler.DevopsEventHandler.handleGitlabProjectEvent(DevopsEventHandler.java:86)
… 31 more
17:50:32.039 [XNIO-2 task-28] INFO  o.s.c.c.c.ConfigServicePropertySourceLocator - Fetching config from server at: http://config-server.choerodon-devops-prod:8010/问题描述：
选择项目后点击创建应用，应用一直在创建中：
请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在
第一步：
我先看了我创建的用户是否在gitlab上面：
io.choerodon.core.exception.CommonException: Unprocessable Entity
at io.choerodon.gitlab.app.service.impl.HookServiceImpl.createProjectHook(HookServiceImpl.java:25) ~[classes!/:0.8.0]
at io.choerodon.gitlab.api.controller.v1.HookController.create(HookController.java:43) ~[classes!/:0.8.0]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_121]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_121]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_121]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_121]
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) ~[spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) ~[spring-webmvc-4.3提出您对于遇到和解决该问题时的疑问您好，请看下这个topic“创建应用报错问题”请问您说的是哪个topic ，kafka的吗。但是kafka没有对外提供，请问要怎么看这个topic是让你看这个帖子哦我的是0.8版本，我是分步安装的，
我的SERVICES_GATEWAY_URL这个值本来就加了http前缀
我是0.8版本，分步安装，所有我本来就有http前缀
SERVICES_GATEWAY_URL我是0.8版本，分步安装，所有我本来就有http前缀
SERVICES_GATEWAY_URL
Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：我根据文档在猪齿鱼上部署一个后端应用，但是在部署的过程中报下面的错误问题描述：下面是我的应用配置文件：replicaCount: 1image:
repository: registry.example.choerodon.io/operation-choerodon-dev/choerodon-todo-servie
pullPolicy: AlwayspreJob:
preConfig:
configFile: application.yml
mysql:
host: 192.168.12.175
port: 3306
database: manager_service
username: root
password: choerodon
preInitDB:
mysql:
host: 192.168.12.175
port: 3306
database: demo_service
username: root
password: choerodondeployment:
managementPort: 18081env:
open:
## register-server
EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://register-server.io-choerodon:8000/eureka/
## config-server
SPRING_CLOUD_CONFIG_ENABLED: true
SPRING_CLOUD_CONFIG_URI: http://config-server.framework:8010/
## mysql
SPRING_DATASOURCE_URL: jdbc:mysql://localhost/demo_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
SPRING_DATASOURCE_USERNAME: root
SPRING_DATASOURCE_PASSWORD: choerodon
## kafka
CHOERODON_EVENT_CONSUMER_KAFKA_BOOTSTRAP_SERVERS: kafka-0.kafka-headless.kafka.svc.cluster.local:9092,kafka-1.kafka-headless.kafka.svc.cluster.local:9092,kafka-2.kafka-headless.kafka.svc.cluster.local:9092
SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS: kafka-0.kafka-headless.kafka.svc.cluster.local:9092,kafka-1.kafka-headless.kafka.svc.cluster.local:9092,kafka-2.kafka-headless.kafka.svc.cluster.local:9092
SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES: zookeeper-0.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.zookeeper.svc.cluster.local:2181
SPRING_KAFKA_PRODUCER_VALUE_SERIALIZER: org.apache.kafka.common.serialization.ByteArraySerializermetrics:
path: /prometheus
group: spring-bootlogs:
parser: spring-bootpersistence:
enabled: false
## A manually managed Persistent Volume and Claim
## Requires persistence.enabled: true
## If defined, PVC must be created manually before volume will be bound
# existingClaim:
# subPath:service:
enabled: false
type: ClusterIP
port: 18080ingress:
enabled: falseresources:
# We usually recommend not to specify default resources and to leave this as a conscious
# choice for the user. This also increases chances charts run on environments with little
# resources,such as Minikube. If you do want to specify resources,uncomment the following
# lines,adjust them as necessary,and remove the curly braces after ‘resources:’.
limits:
# cpu: 100m
memory: 2Gi
requests:
# cpu: 100m
memory: 1.5Gi疑问：提出您对于遇到和解决该问题时的疑问这个应用，已经成功运行一个版本实例了，数据库已经初始化了。现在是我又使用这个应用发布一个新的实例，是不是因为数据库已经创建过了，现在再发布一个新的实例，数据库再次创建导致无法创建成功，所以导致实例发布失败？查看后台devops-service pod日志 发现报错请问有人可以帮忙看看吗？环境现在连接中吗，重新部署可以成功吗重新部署也不行，而且那些部署失败的应用也无法删除
这个是提示信息
抱歉，把这个帖子忘了，首先小红点的是上一次操作失败了，途中信息应该是环境断开连接了，环境连接成功的状态时，创建失败的对象是可以被删除的，处理中状态的对象在平台0.9版本之前如果消息丢失一直处于处理中是无法修复的。在0.9平台版本中可以通过GitOps方式修复。平台的版本是0.7吗？环境状态如果是连接中，但是部署的时候提示连接关闭应该是出异常了。对于平台老版本需要找到具体报错才好判断原因，可能需要重启服务。Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：使用 Swagger 调用 todo-service 的 GET api /v1/tasks/{id} 时返回 403 错误，调用前已经成功登陆。gateway-helper 服务中出现日志如下：MySQL 中的 mgmt_route 表信息：目前找不到是什么原因，请求框架组帮忙！看看iam_service.iam_permission表里面有没有数据，有的话重启下gatewayhelper,没有的话




choerodon 0.9.0布署完成后多处403 Choerodon Framework


    根据文档安装完成后，在整合界面上的组织管理里，应用模板403，创建项目后，项目下面相关的敏捷管理、流水线管理等都是403. 
 [%5BE)%40X%6072O%5B%25K%7B%5DW1JL0DPGK]
[1%7BCP_QGRWAX%5BDVSNJ%7DG4%24Z5] 
检查微服务都是全的。 
[TF%60GUU6%5DT%5B0Y%24_YSXJO)7BV] 
在项目管理－客户端里的服务…
  

看iam-service日志里面，
iam_role 少了 level字段iam_permission 少了level, resource两个字段啥意思，数据库少了这两个字段吗？你们用的什么版本？docker-compse里面的进行的版本和数据库初始化的脚本的版本不一致我的 iam_permission 里没有数据，但是执行了那个 API 之后仍然没有数据：
api-gateway 里面有以下日志：Request get empty jwt需要看manager-service和iam-service的日志了，调下手动刷的接口看看manager-service有没有log出如下信息：
parsePermission send message to kafka failed, RegisterInstancePayload{status=‘null’, appName=‘iam-service’, version=‘v1’, instanceAddress=‘null’, createTime=null} {}这是我compose里面kafka服务的定义：
其实我们是跟着文档内容走的。。。那里面都是0.6.0和0.7.0的。。。。manager-service你的manager-service有没有kafka连不上的异常日志？环境变量配置的kafka地址SPRING_KAFKA_BOOTSTRAP_SERVERS是否正确？这个异常信息就是发送消息到kafka失败了，调用了发送失败回调打印的日志manager-servicekafka 服务应可以通讯了，iam_permission 里面也有数据了，我把 hostname: 127.0.0.1 去掉了。但是调用 todo-service 的时候仍然是 403 错误：iam_permission 里面已经刷新了 todo 相关的权限了相关的权限了哦，那是kafka配错了。还403的话重启下gateway-helper重启了，可还是报：[  XNIO-3 task-1] i.c.g.h.p.RequestPermissionFilter        : error.permissionVerifier.permission, can’t find request service route, request uri /todo/v1/tasks/1, zuulRoutes {event=ZuulRoute{id=‘null’, path=’/event/’, serviceId=‘event-store-service’, url=‘null’, stripPrefix=true, retryable=null, helperService=‘gateway-helper’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}, devops=ZuulRoute{id=‘null’, path=’/devops/’, serviceId=‘devops-service’, url=‘null’, stripPrefix=true, retryable=null, helperService=‘null’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}, iam=ZuulRoute{id=‘null’, path=’/iam/’, serviceId=‘iam-service’, url=‘null’, stripPrefix=true, retryable=null, helperService=‘null’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}, oauth=ZuulRoute{id=‘null’, path=’/oauth/’, serviceId=‘oauth-server’, url=‘null’, stripPrefix=false, retryable=null, helperService=‘null’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}, notify=ZuulRoute{id=‘null’, path=’/notify/’, serviceId=‘notification-service’, url=‘null’, stripPrefix=true, retryable=null, helperService=‘null’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}, manager=ZuulRoute{id=‘null’, path=’/manager/’, serviceId=‘manager-service’, url=‘null’, stripPrefix=true, retryable=null, helperService=‘null’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}, file=ZuulRoute{id=‘null’, path=’/file/’, serviceId=‘file-service’, url=‘null’, stripPrefix=true, retryable=null, helperService=‘null’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}, org=ZuulRoute{id=‘null’, path=’/org/’, serviceId=‘organization-service’, url=‘null’, stripPrefix=true, retryable=null, helperService=‘null’, sensitiveHeaders=[], sensitiveHeadersJson=‘null’, customSensitiveHeaders=false}}manager_service里面route表是有 todo-service这个路由吗？有的话看下config-service日志，看gateway-helper拉的什么配置路由表 mgmt_route 是我手工插进去的路由信息。。目前是有的：
config-service 我这里是没有的。
没config-server拉不到配置啊，要部署配置中心的，你是本地部署吗，还是一键部署Choerodon平台版本: 0.9.0遇到问题的执行步骤:部署sonarqube文档地址:环境信息(如:节点信息):报错日志:
容器日志：原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你用了一个外部的postgreql吗不是，按教程装的。看下您的postgresql有没有什么异常kubectl get po -n choerodon-devops-prod | grep postgresql | awk ‘{print $1}’ | xargs -I – kubectl logs – -n choerodon-devops-prod报这个错误
2018-09-21 02:35:22.738 UTC [24276] FATAL:  no pg_hba.conf entry for host “10.42.1.33”, user “root”, database “root”, SSL off是不是sonar镜像中没有把对应的访问账号传给postgres。你改了SONARQUBE_JDBC_USERNAME 这个参数吗kubectl get po -n choerodon-devops-prod | grep postgresql | awk ‘{print $1}’ | xargs -I – kubectl logs – -n choerodon-devops-prod没改，我只是将helm的install中的镜像从外网拉取改成从内网拉取，其他参数没动能否粘贴一下你部署的命令helm install /data/workspace/helm/sonarqube 
–set persistence.enabled=true 
–set persistence.existingClaim=sonarqube-pvc 
–set env.open.SONARQUBE_JDBC_URL=“jdbc:postgresql://sonarqube-postgresql/sonar” 
–set env.open.SONARQUBE_JDBC_USERNAME=admin 
–set env.open.SONARQUBE_JDBC_PASSWORD=handhand 
–set ingress.enabled=true 
–set ingress.hosts={sonarqube.xpi.io} 
–name sonarqube --namespace=choerodon-devops-prod你有修改postgresql的镜像地址吗replicaCount: 1image:
repository: postgres
tag: 10.3-alpine
pullPolicy: IfNotPresent看下你的 pg_hba.conf  最后一行是否是  host all all all md5 ， 我尝试用官方镜像发现已经变成了 host all all all trust嗯，我把k8s重新建。然后把数据库建在独立的服务器上。看看。离0.9发布也要2个星期了吧。
1.0什么时候发布呢？
1.0的改动有什么？1，本周末会发布0.10，主要集中在局部功能的提升和缺陷的修复，具体可关注版本更新信息；
2，目前的计划我们会在2018年底发布正式版本1.0，系统的最大的变化是会基于Istio做微服务治理和服务化管理；同时基于这段时间多个项目的使用反馈增加系统的稳定性。谢谢请问猪齿鱼框架版本( Choerodon Framework ) 和起步依赖版本（Choerodon Starters）之间有一个版本关系对照表么？还是起步依赖版本始终使用最新版？Also file an issue on github:


github.com/choerodon/choerodon-starters





Issue: Is there relationship between choerodon framework version and choerodon starters version?


	opened by eliu
	on 2018-09-21




As title implied..







你好。对于框架自身而言：已经存在的tag，Choerodon Starter 的版本都存在于pom文件中，且不会被修改。主分支上Choerodon Starter 的版本会使用还未发布的最新版本。对于自己开发的服务而言，不强制Choerodon Starter 的版本，建议与框架对应版本一致。框架与依赖的版本如下：好的，多谢！Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：按照猪齿鱼后端开发手册尝试创建todo-service并发布至swagger问题描述：todo-service 正常运行，eureka 可以发现该服务，但swagger-ui中查不到任何与todo-service有关的api所有的相关的服务均在本地已启动：http://localhost:8000
swagger-ui:
todo-service 启动入口：CustomExtraDataManager另外，请问一下 CustomExtraDataManager 的具体用法和作用是什么？不是很懂，谢谢！CustomExtraDataManager是动态配置路由用的，ChoerodonRouteData里面的数据在部署该服务的时候会把数据插入到manager_service.mgmt_route表中。swagger ui下拉列表先查的route表，看下route表有没有todo-service这个路由，没有的话重启下todo-serivice试试，如果还没有的话，调用swagger界面 route-controller -> create方法新建一个路由:但是我的服务启动的时候貌似并没有执行这个CustomExtraDataManager，我想知道原因是什么。这个类所在的包路径有什么要求么？会不会没有被扫描到？扫描的是@ChoerodonExtraData注解，应该是都能扫到的嗯，我的关键代码已经贴在OP上，您看有什么问题么？实际上现在就是没有执行。请问这个问题有解么？看下这个项目有没有引choerodon-starter-swagger这个包，然后看下这个服务有没有注册到注册中心。项目中已经引入starter-swagger了：注册中心也有注册：你看看iam_permission表中有没有todo-service服务的permission，没有的话看下按我之前回的那个403帖子，调一下接口看看能不能刷进去权限，刷进去的话就是go-register -> manager-service这块的毛病，看下go-register有没有发消息到manager-service，看manager-service的日志，有没有接收到消息，然后发送消息到iam-service，刷不进去，看下iam-service日志的信息。这是manager-service接受和发送消息的代码
Choerodon平台版本: 0.9.0遇到问题的执行步骤:
使用一键自动安装成功之后，使用上其他都正常，只有wiki管理界面点击报错403文档地址:环境信息(如:节点信息):
4台服务器，centos 7.5 ，每台4核CPU 8G内存150G磁盘报错日志:
403，抱歉，您没有访问权限！原因分析:现在不知道是哪里授权出错了疑问:使用自动安装的，其他服务都正常！一键安装不包括 wiki哦， 因为wiki安装需要较长的时间，如果你要使用wiki需要执行手动安装wiki
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-wiki/你好：
哪其他的服务，比如下图中的持续交付、敏捷管理、测试管理等也是要单独部署的吗？除了知识管理 其他的都已经包含在一键安装里面了猪齿鱼安装文档里面，正式环境和测试环境安装方式不太一样，正式部署的文档是以阿里云
上部署为例子。
想请问一下，如果是在客户自己的虚拟机上部署猪齿鱼正式环境，而不是阿里云这种公有云，可以按照部署测试环境的文档来部署吗？如果这样可能会有什么问题吗？测试环境模式使用我们默认配置，默认配置在大多数情况下都是可以正常使用的，对于不同的云环境，云服务商可能会针对它的服务器提供更加适合它的配置或插件，使用优化过的插件可以获得更佳性能和稳定性，如果您的云提供商没有对网络或者其他组件进行过优化，你仍然可以使用测试环境方式部署正式环境。好的，谢谢！Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：本地后端服务搭建问题描述：请问后端开发文档中的 Jwt_Token 如何获取？ api-gateway 服务的运行日志中并无文档中所述，可以找到 jwt_token 的值。以下来自文档内容：3.  然后通过 api-gateway  的输出日志，获取登录用户的 jwt_token 。然后添加请求头。认证流程：
用户登录 - 跳转到oauth-server - 登录成功前端获取access_token - 前端发送请求到api-gateway - 网关将access_token转为JWT - zuul路由到真实服务。JWT其实是加密的CustomUserDetails。wiki上用admin登录失败，界面不显示
您好，请问你使用的wiki是哪个版本的呢？你这个wiki应该是没有部署成功的。使用的是升级到0.9.0以后的wiki版本，是10.4
我们0.9.0版本的wiki不是这个样子的呀。你们原来是使用的Choerodon猪齿鱼0.8.0版本的wiki吗？还是新部署的0.9.0版本的wiki呢？新布署的0.9.0的，原来的0.8.0的好像不是这样的。这是0.8.0升级到0.9.0版本wiki的步骤，http://choerodon.io/zh/docs/installation-configuration/update/0.8-to-0.9/
这是新部署0.9.0版本的wiki的步骤：
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-wiki/
是按新布的步骤做的。那你这个可能是因为初始化wiki的job没有成功，下个版本我们会优化这个部署流程。你先尝试在wiki中使用admin/choerodon登录，看是否可以。如果可以，接下来就手动修复下。wiki配置初始化的job是成功的，包括service的，也是。

使用域名访问wiki的时候，没有提示登录的页面，直接是正常按照我们的部署流程，job会自动初始化wiki，应该是没有问题的。你的wiki 的pod绑定的卷目录能看到吗？你看下里面有内容吗？
你可以点击这个 Log-in 按钮进行登录吗？
用admin是可以登录的。那你把地址栏的path改成这个，/bin/import/XWiki/XWikiPreferences  ， 看是否可以访问到。正常右边这个抽屉栏应该是有一个按钮的，你这个可能因为初始化错误，导致数据错乱了。如果没办法进入这个页面，那就只能重新部署了。在未登录的情况下，使用/bin/import/XWiki/XWikiPreferences，跳出登录页面。
你这个是进入到pod里面看到的data，应该有一个绑定在实体机上的目录。
这个是有的。用的是nfs,在nfs服务器上的wiki目录下能看到这些文件。

和在容器中看到的是相同的。嗯，那这个数据卷是没错的。
但是你这种情况，因为配置初始化失败，导致数据错乱，admin用户也没办法进入到这个导入界面。所有你可能只能重新部署wiki了，真的很抱歉。 你现在这个新的wiki里面有用户数据吗？如果没有的话，你可以这样做：
1、停止并删除wiki的实例。
2、清空wiki数据库中的所有表。
3、删除卷目录，也就是nfs服务器上wiki目录下的内容。
4、按照部署手册再安装一次。
安装过程中查看下xwiki-init-config的pod日志是否有异常。
真的很抱歉，我们下个版本就会优化这个配置初始化流程，可以较大程度的降低失败的概率。我先重新布署一次试试。有问题再请教。重新布署wiki,配置初始化时错误
配置初始化时的这个错误时正常的，因为初始化的时候就是给wiki打入这个包，这样在wiki正式启动之后就不会有这个错了。你继续看后面的日志内容。最好截图给我看下。在参照微服务后端开发的时候启动报错
Caused by: java.lang.IllegalStateException: Ambiguous mapping. Cannot map ‘/v1/users’ method
private org.springframework.http.ResponseEntity<io.choerodon.todo.api.dto.UserDTO> io.choerodon.todo.api.controller.v1.UserController.create(io.choerodon.todo.api.dto.UserDTO)
to {[/],methods=[POST]}: There is already ‘/v1/tasks’ bean method两个controller如下
注释任何一个， 或者在任意一个create上面加上其他路径，就没问题？这就是springMVC报的错，你有俩接口的url冲突了两个url都不一样， 怎么会冲突呢？你好，可以通过idea的插件检查下有没有接口冲突
嗯嗯， 谢谢， 我试下我是了下需要
@RestController
@RequestMapping(value = “/v1/users”)
这样的注解才不会冲突？这个文档是不是要更新下？
http://choerodon.io/zh/docs/development-guide/backend/demo/api/这个是我们的错误，我们休整一下，谢谢Choerodon平台版本: 0.9.0遇到问题的执行步骤:部署的时候看到gitlab上面已经创建了环境的文件，部署一直处于循环当中
之前参与过这个帖子的讨论，里面提到的问题都已经配置过了。




部署流水线部署报错 Continuous Delivery


    你把SSH那个YML卸载，修改一下。另外安装 
原文件： 
 [image] 
修改成： 
[image]
在试一下
  

这是choerodon-agent里面报的问题
devops-service 有这种错
gitlab webhook回调有权限问题，不过还没解决
你好，请问一下devops_service的版本是多少，还有devops-service服务中这个部署参数是否有SECURITY_IGNORED是否包含了 /webhook/**.是的，缺少了这个，谢谢按照，demo规范写出来的代码，访问时，出错。session was not found 怎么解决呢？你好，能将错误的代码和错误信息贴出来吗Choerodon平台版本：0.9.0运行环境：一键安装问题描述：敏捷管理 菜单—》如何统计每个人的周工作执行的操作：报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作目前系统对于工作量的统计主要是以迭代为单位，后续的版本会加强问题的视图设置，会有灵活配置视图的功能。
下图为迭代工作量统计：
其他的一些数据在报告里也会呈现，目前报告多为切合敏捷流程的相关报告额，好的迭代这样的话只能每周创建一个冲刺？冲刺只能开启一个。不能多个因为要基于周统计工作时间就定义一周一个迭代肯定不对，我们后续会添加其他功能支持这种多时间维度的工作量统计，谢谢反馈这样的需求。迭代的时长是根据项目和团队计划而定的，一般是一周、两周、四周，不超过一个月为宜。
一个项目只能有一个开启的活跃冲刺Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：文档地址：http://choerodon.io/zh/docs/development-guide/backend/intergration/init/
文档章节： 开发手册 - 后端开发手册 - 测试与集成 - 数据初始化
运行 init-manager-service.sh 之后报 groovy.lang.MissingPropertyException: No such property: helper for class: org.liquibase.groovy.delegate.ChangeSetDelegate完整的错误栈如下：而且文档中让执行的脚本命令是不是也不对啊？不是应该执行 init-manager-service.sh 吗？
你好，麻烦提供一下工具包的版本，和初始化服务的版本。文档的错误我们修复一下你好，工具包版本是：0.5.2.RELEASE如果iam-service 和 manager-service 是拉取的master分支代码。可以下载最新的工具包0.6.2.RELEASE。目前文档上给的连接指向0.5.2的：http://choerodon.io/zh/docs/development-guide/backend/demo/init_db/0.6.2 工具包的下载地址可以提供一下吗？谢谢我们会尽快修复文档的错误的下载链接：
https://oss.sonatype.org/content/groups/public/io/choerodon/choerodon-tool-liquibase/0.6.2.RELEASE/choerodon-tool-liquibase-0.6.2.RELEASE.jar好的，谢谢。 刚刚试过 0.6.2 版本的工具包并不会报错。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
项目管理中->待办事项，创建冲刺，显示成功，但刷新无结果。如图：
创建问题失败，日志报错：
已在 敏捷管理功能不能使用 回复刚发现这个产品确实有点惊喜。整个平台的想法很好。完整的把devops流程串起来了。完整度很高。但是产品的成熟度还有待加强，至少目前在部署这一步就好多问题。还有社区建设，文档细化，建议可以加上常见问题解决方法,我看技术人员每天重复答贴也是蛮累的。也确实是很想用起来，也在这上面花了好几周的时间折腾。无奈坑确实太多了，有点爱不起了，最终只能先放手了。期间也提了很多问题，也很感谢技术人员的及时支持。希望产品越做越好，早日实现产品化吧。​你好！我是Choerodon猪齿鱼社区的负责人 甘闪闪，非常感谢你提出的意见，我们会认真对待。Choerodon猪齿鱼产品部署这边确实有很多问题，我们内部也在不断的改进，希望产品部署更加方便；社区中的文档等我们会进一步细化更新。另外，我们能相互加个微信号码，我的微信是18601777272。好的  我加你了Choerodon平台版本: 0.9.0遇到问题的执行步骤:
创建应用步骤，一直卡在创建中。查看gitlab已经创建完成。环境信息(如:节点信息):
k8s 1.8.5报错日志:
后台日志这个日志是git问题？发现没有gitlba-service服务https://choerodon.com.cn里面也有同样的问题
@vinkdong @crockitwood  这个问题可以帮忙看下吗  谢谢！是采用一键部署的吗，查看一下devops-serviceSERVICES_GATEWAY_URL这个变量是否没带http://您好, 请把创建好的gitlab project clone下来， 做下修改，然后git push 看是否能够push成功我看了  有http://没问题  可以正常push。创建应用的时候选择的是内置的模板吗？ 你在重新创建一个应用试一下，看是不是都是报error.git.push？两种情况都测试了。都报error.git.push根据文档安装完成后，在整合界面上的组织管理里，应用模板403，创建项目后，项目下面相关的敏捷管理、流水线管理等都是403.
查看了iam_permission表，应该是相应的权限没有添加上。如图，请问如何添加这一块的权限。每个模块的功能接口路径该怎么查。有没有相应的文档。请问如何联系你们，方便沟通。
权限表如下图：
我看你的反馈是，管理里面的微服务管理和组织下的客户端页面都是正常的，没有403，然后应用模板和敏捷管理403，这两个页面分别是devops-service和agile-service，看下permission表里面有没有这两个service_name的permission，没有的话看下这两个服务有没有正常启动，如果正常的话尝试重启下这两个服务，如果还不行用admin登陆平台，管理 -> api管理 -> api测试 -> manager-service
看了下文档，初始化数据库的，只有mysql格式的，
并且发现manager-service的提供的liquibase脚本，没有序列。抱歉，更新完，看到manager_serivice的序列了报ORA-OO942是你这个用户的权限不够，用dba给这个用户加上如下权限就行了
GRANT select ON V_$PARAMETER TO manager_service;贴一个我测试时的建库建表脚本Choerodon平台版本：0.8.0运行环境：https://choerodon.com.cn问题描述：
创建应用，生成的代码库有时候会创建2个master保护分支，如下图
哪个项目和应用，我去看一下最新发现的项目，还有问题
https://code.choerodon.com.cn/hand-rongjing-hec/brms/settings/repository
这个查看所有api接口的管理界面需要什么权限才可以看到呢
暂时需要平台管理员的角色。下一个版本会添加平台开发者的角色，包含微服务管理、api测试、事务管理等功能权限Choerodon平台版本: 0.9.0遇到问题的执行步骤:
wiki部署报错。环境信息(如:节点信息):
k8s 1.8.5报错日志:
13-Sep-2018 01:30:59.326 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [“http-nio-8080”]
13-Sep-2018 01:30:59.347 INFO [main] org.apache.coyote.AbstractProtocol.start Starting ProtocolHandler [“ajp-nio-8009”]
13-Sep-2018 01:30:59.352 INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 49036 ms
2018-09-13 01:31:53,770 [XWiki initialization] INFO  .HibernateDataMigrationManager - Storage schema updates and data migrations are enabled
13-Sep-2018 01:31:54.054 INFO [MessageDispatcher] org.artofsolving.jodconverter.office.OfficeConnection$1.disposing disconnected: ‘socket,host=127.0.0.1,port=8100,tcpNoDelay=1’
13-Sep-2018 01:31:54.054 WARNING [MessageDispatcher] org.artofsolving.jodconverter.office.PooledOfficeManager$1.disconnected connection lost unexpectedly; attempting restart
13-Sep-2018 01:31:54.063 INFO [OfficeProcessThread-0] org.artofsolving.jodconverter.office.ManagedOfficeProcess.doEnsureProcessExited process exited with code 137
13-Sep-2018 01:31:54.105 INFO [OfficeProcessThread-0] org.artofsolving.jodconverter.office.OfficeProcess.start starting process with acceptString ‘socket,host=127.0.0.1,port=8100,tcpNoDelay=1’ and profileDir ‘/usr/local/tomcat/temp/.jodconverter_socket_host-127.0.0.1_port-8100’
13-Sep-2018 01:31:54.131 INFO [OfficeProcessThread-0] org.artofsolving.jodconverter.office.OfficeProcess.start started process; pid = 162
2018-09-13 01:31:54,553 [XWiki initialization] INFO  .HibernateDataMigrationManager - No data migration to apply for wiki [xwiki] currently in version [1004001]
2018-09-13 01:31:54,553 [XWiki initialization] INFO  .HibernateDataMigrationManager - Checking Hibernate mapping and updating schema if needed for wiki [xwiki]
13-Sep-2018 01:31:54.635 SEVERE [OfficeProcessThread-0] org.artofsolving.jodconverter.office.ManagedOfficeProcess$5.run could not restart process
org.artofsolving.jodconverter.office.OfficeException: could not establish connection
at org.artofsolving.jodconverter.office.ManagedOfficeProcess.doStartProcessAndConnect(ManagedOfficeProcess.java:162)
at org.artofsolving.jodconverter.office.ManagedOfficeProcess.access$000(ManagedOfficeProcess.java:25)
at org.artofsolving.jodconverter.office.ManagedOfficeProcess$5.run(ManagedOfficeProcess.java:123)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.artofsolving.jodconverter.office.OfficeException: office process died with exit code 137
at org.artofsolving.jodconverter.office.ManagedOfficeProcess$6.attempt(ManagedOfficeProcess.java:155)
at org.artofsolving.jodconverter.office.Retryable.execute(Retryable.java:36)
at org.artofsolving.jodconverter.office.Retryable.execute(Retryable.java:25)
at org.artofsolving.jodconverter.office.ManagedOfficeProcess.doStartProcessAndConnect(ManagedOfficeProcess.java:135)
… 5 more
Caused by: java.net.ConnectException: connection failed: ‘socket,host=127.0.0.1,port=8100,tcpNoDelay=1’; java.net.ConnectException: Connection refused (Connection refused)
at org.artofsolving.jodconverter.office.OfficeConnection.connect(OfficeConnection.java:108)
at org.artofsolving.jodconverter.office.ManagedOfficeProcess$6.attempt(ManagedOfficeProcess.java:141)office的一个插件报错，安装时可以不用理会这个问题，一般安装成功后重启这个服务就可。安装完成后前台还是403  需要做什么操作？2018-09-13 10:44:14.689  WARN [wiki-service,] 1 — [nfoReplicator-0] c.n.discovery.InstanceInfoReplicator     : There was a problem with the instance info replicatororg.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘eurekaInstanceConfigBean’ defined in class path resource [org/springframework/cloud/netflix/eureka/EurekaClientAutoConfiguration.class]: Unsatisfied dependency expressed through method ‘eurekaInstanceConfigBean’ parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘inetUtils’ defined in class path resource [org/springframework/cloud/commons/util/UtilAutoConfiguration.class]: Unsatisfied dependency expressed through method ‘inetUtils’ parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘inetUtilsProperties’ defined in class path resource [org/springframework/cloud/commons/util/UtilAutoConfiguration.class]: Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@58fdd99 has not been refreshed yet
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:749) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:467) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1173) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1067) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:513) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:208) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1138) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1066) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:835) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.resolvePreparedArguments(ConstructorResolver.java:785) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:415) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1173) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1067) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:513) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory$2.getObject(AbstractBeanFactory.java:345) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.cloud.context.scope.GenericScope$BeanLifecycleWrapper.getBean(GenericScope.java:359) ~[spring-cloud-context-1.2.4.RELEASE.jar!/:1.2.4.RELEASE]
at org.springframework.cloud.context.scope.GenericScope.get(GenericScope.java:176) ~[spring-cloud-context-1.2.4.RELEASE.jar!/:1.2.4.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:340) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.target.SimpleBeanTargetSource.getTarget(SimpleBeanTargetSource.java:35) ~[spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.getTarget(CglibAopProxy.java:705) ~[spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:655) ~[spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at com.netflix.appinfo.ApplicationInfoManager$$EnhancerBySpringCGLIB$$90c5ddb4.refreshDataCenterInfoIfRequired() ~[eureka-client-1.6.2.jar!/:1.6.2]
at com.netflix.discovery.DiscoveryClient.refreshInstanceInfo(DiscoveryClient.java:1357) ~[eureka-client-1.6.2.jar!/:1.6.2]
at com.netflix.discovery.InstanceInfoReplicator.run(InstanceInfoReplicator.java:100) ~[eureka-client-1.6.2.jar!/:1.6.2]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_121]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_121]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_121]
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [na:1.8.0_121]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘inetUtils’ defined in class path resource [org/springframework/cloud/commons/util/UtilAutoConfiguration.class]: Unsatisfied dependency expressed through method ‘inetUtils’ parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘inetUtilsProperties’ defined in class path resource [org/springframework/cloud/commons/util/UtilAutoConfiguration.class]: Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@58fdd99 has not been refreshed yet
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:749) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:467) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1173) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1067) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:513) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:208) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1138) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1066) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:835) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:741) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
… 37 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘inetUtilsProperties’ defined in class path resource [org/springframework/cloud/commons/util/UtilAutoConfiguration.class]: Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@58fdd99 has not been refreshed yet
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:564) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:483) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:208) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1138) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1066) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:835) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:741) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
… 51 common frames omitted
Caused by: java.lang.IllegalStateException: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@58fdd99 has not been refreshed yet
at org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:1068) ~[spring-context-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.context.support.GenericApplicationContext.getAutowireCapableBeanFactory(GenericApplicationContext.java:307) ~[spring-context-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.validation.beanvalidation.LocalValidatorFactoryBean.afterPropertiesSet(LocalValidatorFactoryBean.java:275) ~[spring-context-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.boot.context.properties.ConfigurationPropertiesBindingPostProcessor$ValidatedLocalValidatorFactoryBean.(ConfigurationPropertiesBindingPostProcessor.java:412) ~[spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.boot.context.properties.ConfigurationPropertiesBindingPostProcessor.getValidator(ConfigurationPropertiesBindingPostProcessor.java:368) ~[spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.boot.context.properties.ConfigurationPropertiesBindingPostProcessor.determineValidator(ConfigurationPropertiesBindingPostProcessor.java:352) ~[spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.boot.context.properties.ConfigurationPropertiesBindingPostProcessor.postProcessBeforeInitialization(ConfigurationPropertiesBindingPostProcessor.java:314) ~[spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.boot.context.properties.ConfigurationPropertiesBindingPostProcessor.postProcessBeforeInitialization(ConfigurationPropertiesBindingPostProcessor.java:291) ~[spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:409) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1620) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:555) ~[spring-beans-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
… 61 common frames omitted2018-09-13 10:44:15.068  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.wiki.infra.feign.IamServiceClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 10:44:15.069  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 10:44:15.070  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaMonitorClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 10:44:15.070  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.consumer.FailedMsgEventStoreFeign’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 10:44:15.071  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.producer.execute.EventStoreClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 10:44:15.077  INFO [wiki-service,] 1 — [pool-2-thread-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@5bb76062: startup date [Thu Sep 13 10:44:15 CST 2018]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@58fdd99wiki 后台服务一直重启。请问Choerodon的其他服务启动时正常的吗？wiki-service一直是因为上面的报错吗？其他服务都正常。上面的报错。 还有这个WARN。
2018-09-13 13:03:09.764  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 13:03:09.765  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaMonitorClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 13:03:09.766  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.consumer.FailedMsgEventStoreFeign’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 13:03:09.766  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.producer.execute.EventStoreClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 13:03:09.772  INFO [wiki-service,] 1 — [pool-2-thread-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@1fc5f848: startup date [Thu Sep 13 13:03:09 CST 2018]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@58fdd99
2018-09-13 13:03:09.786  WARN [wiki-service,] 1 — [pool-2-thread-1] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPing
2018-09-13 13:03:10.793  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.wiki.infra.feign.IamServiceClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-09-13 13:03:10.794  WARN [wiki-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
error: unexpected EOF–name=wiki-service 
–version=0.9.1 \是不是版本不对？是0.9.1，没错的。你如果你其他服务都启动正常的话，你再尝试重新部署下wiki-service试试。我这边测试的这个版本的部署是没问题的哦！其他模块都正常跑了。  重新部署了还是不行。  这个错误是是服务没注册上吗？这个是一个WARN ， 应该不影响服务正常运行的。你现在的问题是wiki-service会自动重启？服务分配的内存是多少呢？会不会是资源不足，导致没办法跑起来。我加到4G了。请问一下，k8s部署wiki的命名空间有没有加到注册服务配置参数里面。如果所有的服务部署在一个命令空间下，就不存在我描述的这个问题。helm install c7n/wiki-service 
–set preJob.preConfig.mysql.host=choerodon-mysql 
–set preJob.preConfig.mysql.port=3306 
–set preJob.preConfig.mysql.database=manager_service 
–set preJob.preConfig.mysql.username=devops 
–set preJob.preConfig.mysql.password=password 
–set preJob.preInitDB.mysql.host=choerodon-mysql 
–set preJob.preInitDB.mysql.port=3306 
–set preJob.preInitDB.mysql.database=wiki_service 
–set preJob.preInitDB.mysql.username=choerodon 
–set preJob.preInitDB.mysql.password=password 
–set env.open.SPRING_DATASOURCE_URL=“jdbc:mysql://choerodon-mysql:3306/wiki_service?useUnicode=true&  characterEncoding=utf-8&useSSL=false” 
–set env.open.SPRING_DATASOURCE_USERNAME=choerodon 
–set env.open.SPRING_DATASOURCE_PASSWORD=password 
–set env.open.EUREKA_CLIENT_SERVICEURL_DEFAULTZONE=“http://register-server.choerodon-devops-ofc:8000/eureka/” 
–set env.open.EUREKA_DEFAULT_ZONE=http://register-server.choerodon-devops-ofc:8000/eureka/ 
–set env.open.CHOERODON_EVENT_CONSUMER_KAFKA_BOOTSTRAP_SERVERS=“kafka-0.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092,kafka-1.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092,kafka-2.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092” 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=“kafka-0.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092,kafka-1.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092,kafka-2.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092” 
–set env.open.SPRING_KAFKA_BOOTSTRAP_SERVERS=“kafka-0.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092,kafka-1.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092,kafka-2.kafka-headless.choerodon-devops-ofc.svc.cluster.local:9092” 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES=“zookeeper-0.zookeeper-headless.choerodon-devops-ofc.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.choerodon-devops-ofc.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.choerodon-devops-ofc.svc.cluster.local:2181” 
–set env.open.SPRING_KAFKA_PRODUCER_VALUE_SERIALIZER=org.apache.kafka.common.serialization.ByteArraySerializer 
–set env.open.SPRING_CLOUD_CONFIG_ENABLED=true 
–set env.open.SPRING_CLOUD_CONFIG_URI=http://config-server.choerodon-devops-ofc:8010/ 
–set env.open.WIKI_CLIENT=xwiki 
–set env.open.WIKI_URL=http://wiki.66team.cn 
–set env.open.WIKI_TOKEN=Choerodon 
–set env.open.WIKI_DEFAULT_GROUP=XWikiAllGroup 
–set resources.limits.memory=4Gi 
–name=wiki-service 
–version=0.9.1 
–namespace=choerodon-devops-ofc在同一个命名空间的。注册服务管理页面查看下是否有wiki-service服务。注册服务管理页面查看下是否有wiki-service服务。没有注册上。我把内存加大了。现在日志一直打这个。
2018-09-17 09:35:07.657  WARN [wiki-service,] 1 — [pool-2-thread-1] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPingCaused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘inetUtils’ defined in class path resource [org/springframework/cloud/commons/util/UtilAutoConfiguration.class]: Unsatisfied dependency expressed through method ‘inetUtils’ parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘inetUtilsProperties’ defined in class path resource [org/springframework/cloud/commons/util/UtilAutoConfiguration.class]: Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@58fdd99 has not been refreshed yet[Fatal Error] :1:1: Content is not allowed in prolog.
2018-09-17 05:09:03,926 [localhost-startStop-1] WARN  .e.r.i.l.LocalExtensionStorage - Failed to load extension from file [/usr/local/xwiki/data/extension/repository/org%2Ecryptomator%3Asiv-mode/1%2E3%2E1/._org%2Ecryptomator%3Asiv-mode-1%2E3%2E1.xed] in local repository
org.xwiki.extension.InvalidExtensionException: Failed to parse descriptor
at org.xwiki.extension.repository.internal.DefaultExtensionSerializer.getExtensionElement(DefaultExtensionSerializer.java:271)
at org.xwiki.extension.repository.internal.DefaultExtensionSerializer.loadLocalExtensionDescriptor(DefaultExtensionSerializer.java:248)
at org.xwiki.extension.repository.internal.local.LocalExtensionStorage.loadDescriptor(LocalExtensionStorage.java:170)
at org.xwiki.extension.repository.internal.local.LocalExtensionStorage.loadExtensions(LocalExtensionStorage.java:142)
at org.xwiki.extension.repository.internal.local.LocalExtensionStorage.loadExtensions(LocalExtensionStorage.java:139)
at org.xwiki.extension.repository.internal.local.LocalExtensionStorage.loadExtensions(LocalExtensionStorage.java:139)
at org.xwiki.extension.repository.internal.local.LocalExtensionStorage.loadExtensions(LocalExtensionStorage.java:111)
at org.xwiki.extension.repository.internal.local.DefaultLocalExtensionRepository.initialize(DefaultLocalExtensionRepository.java:106)
at org.xwiki.component.embed.InitializableLifecycleHandler.handle(InitializableLifecycleHandler.java:39)
at org.xwiki.component.embed.EmbeddableComponentManager.createInstance(EmbeddableComponentManager.java:360)
at org.xwiki.component.embed.EmbeddableComponentManager.getComponentInstance(EmbeddableComponentManager.java:446)
at org.xwiki.component.embed.EmbeddableComponentManager.getInstance(EmbeddableComponentManager.java:196)
at org.xwiki.component.embed.EmbeddableComponentManager.getDependencyInstance(EmbeddableComponentManager.java:401)
at org.xwiki.component.embed.EmbeddableComponentManager.createInstance(EmbeddableComponentManager.java:350)
at org.xwiki.component.embed.EmbeddableComponentManager.getComponentInstance(EmbeddableComponentManager.java:446)
at org.xwiki.component.embed.EmbeddableComponentManager.getInstance(EmbeddableComponentManager.java:196)
at org.xwiki.component.embed.EmbeddableComponentManager.getDependencyInstance(EmbeddableComponentManager.java:401)
at org.xwiki.component.embed.EmbeddableComponentManager.createInstance(EmbeddableComponentManager.java:350)
at org.xwiki.component.embed.EmbeddableComponentManager.getComponentInstance(EmbeddableComponentManager.java:446)
at org.xwiki.component.embed.EmbeddableComponentManager.getInstance(EmbeddableComponentManager.java:196)
at org.xwiki.component.embed.EmbeddableComponentManager.getInstance(EmbeddableComponentManager.java:184)
at org.xwiki.container.servlet.XWikiServletContextListener.contextInitialized(XWikiServletContextListener.java:120)
at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4792)
at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5256)
at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)
at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:754)
at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:730)
at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:734)
at org.apache.catalina.startup.HostConfig.deployDirectory(HostConfig.java:1140)
at org.apache.catalina.startup.HostConfig$DeployDirectory.run(HostConfig.java:1875)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.xml.sax.SAXParseException: Content is not allowed in prolog.
at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:121)
at org.xwiki.extension.repository.internal.DefaultExtensionSerializer.getExtensionElement(DefaultExtensionSerializer.java:269)
… 34 common frames omitted现在是xwiki报这个错误。这个xwiki的报错是启动的时候还是运行了一段时间的呢？前台创建空间的时候。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
创建项目环境信息(如:节点信息):
k8s 1.8.5报错日志:
通过前台创建项目，显示‘创建成功’，但是前台列表里面没有显示创建的项目。你好，是自己搭建的平台吗是的。 重新部署就可以了。  部署了几十遍。  Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：k8s在部署一个后端应用时：这些地方都可以配置数据库，eureka等等。那请问这三者之间有什么区别？
以及要这么设计的原因是什么。希望能够解答一下我对这个的迷惑，谢谢！resources 下的文件是spring boot 启动的配置文件。用于本地和定义服务默认配置项。values 里面的文件包含环境变量和k8s chart中提前定义好的变量。用于定义发布的chart应用中，可以修改的默认配置项。部署时的页面上的配置文件是基于chart 包中values 里面的文件和上次部署修改过后生成的。用于实际部署中服务的配置项。其中 部署时配置的环境变量 > values 里面的默认值 > spring 配置文件里的默认值。通过这种方式将服务的配置抽离，增强不同环境下部署的便捷性，避免因为环境不同而所需配置不同带来的问题。那比如说 部署时配置的环境变量或者values里面的数据库配置，这个我微服务里面的代码能够读取到吗？或者说 我的应用要使用的数据库就是配置在这个地方吗？可以的，spring boot的配置读取中，环境变量的优先级高于application.yml。通过部署或者values本质就是配置同名环境变量。例如：环境变量中的SPRING_DATASOURCE_URL对应application.yml 文件中的spring.datasource.url可以参考
https://docs.spring.io/spring-boot/docs/1.5.16.RELEASE/reference/htmlsingle/#boot-features-external-config好的，谢谢！Choerodon平台版本: 0.8.0遇到问题的执行步骤:
由于一开始的服务器的磁盘空间不够了，挂载了一个新的磁盘，之后将nfs服务停掉后，把之前nfs开放的路径进行备份，并挂载到新的磁盘上。之后重启所有服务器，及猪齿鱼相关服务器。发现就gitlab一直启动不了，查看了日志发现有报错环境信息(如:节点信息):
4节点  32g报错日志:
疑问:
https://gitlab.com/gitlab-org/omnibus-gitlab/issues/2785
发现gitlab官网上有一样的问题 ，但未找到解决方案你好，这个问题我们也没有遇到过，请尝试参考这里进行设置，谢谢。I’m trying to install gitlab on debian jessie system by package manager. After successful install it requires me to run ‘gitlab-ctl reconfigure’, and when i do it, fails with such output:   [skipped successful steps] *...我根据这个上面说的 将我在nfs对应gitlab的路径全部设置成755了还是不行我解决了 ，我第一次 复制的时候 用的 cp -r 导致 文件属性没用复制过来 ，我再次使用 cp -a 进行复制就可以了打开哪个界面出现的呢选完组织进的首页，看路由应该也可以看出来哦我这边是好的，可以F12看一下控制台输出，有没有报错信息。一般chrome 出现“喔唷，崩溃啦” 是因为浏览器自身的原因。啊？两个浏览器都这样诶非常抱歉给你的使用带来不便！这确实是我们的bug，这个bug本来已在新版本上修复，由于考虑到触发条件比较严格所以没有即时修订再次抱歉:joy:客气啦哈。感谢CMDB在整个运维体系的基石。后续猪齿鱼会考虑增加这个模块吗？暂时还没有这方面的产品规划。{
“accessTokenValidity”: 60,
“additionalInformation”: “”,
“authorizedGrantTypes”: “implicit,client_credentials,authorization_code,refresh_token”,
“autoApprove”: “default”,
“name”: “iam”,
“objectVersionNumber”: 0,
“organizationId”: 1,
“refreshTokenValidity”: 60,
“resourceIds”: “default”,
“scope”: “default”,
“secret”: “secret”,
“webServerRedirectUri”: “http://iam.choerodon.example.choerodon.io”
}这里面的secret取的是什么值呢？这个跟gitlab的webhood调用是否成功有没有关系？问：这个跟gitlab的webhood调用是否成功有没有关系？
答：没关系哈那哪些点跟webhood有关系呢，现在gitlab回调是403webhook的回调和client没关系，webhook的回调只与devops服务的api有关系，是升级到0.9之后，webhook的回调一直403吗？刚开始是ssh那些配置没配好。后面在环境里部署应用还是一直转，然后就查到webhook权限有问题。
请查看集群中部署的devops服务中的环境变量 SECURITY_IGNORED的值是否含有 /webhook/**嗯， 就是这个原因，可以了，谢谢1、查询条件优化建议：
用户常用的待办事项、问题管理等敏捷管理功能下由于条目可能会很多，因此便捷的查询条件显得尤为重要。当前查询条件太过死板和单一。甚至基本的按照问题标题、报告人、标签等问题基本信息进行查询的条件都不具备，建议尽快完善查询条件，优化查询方式，或支持普通用户自定义查询条件等。
普通用户首页便捷模块问题：
普通用户首页的看板内容，有的是猪齿鱼产品相关内容或教程如测试管理、文档、快速入门等，有的可以转接到当前项目下的工作内容，如我未完成的问题、版本进度等，两类看板乱序放在一起，用户很容易感到困惑，而且用于自定义看板只能选择是否启用看板展示，不能自定义拖拽看板位置，现在感觉看板的实际作用不是很大。且敏捷管理的首页只有刚登陆首页后选择项目才可跳转到，进入敏捷管理的具体功能后无法回到此看板页面，只能重新进入网站首页再次点击项目才可看到。
2、权限管理问题：
目前一个项目下创建的问题，所有项目成员均可修改问题内容。这样对于问题的安全性管理很不友好，通常一个问题或待办事项只有经办人、报告人或管理员才会对问题本身有管理权，其他用户最多有备注或者添加信息的权利，可以直接修改问题很有可能会对问题本身描述发生误改动，且当前的修改方式比较快速，直接点击字段即可修改，就更加大了误操作的概率。建议对此问题尽快做出优化。谢谢！一个一个问题回答：待办事项中，可能确实存在查询条件较少的问题，其中按照问题标题查找这个可能是比较需要的，报告人可以通过切换泳道模式太进行按人分组。问题管理界面的查询条件相对是比较完善的，存常见字段状态，优先级到概要，编号再到版本，史诗都有。我们会在这方面进行规整和重新思考。关于首页的问题，如果用户直接登录，可以看到上面的项目是没有选择的，这时候进入的是全局层的主页，即你提到的测试管理，文档，快速入门。而选择项目后会进入项目层主页，即你提到的测试管理，文档，快速入门，和敏捷的一系列卡片。一般也只有第一次进行自定义时会比较多，这方面应该不是问题。卡片之间是可以进行拖拽的，选中标题部分即可拖拽。回到首页的问题，进入任何路由后，点左上角Choerodon左边的三条横杠，打开缩略菜单，那个主页按钮就是回到项目层首页的。3.关于权限管理问题我们也有考虑到过，所以让每个人的修改生成活动日志记录并显示，而且一个团队不会存在故意修改他们信息的情况。对你的问题我们会进行讨论和重新预计，thx:grin:回复收到，谢谢！希望猪齿鱼越来越好用。Choerodon平台版本：SaaS 最新版运行环境：自主搭建问题描述：在活跃冲刺中新建看板列和看板状态时，如果列和状态的名称一致，就会互相提示名字重复，并且点击创建按钮之后就没反应了。不知道是我的操作问题，还是系统界面的逻辑处理问题。创建列时会同步创建同名的状态，创建同名状态时不会通过校验，目前卡住的情况属于bug，我们会及时修复，谢谢反馈Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：在组织下面创建项目的时候，如果项目的中文名字和别的项目一样，那么gitlab那里就没有创建出group。但是猪齿鱼平台却没有报错，而是创建成功。这时候去猪齿鱼这个项目下创建应用就会报创建应用失败，gitlab.group.id找不到。修改的数据：原因分析：疑问：您好，这个是devops服务和iam服务现在创建项目名称校验不统一的情况，我们会尽快修复，目前不支持创同名的项目。Choerodon平台版本：0.9.0运行环境：pass问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。
一直在运行中的实例， 如何删掉？
还有创建中的应用，如何删掉？
提出您认为不合理的地方，帮助我们优化用户操作
应该是后台出bug了你好，这边我通过看日志，发现实例删除中是因为，前几个实例部署时，环境中并没有创建c7nhelmrelease这个crd，导致环境中不存在这个实例，以致于删除时，环境不存在实例，没删除结果的反馈。这里逻辑有一定漏洞，实例这边可以通过操作配置库清楚删除中实例。在后面新版本中，我们会通过其他一致性同步逻辑清理中间状态。对于应用，这个应用是以前创建的，时间过去太久了，判断不了原因，对于现在版本中，可以通过sega重试创建。现在暂时没有删除创建中应用功能。好的， 感谢
这个有办法删除吗，一直处于这个状态Choerodon平台版本: 0.8.0遇到问题的执行步骤:
发现使用c7n/mysql 部署的mysql的时区为 UTC的 ，怎么操作可以直接同步服务器的时区信息文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/mysql/报错日志:

mysql容器的时间

服务器时间在部署mysql时可以指定环境变量 如TZ='America/Los_Angeles' 以指定时区网上说是因为 kubenertes-cni 0.5.1 没有 portmap 插件引起的，不过 kubeadm-ansible 也是用的1.8.5, 它是如何部署的呢？重启docker试一下 ?重启了，还是一样的。只有kube-dns会这样吗 还是其他pod也是一样的， 把其他pod删除试试我的 flannel 网络插件中有设置 portmap = true，网上说 kubernetes-cni 默认不会装 portmap 插件。 我在部署完1.8.5集群之后，强制将kubeadm, kubelet 和kubectl, 升级到1.9.2 然后 kubernetes-cni升级到0.6.0之后就正常了。我奇怪的是，kubeadm-ansible 是怎么部署的？因为我部署完之后发现也都变成1.9.2和0.6.0了。。。请在每个节点执行以下语句修复错误Calico CNI plugin. Contribute to projectcalico/cni-plugin development by creating an account on GitHub.因为你执行了yum update或yum upgrade操作用你提供的方法，问题已经得到了解决。多谢！调整后的脚本如下：执行了 yum update 版本会立即跳跃到 v1.11.3 …这个只是yum包升级了而已，但其实集群api server还是1.8.5，故集群版本还是1.8.5Choerodon平台版本：0.9.0运行环境：SaaS环境问题描述：已做修改，感谢提醒。Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
通过gitlab-ci对choerodon-front进行编译打包遇到pymysql找不到
但是gitlab-ci使用的镜像是cifront;应该已经安装了pymysql,为什么没有起作用呢？
原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问如果在gitlab-ci加上 pip install pymysql, 会报https://pypi.python.org/simple/这个地址找不到相应的东西。
我看到猪齿鱼用到自己的地址拉取。这个地址是怎么去修改的？
你好，可以参考最新的版本使用registry.cn-hangzhou.aliyuncs.com/choerodon-tools/cifront:0.6.0解决了，谢谢Choerodon平台版本: 0.8.0遇到问题的执行步骤:
突然发现自主搭建的猪齿鱼平台不可登陆，登陆服务器发现有个node 计划停用环境信息(如:节点信息):
4节点  32g报错日志:
是否有人手动设置了这个属性呢
try:Choerodon平台版本: 0.9.0遇到问题的执行步骤:
notify-service 状态404
日志未见异常文档地址:环境信息(如:节点信息):报错日志:
2018-09-12 22:25:51.409  INFO [notify-service,] 1 — [ XNIO-2 task-31] c.c.c.ConfigServicePropertySourceLocator : Located environment: name=manager-service, profiles=[default], label=null, version=null, state=null
2018-09-12 22:26:47.100  INFO [notify-service,] 1 — [ntainer#0-0-L-1] i.c.n.a.e.RegisterInstanceListener       : receive message from register-server, {“status”:“UP”,“appName”:“oauth-server”,“version”:“0.9.0”,“instanceAddress”:“172.20.203.15:8020”,“createTime”:“2018-09-12 22:26:47”}
2018-09-12 22:26:47.100  INFO [notify-service,] 1 — [ntainer#0-0-L-1] i.c.n.a.e.RegisterInstanceListener       : skip message that is skipServices, RegisterInstancePayloadDTO(status=UP, appName=oauth-server, version=0.9.0, instanceAddress=172.20.203.15:8020, createTime=Wed Sep 12 22:26:47 CST 2018, apiData=null)
2018-09-12 22:29:57.958  INFO [notify-service,] 1 — [trap-executor-0] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
2018-09-12 22:31:01.346  INFO [notify-service,] 1 — [ XNIO-2 task-30] c.c.c.ConfigServicePropertySourceLocator : Fetching config from server at: http://config-server.choerodon-devops-prod:8010/
2018-09-12 22:31:01.362  INFO [notify-service,] 1 — [ XNIO-2 task-30] c.c.c.ConfigServicePropertySourceLocator : Located environment: name=manager-service, profiles=[default], label=null, version=null, state=null
2018-09-12 22:34:57.959  INFO [notify-service,] 1 — [trap-executor-0] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
2018-09-12 22:36:01.351  INFO [notify-service,] 1 — [ XNIO-2 task-28] c.c.c.ConfigServicePropertySourceLocator : Fetching config from server at: http://config-server.choerodon-devops-prod:8010/
2018-09-12 22:36:01.368  INFO [notify-service,] 1 — [ XNIO-2 task-28] c.c.c.ConfigServicePropertySourceLocator : Located environment: name=manager-service, profiles=[default], label=null, version=null, state=null
2018-09-12 22:39:57.960  INFO [notify-service,] 1 — [trap-executor-0] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configuration
2018-09-12 22:41:11.350  INFO [notify-service,] 1 — [ XNIO-2 task-27] c.c.c.ConfigServicePropertySourceLocator : Fetching config from server at: http://config-server.choerodon-devops-prod:8010/
2018-09-12 22:41:11.371  INFO [notify-service,] 1 — [ XNIO-2 task-27] c.c.c.ConfigServicePropertySourceLocator : Located environment: name=manager-service, profiles=[default], label=null, version=null, state=null
2018-09-12 22:44:25.508  INFO [notify-service,] 1 — [ntainer#0-0-L-1] i.c.n.a.e.RegisterInstanceListener       : receive message from register-server, {“status”:“UP”,“appName”:“file-service”,“version”:“0.9.0”,“instanceAddress”:“172.20.143.247:9090”,“createTime”:“2018-09-12 22:44:25”}
[root@k8sm1 ~]#
[root@k8sm1 ~]#
[root@k8sm1 ~]#
[root@k8sm1 ~]#
[root@k8sm1 ~]# curl -s $(kubectl get po -n choerodon-devops-prod -l choerodon.io/release=notify-service -o jsonpath="{.items[0].status.podIP}"):18085/health | jq -r .status
404原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问您好，sorry，这里文档有点问题，把18085改为18086试试。Choerodon平台版本: 0.9.0遇到问题的执行步骤:
一键部署环境信息(如:节点信息):
k8s 1.8.5报错日志:
choerodon-notify-service-d8dff7b94-2rbjr                0/1       OOMKilled   2[root@sh2optestsrv1 ~]# free -m
total        used        free      shared  buff/cache   available
Mem:          32156        2307       27068         440        2780       28836
Swap:             0           0           0请调整根据以下命令进行资源限制调整调整resources.limits.memory为4Gi可以了。谢谢Choerodon平台版本：0.6.0运行环境：公司提供我在猪齿鱼平台的文档上看看可以创建监控来监控集群和应用。请问这个需要怎么使用呢？文档上说需要使用管理员账号，指的是猪齿鱼平台的管理员吗？组织管理员不可以使用该功能吗？文档链接：http://choerodon.io/zh/docs/user-guide/operating-manage/newtemplate/您好，这一块的东西需要安装监控相关的产品，目前在安装文档中还没有包含这一块的内容。需要安装什么呢？是在猪齿云平台安装，还是在运行应用的kubernetes集群中安装？@xinghao 您好，请参考下面的文档
http://choerodon.io/zh/docs/installation-configuration/steps/monitoring/使用的不是自搭的猪齿云平台，如果要使用这个监控，需要怎么做？在自己的k8s集群部署即可。你好，参考官网文档部署监控的时候，发现

请问这个clusterName填的是什么，在什么地方设置的？集群名称任意指定字符串即可 用以多集群时区分不同集群，我们文档已经更新 建议你部署新版本http://choerodon.io/zh/docs/installation-configuration/steps/operation/monitoring/Choerodon平台版本: 0.9.0遇到问题的执行步骤:
查看kubelet的日志，有大量报错信息。使用 sudo journalctl -u kubelet -f -n 100 命令查询环境信息(如:节点信息):
4节点 16G报错日志:
疑问:
是有哪里部署的不正确？粘贴下一下 docker info 的信息请问你这是全新搭建还是搭建完运行了一段时间出现的？sudo journalctl -u docker -f -n 100再看看docker日志你的文件系统不支持 d_type ？Explains what is d_type, how it affects overlayfs, and how Docker would run into problem if it is missing, and how to fix the problem.Choerodon平台版本: 0.9.0遇到问题的执行步骤:
一键部署环境信息(如:节点信息):
choerodon-gitlab-758b875586-fxlds报错日志:
choerodon-gitlab-758b875586-fxlds       pod日志：如果你使用自己搭建的k8s，需要特别注意网络和存储权限，如在某些网络组件上默认禁止了不同namespace间的网络通信。在高版本的k8s中已默认禁止了configmap的写权限等。


github.com/kubernetes/kubernetes





Issue: configMap and secrets volumeMount are always mounted readOnly in 1.9.6


	opened by primeroz
	on 2018-04-04


	closed by k8s-ci-robot
	on 2018-04-05


Is this a BUG REPORT or FEATURE REQUEST?:
/kind bug
What happened:
After upgrading from 1.9.4 to 1.9.6 configMap and secrets volumes are always...

kind/bug
sig/storage






挂载pv就会报错。使用本机存储–set persistence.enabled=false 可以部署成功我把k8s版本降到1.8，这个问题还是同样存在。查看下你的绑定到gitlab的目录的权限，你是否用root用户创建的nfs文件目录？是的  初始化是用root创建的是需用用root创建吗？ 正确的权限是什么？在nfs服务器上查看是root
[root@sh2nfsserverj64 io-choerodon]# ls -lh | grep gitlab
drwxr-xr-x 2 root root 4.0K 9月  11 11:02 gitlab挂载到客户端变成nobody
[root@ofck8snode01 io-choerodon]# ls -lh | grep gitlab
drwxr-xr-x 2 nobody nobody 4.0K 9月  11 11:02 gitlab尝试下下面步骤
1.删除gitlab文件夹
2.使用root用户登录一个非nfs服务器的主机
3.创建一个临时文件夹服务默认使用root用户，对应nfs中指定文件夹都应该属于root用户，如果你使用一个nobody的用户创建就会导致存在权限问题。后续的mysql等也可能出现同样的问题造成该问题的原因是，在centos 6版本中默认使用的nfs-v4版本，其提供了称为rpc.idmapd 的守护进程，并使用 /etc/idmapd.conf 的配置文件。当请求加载nfsv4 时，该守护进程将处理 UID 和 GID 映射。默认使用nis,没有nis它会自动映射成nobody用户。既然找到了原因，找解决方法也就不难了，大致找了下网上的解决方法，分如下两种。方法一：mount中指定参数法在mount挂载的时候指定使用v3版本去挂载，如下：mount  -t  nfs  -o  vers=3  ip:/data1   /data1
显然不推荐该方法，既然有了V4版本，其肯定较V3版本做了很多优化，如果再用V3，显然跟不上时代的步伐 。用这种情况就是root权限了。 我再试试。谢谢！我们建议使用centos7.2以上的系统，默认即为root权限。[root@ofck8snode01 u01]# cat /etc/redhat-release
CentOS Linux release 7.5.1804 (Core)我的系统是7.5的现在权限都是root   但是还是一样的问题   你们部署的时候没遇到过吗请检查下你的nfs服务是否配置有问题，能够粘贴下 NFS服务器中这个文件的内容/u01  192.168..(insecure,rw,sync,no_root_squash)调成如下试试：一样的问题。
我看了其实就是在pod里面通过nfs挂载的/var/opt/gitlab的用户属主是属于nfsnobody:nfsnobody
git安装脚本判断不属于git:git就return 1报错退出Choerodon平台版本: 0.9.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问看下这篇帖子能否帮到您


github.com/monitoringartist/dockbix-xxl





Issue: [ERROR] InnoDB: Tried to read xyz bytes at offset xyz. Was only able to read 0.


	opened by lippoliv
	on 2017-06-02


	closed by jangaraj
	on 2017-06-02


Hey there,
as of I have a new strategy to Setup and Backup my Servers, I wanted to re-setup my Monitoring-Server as...







如果我重新部署mysql，哪些要重新deploy了如果你Gitlab数据库也在部署的mysql中，那么Gitlab需要重新deploy。然后猪齿鱼的所有服务请重新deploy。Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：k8s
3台腾讯云服务器遇到问题时的前置条件：
点击创建应用git地址无法显示：
问题描述：
部署成功之后，点击创建应用后，项目显示创建中，后台无法创建，后台devops-service和gitlabservice都报错；
devops-service的后台报错
但是项目在gitlab中确实是创建了
我查看了devops-service 中的
第一步我找到io.choerodon.devops.api.eventhandler.DevopsSagaHandler.createApp这个方法
提出您对于遇到和解决该问题时的疑问处理方式请查看您好：我是分步部署的: 使用您的 helm upgrade devops-service 升级，但是报：
UPGRADE FAILED: jobs.batch “devops-service-init-db” already exists
我就看了一下里面的配置，但是我的 SERVICES_GATEWAY_URL: http://api.wxc.pakchoi.top
就是戴上了这个http://
@younger我大致找到原因了，感觉是代码的问题，
首先，我0.9部署完成之后，创建应用的时候，无法显示git地址，是因为，0.9版本的持续交付这块加了GitOps模型 的原因，这块点击创建应用，开始创建项目，组织等等都是没有问题；
Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：github下载了choerodon-starters,
对这个项目进项mvn install的时候报如下错误
我看了下这个项目里也没有指定私库，想问你们是编译的时候怎么拉取到这个包的？还是把私库去掉了，需要我们自己添加私库之类的？你好，可以将对应版本的ojdbc 下载到本地仓库，然后进行编译。如果需要在ci里运行的话，可以将ojdbc上传到私服明白了

choerodon.starters.version的版本已经到0.6.3了吗你好，choerodon.starters.version 目前已经发布的版本是0.6.2。master分支的代码是我们在开发中的，你可以拉取最新tag下的代码。非常感谢Choerodon平台版本: 0.9.0遇到问题的执行步骤:
使用基础组件部署中的部署的mysql，怎么设置ingress，开放到外部网络访问文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/mysql/mysql使用tcp协议，你不能通过http协议转发，如果需要外部访问你可以在service上绑定一个主机ip是否再对应服务的 svc上 加上 externalIPs ：【开放外网的节点的内网IP】就可以了吗对是这样的Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：
编写好CI之后，报错问题描述：
我的CI：请问你使用的choerodon平台是自己搭建的还是我们这边的？自己搭建的，按照官方文档搭建的一套.auto_devops: &auto_devops |请在这一句代码下面添加然后查看CI日志拿到这句输出的链接  然后手动请求这个链接  看看是否异常这是注册的runner后从配置文件中获取的token
请问你使用的是分步搭建还是一键部署？如果你是分步部署，请更新下面环境变量，注意替换API Gateway地址为你自己的API Gateway地址，这里更新是为了加上http://前缀针对这个问题，我建的0.9版本的
我先找了我创建的这个token
@younger   .Choerodon平台版本: 0.9.0遇到问题的执行步骤:
在H3 Cloud 私有云部署完K8S，猪齿鱼后，发现集群不稳定。经常某个节点cup会达到100%导致死机文档地址:http://v0-8.choerodon.io/zh/docs/installation-configuration/steps/kubernetes/
环境信息(如:节点信息):
node1 12核  64G
node2  8核   64G
node3  8核   64G
node4  8核   64G原因分析:
目前还没有找到具体的原因，目前只是怀疑这个私有云环境里网络和ansible脚本默认使用的网络类型vxlan不兼容。是否有什么办法可以追踪这个问题，或者查看那个日志可以看到cup?
如果是CPU饱和,你应该先查看那个应用占了高的CPUkubelet 这个应用占用的 cpu 特别高您指的100%是平均每个cpu的吗，如果直接死机，建议你先检查下硬件是否有问题。在相同的环境下，又另一家公司部署了一套大数据的平台，他们的应用都很稳定能否具体描述一下发生的问题，以及如何重现Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：在重启自己的微服务之后，出现启动不了的情况，之后重启了kafuka和zookeeper的镜像，然后重启ManagerService的时候出现了如下问题报错信息(请尽量使用代码块的形式展现)：
Network:org.springframework.beans.factory.BeanCreationNotAllowedException: Error creating bean with name ‘eurekaAutoServiceRegistration’: Singleton bean creation not allowed while singletons of this factory are in destruction (Do not request a bean from a BeanFactory in a destroy method implementation!)
at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:216) [spring-beans-4.3.18.RELEASE.jar:4.3.18.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:308) ~[spring-beans-4.3.18.RELEASE.jar:4.3.18.RELEASE]
at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) ~[spring-beans-4.3.18.RELEASE.jar:4.3.18.RELEASE]Error creating bean with name ‘eurekaAutoServiceRegistration’运行主类@EnableEurekaClient的注解去掉了吗？端口也没有被占用?谢谢 已经解决   是端口被占用问题现在我们准备切oracle 数据库，发现 猪齿鱼的liquibas工具jar包，不支持oracle，能不能发布一版oracle的工具包你好。有关oracle的支持我们已经在迭代中了，将于下一个版本推出能先发一个可用的测试版的工具包吗，我们现在用oracle数据库，挺急的工具包会优先于平台发布的。但是choerodon 自身的服务的改造相对会持续较长的时间。你们是只需要工具包来支持自己的服务还是包含choerodon服务在内的所有服务都支持oraclechoerodon所有服务都支持oracle，但现在初始化 manager_service和iam_service的表没法初始化choerodon支持oracle 是有一个明确的迭代计划的。如果等不及的话，建议先使用mysql作为choerodon的数据库，oracle作为自己服务的数据库，等到后面发布之后将数据迁移至oracle上即可。那现在有可用的oracle的liqubase工具包0.6.2的工具包会在本周内发布。liquibase的oracle工具包发布了吗有工具包地址吗？
是这个吗？
@wuyizhh @wenhao
iquibase 工具包：
https://oss.sonatype.org/content/groups/public/io/choerodon/choerodon-tool-liquibase/0.6.2.RELEASE/
mybatis-mapper 工具包：
https://oss.sonatype.org/content/groups/public/io/choerodon/choerodon-starter-mybatis-mapper/0.6.2.RELEASE/0.9版本安装！我想安装0.8版本，可是更新helm之后  c7n里面得版本都是9了，请问怎么自主选择版本如果你的仓库名为 c7n 那么可以执行以下命令查看所有版本号。安装指定版本请添加--version参数，比如安装agile-service的0.9.1版本命令如下：好的，明白了，谢谢，大神；Choerodon平台版本: 0.6.0遇到问题的执行步骤: 验证部署文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/dns/#自主搭建dns环境信息(如:节点信息):
[root@node1 ~]# kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
node1     Ready     master    6d        v1.8.5报错日志:
[root@node1 ~]# kubectl exec -n choerodon-devops-prod $(kubectl get po -n choerodon-devops-prod -l example.choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host example.choerodon.io
error: error executing jsonpath “{.items[0].metadata.name}”: array index out of bounds: index 0, length 0
Error from server (NotFound): pods “Error” not found独立抽取子语句执行后：
[root@node1 ~]# kubectl get po -n choerodon-devops-prod -l example.choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}"
Error executing template: array index out of bounds: index 0, length 0. Printing more information for debugging the template:
template was:
{.items[0].metadata.name}
object given to jsonpath engine was:
map[string]interface {}{“kind”:“List”, “apiVersion”:“v1”, “metadata”:map[string]interface {}{“selfLink”:"", “resourceVersion”:""}, “items”:[]interface {}{}}error: error executing jsonpath “{.items[0].metadata.name}”: array index out of bounds: index 0, length 0好像kube-dns.cm.yml里面的域名没有注册成功？  但是验证之前的全部语句都是执行成功的，没有报错。亲，若你设置的域名为example.choerodon.io  那么检查命令为：kubectl exec -n choerodon-devops-prod $(kubectl get po -n choerodon-devops-prod -l choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host example.choerodon.io试过这个命令了，因为报这个，才把前面那个域名换成example.choerodon.io试一试
[root@node1 ~]# kubectl exec -n choerodon-devops-prod $(kubectl get po -n choerodon-devops-prod -l choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host example.choerodon.io
;; connection timed out; no servers could be reached
command terminated with exit code 1请执行一下以下命令，提供一下返回结果kubectl get cm dnsmasq-cm -n choerodon-devops-prod -o yaml[root@node1 /]# kubectl get cm dnsmasq-cm -n choerodon-devops-prod -o yaml再执行下这个kubectl get svc dnsmasq -n choerodon-devops-prod -o yaml[root@node1 /]# kubectl get  svc dnsmasq -n choerodon-devops-prod -o yaml查看上述结果一切正常，再看看这个配置文件[root@node1 /]# kubectl get cm kube-dns -n kube-system -o yaml正确结果应该为：请执行以下命令修改正确我看了下，好像除了转义符之外，没啥区别？就是不能要转移符啊编辑后保存不行：
A copy of your changes has been stored to “/tmp/kubectl-edit-0133o.yaml”
error: Edit cancelled, no valid changes were saved.运行你的命令后就是编辑临时文件，有其他方法吗？那就先删除这个cm然后再按照官网文档创建这个cm，然后重启kube-dns就好哈kubectl get cm kube-dns -n kube-system -o yaml我删除了重做一遍，那个文件还是有转义符。我的kube-dns.cm.yml文件是跟你们教程的一模一样的，就改了IP地址。你好，请使用这个哈，注意内容最后不能有空格哈，官网的多了一个空格，我们将尽快修复，给你带来不便，敬请谅解。错误如下：[root@node1 ~]# kubectl get cm kube-dns -n kube-system -o yaml
apiVersion: v1
data:
stubDomains: |
{“example.choerodon.io”: [“172.16.24.141”]}
kind: ConfigMap
metadata:
annotations:
kubectl.kubernetes.io/last-applied-configuration: |
{“apiVersion”:“v1”,“data”:{“stubDomains”:"{“example.choerodon.io”: [“172.16.24.141”]}\n"},“kind”:“ConfigMap”,“metadata”:{“annotations”:{},“name”:“kube-dns”,“namespace”:“kube-system”}}
creationTimestamp: 2018-09-06T01:16:28Z
name: kube-dns
namespace: kube-system
resourceVersion: “816662”
selfLink: /api/v1/namespaces/kube-system/configmaps/kube-dns
uid: 7ae649b6-b172-11e8-99f7-005056bf6ceb删除了空格之后是没有了转义符，但是下面那个还是有转义符。  用kubectl exec -n choerodon-devops-prod $(kubectl get po -n choerodon-devops-prod -l choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host example.choerodon.io  命令会报错：[root@node1 ~]# kubectl exec -n choerodon-devops-prod $(kubectl get po -n choerodon-devops-prod -l choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host example.choerodon.io
;; connection timed out; no servers could be reached
command terminated with exit code 1下面那个还是有转义符是正常的请问修改了这个configmap之后你重启kube-dns了吗?需要重启kube-dns之后再执行验证命令修改文件后，执行顺序如下：
kubectl apply -f kube-dns.cm.yml
kubectl scale deployment kube-dns -n kube-system --replicas=0
kubectl scale deployment kube-dns -n kube-system --replicas=1
kubectl exec -n choerodon-devops-prod $(kubectl get po -n choerodon-devops-prod -l choerodon.io/infra=dnsmasq -o jsonpath="{.items[0].metadata.name}") host example.choerodon.io重做了一遍，还是一样的问题，能支持一下吗？Choerodon平台版本: PaaS平台遇到问题的执行步骤:环境初始化执行脚本失败， 如下图文档地址:环境信息(如:节点信息):报错日志:
Error: failed to download “http://chart.choerodon.com.cn/choerodon/c7ncd/charts/choerodon-agent-0.9.7.tgz”原因分析:这个压缩包不存在@vinkdong你好，你是用的自己搭建的平台吗？你需要先执行下面条命令 @coder-zhw @crockitwood嗯， 感谢，我root用户下面执行就没有问题在开发中，自己的项目中会用到一个私有的maven仓库，请问，怎么把这个仓库配置到猪齿鱼上1、本地修改maven 的setting配置
2、服务器gitlab 修改 runner 的maven setting配置Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：自主搭建问题描述：
部署应用后，实例状态一直处于 处理中，并且容器状态也是0。
再次追踪agent日志 发现之前报错已经没有了这个文件夹的777权限是您手动添加的吗？是的 ，当时把 nfs开放得路径全加上 777了 ，被自己坑了manager-service升级到0.8.0.RELEASE后，发现swagger页面相关API(SwaggerController、ServiceController…)都已经去掉了，请问替代的方案是什么？manager-service我们manager-service的0.8.0.RELEASE版本还没有去掉swagger，你拉的代码是master分支吗？请切到tag   0.8.0。我们框架也在实现API测试功能，类似于swagger，后续版本发布直接拉的master最新的代码。API测试功能是在独立出来的test-manager-service吗？后续集成到猪齿鱼测试平台上？Choerodon平台版本：0.9.0运行环境：一键安装问题描述：执行的操作：
活动冲刺—看板报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作打开该看板的配置：
好的。看到了。谢谢Choerodon平台版本: 0.9.0遇到问题的执行步骤:
公司是金融机构，网络管控严格，如何才能离线安装？
能否出一个离线安装的步骤感谢您的关注，我们正在测试和验证离线环境，验证通过之后会及时的更新文档，同时也会在论坛回复您。在这期间您可以尝试安装和使用非离线环境以熟悉我们的平台。好的 万众期待Choerodon平台版本：0.9.0运行环境：https://choerodon.com.cn个人理解：参数的配置是如何从部署流水线中的实例的配置文件传递到docker容器，下面是我个人的理解，不知道是否准确疑问实例部署的时候，配置信息， 不完全和values.yaml里面一致，会多出一个env的配置和deployment一致，这两个各自有什么用处？有什么区别？
values.yaml里面的配置项是如何和springboot的配置映射的？比如deployment.SPRING_DATA_URL对应springboot里面的数据url的配置，这些是c7n的约定吗？values.yaml 为默认的配置， 实例部署时应该是您的真实值SPRING_DATASOUCE_URL是springboot配置数据库指定的环境变量。你的理解是正确的，然后你问题回答是：１,values.yaml是用来渲染chart目录下面各个对象，然后里面的值都是默认值，根据部署环境的不同，实际的参数值可能会不一样，然后values.yaml 里面的envs就是对应服务的配置文件里面的参数。比如spring-boot里面的application.yaml文件里面的参数,然后deployment是用来渲染chart目录下 deployment模板里面的值，服务端口和管理端口２,values.yaml里面的env下面的配置项在容器启动的时候。会作为环境变量替换容器运行时的参数。
比如env.open.SPRING_DATA_URL 的值会替换掉spirng-boot服务启动时的application.yaml文件中的SPRING_DATA_URL这个参数值,作用类似于感谢您的回复~~我这样理解您的回复不知道对不对，deployment和env其实都是用来渲染helm的chart，具体取什么值，其实还是看chart模板里面怎么写，只是一些相对固定的配置项会放在deployment里面，和环境相关的会放在env里面；env.open.SPRING_DATA_URL这些参数替换spring-boot的application.yaml是spring-boot的特性，还是在编译的基础容器（registry.cn-hangzhou.aliyuncs.com/choerodon-tools/javabase:0.5.0） 里面处理的？有没有说明文档？比如可以配置哪些参数， 参数映射？Choerodon平台版本：0.9.0运行环境：一键安装问题描述：环境已经添加，部署报错执行的操作：
部署应用报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作请问choerodon平台是新安装的0.9.0还是从0.8升级上去的？是新一键安装的0.9.0版本。是新一键安装的0.9.0版本。在环境总览页点击配置库，看看是能进去git配置库。
用管理员用户点击管理，点击全局事务，点击里面的事务实例,然后找到所属事务定义编码为create-env的记录，看状态是否是已完成，如果失败，里面有失败的报错原因8月27号晚上创建的，那是在修复devops-service gateway环境变量值之前创建的，创环境是类似于应用也会有创建webhook的操作，所以失败了，你现在在这个项目下重新创建一个环境应该就可以了。创建完之后可以通过上述说的实例事务哪里确认,事务完成就表示创建成功了, 然后用新环境部署就可以了额，是不是每次都要在创建项目之后在创建环境？对 环境只能在项目下创建额，那之前创建的 可以删了？在另外创建一个试一下环境可以禁用掉，不可以删除，你可以吧之前的环境禁用掉。可以找到环境了，但部署还是报错：
choerodon-devops-service服务报错：
gitlab配置库又创建了：
choerodon-agent客户端日志：
麻烦帮忙看看。。。控制台一直在打转：
agent运行正常：
harbor 是可以连接的：
按照报错信息,可能是克隆环境库的时候没有克隆成功可以先进入集群内devops-service所在的namespace,执行下图操作然后路径为
查看该目录下有没有文件namespacechoerodon-devops-service 容器里 没有ymal 文件。root@i-wz9j8qbz9c9gmmgw1zjq:/jmsw# kubectl exec -it choerodon-devops-service-b956f5765-mswft -n jmsw-devops bash
bash-4.3#
bash-4.3#
bash-4.3#
bash-4.3# ls
Charts               dev                  etc                  lib                  proc                 sbin                 tmp
bin                  devops-service-repo  gitops               media                root                 srv                  usr
deployfile           devops-service.jar   home                 mnt                  run                  sys                  var
bash-4.3# cd gitops/
bash-4.3# ls
operation
bash-4.3# cd operation/
bash-4.3# ls
land-test
bash-4.3# cd land-test/
bash-4.3# ls
jmsw-dev   jmsw-test
bash-4.3# cd jmsw-test/
bash-4.3# ls
bash-4.3# ls
bash-4.3# pwd
/gitops/operation/land-test/jmsw-test
bash-4.3# cd …
bash-4.3# ls
jmsw-dev   jmsw-test
bash-4.3# ll jmsw-dev/
bash: ll: command not found
bash-4.3# ll jmsw-test/
bash: ll: command not found
bash-4.3# ls jmsw-dev/
bash-4.3# ls jmsw-test/
bash-4.3#如果目录下没有文件，请确认一键部署的时候按照这个文档执行http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/#启用SSH协议ssh 添加了：
root@i-wz9ik7jd1mwbfygjq7bs:~# netstat -ntlp
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      27939/kubelet
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      25871/kube-proxy
tcp        0      0 0.0.0.0:33322           0.0.0.0:*               LISTEN      14583/sshd
tcp        0      0 127.0.0.1:6443          0.0.0.0:*               LISTEN      27444/nginx: master
tcp        0      0 192.168.1.103:22        0.0.0.0:*               LISTEN      25871/kube-proxyDNS这一部不了解啥意思：
是要解析 33322 端口还是 22 端口？Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：猪齿鱼微服务框架有定时任务的功能吗？任务计划功能正在目前的迭代中实现，在下一个版本中会发布。已经解决 不用回复查看下pod的信息不能连接到你的NFS服务，确定nfs服务能够正常使用吗?我的nfs服务器是在k8s-master主机上面搭建的，这个应该不会有影响吧，我的nfs服务通过k8s-node1等节点访问可正常的修改那个/etc/hosts文件是所有节点都要修改的哈。请在k8s-master执行cat /etc/hosts粘贴下结果。这是内存不够了哦为什么搭建好chartmuseum出现了文档说的那个页面，但是访问api 的时候是错误的


也无法添加仓库[root@node1 charts]# helm repo add test1 http://charts.dev.hand-ams.com
Error: Looks like “http://charts.dev.hand-ams.com” is not a valid chart repository or cannot be reached: Failed to fetch http://charts.dev.hand-ams.com/index.yaml : 404 Not Found请注意你这里设置租户层级，若你直接访问域名有文档所述页面那么说明你是搭建成功的。chartmuseum的使用方式请参考官网



GitHub



helm/chartmuseum
Helm Chart Repository with support for Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage, Alibaba Cloud OSS Storage, and Openstack Object Storage - helm/chartmuseum





Helm Chart Repository with support for Amazon S3, Google Cloud Storage, Microsoft Azure Blob Storage, Alibaba Cloud OSS Storage, and Openstack Object Storage - helm/chartmuseum如果想将chart文件存放在自己的chartmuseum仓库，需要怎么办？是不是只要修改这个地址

这个token是在gitlab 的CI/CD变量里配置的?，请问这是什么地方的tokenchartmuseum 需要与devops-service共享存储目录，你不能随意更改。如果需要修改，请在devops-service中修改相应的配置。Choerodon平台版本：0.9.5运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：C7NRelease.yaml已安装
问题描述：部署应用之后，状态运行中，pod数为0，在k8s里面也未执行部署
疑问：何种原因会导致这个情况？
如何查看choerodon的部署日志？在k8s里面也未执行部署指的是helm get不到这个实例吗，这边不是有个nginx-demo部署成功了吗部署日志暂时只能去看环境客户端pod日志helm ls 看不到部署的chart包
这种情况， 要如何排查错误？上面那个不存在的实例，不就是因为这在执行crd之前部署的吗，后面的能正常使用了吧。执行crd之后部署的，那个nginx能正常部署未能正常部署的要如何解决？您好，编辑实例，点击修改配置信息，重新部署！重新部署，一直在处理中那可以进入配置库，找到实例对应的文件,修改实例文件，新增空行或者在values参数里面随便新增变量都行，然后提交，如图所示:做完这些操作呢？提交之后，界面上实例就会重新部署了，其实界面上操作对象的实质就是操作这个环境库里面的对象文件我离线部署的时候 也遇到了相同的情况 ，但根据你之前的操作修改文件没有任何反应Choerodon平台版本: 0.9.0遇到问题的执行步骤:
发现网络不稳定，ping 集群里的各个节点 ，延迟在 <1ms ~ 100ms 波动。
但重启完某个节点后，网络就稳定了 保持在 <1ms环境信息(如:节点信息):
H3cloud 私有云， 内网环境 ，隔离外网。
离线部署完 整套K8S 猪齿鱼平台 和监控 。
4节点 8核 32G而且发现 使用 kubectl logs 去追踪日志时 ，会卡死 ，但我用 docker logs 去追踪通一个容器时 就没有问题，我怀疑 是 k8s api 或者 k8s 网络层 存在问题服务器间的延迟主要由外部的网络设备决定，建议您联系您的服务器提供商。Choerodon平台版本：0.6.0
0.9运行环境：自主搭建问题描述：
Exception handling request to /agent/
org.springframework.web.util.NestedServletException: Request processing failed; nested exception is org.springframework.web.socket.server.HandshakeFailureException: Uncaught failure for request http://devops.service.wxc.pakchoi.top/agent/?version=0.9.5&envId=1&key=env:choerodon-dev.envId:1; nested exception is java.lang.RuntimeException: already have a agent in this env
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:982)
at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:111)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:208)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:64)
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132)
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:336)
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.web.socket.server.HandshakeFailureException: Uncaught failure for request http://devops.service.wxc.pakchoi.top/agent/?version=0.9.5&envId=1&key=env:choerodon-dev.envId:1; nested exception is java.lang.RuntimeException: already have a agent in this env
at org.springframework.web.socket.server.support.WebSocketHttpRequestHandler.handleRequest(WebSocketHttpRequestHandler.java:174)
at org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter.handle(HttpRequestHandlerAdapter.java:51)
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
… 75 common frames omitted
Caused by: java.lang.RuntimeException: already have a agent in this env
at io.choerodon.websocket.security.AgentSecurityInterceptor.check(AgentSecurityInterceptor.java:60)
at io.choerodon.websocket.security.SecurityCheckManager.check(SecurityCheckManager.java:29)
at io.choerodon.websocket.websocket.RequestParametersInterceptor.beforeHandshake(RequestParametersInterceptor.java:51)
at org.springframework.web.socket.server.support.HandshakeInterceptorChain.applyBeforeHandshake(HandshakeInterceptorChain.java:59)
at org.springframework.web.socket.server.support.WebSocketHttpRequestHandler.handleRequest(WebSocketHttpRequestHandler.java:163)
… 79 common frames omitted请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：使用了一下迭代管理，到冲刺发布以后就结束了。 之前还想着是需求和feature相关之类的。迭代发布会和release分支关联在一起执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作Choerodon平台版本：0.9.0运行环境：一键安装问题描述：我有一次更新出错了，如何恢复上一次的更新呢？执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作请问你是使用什么方式进行更新的？部署流水线 方式啊
Choerodon平台版本: 0.9.0遇到问题的执行步骤:
登录验证报错日志:
前台一直转圈。一片白屏
没有找到后台具体报错日志。在浏览器控制台禁用缓存 刷新并查看下网络请求Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：持续集成CI在mvn-package阶段报错报错信息(请尽量使用代码块或系统截图的形式展现)：@vinkdong @TimeBye 两位帮忙看下， 是不是pass平台资源不够？自主搭建的Choerodon 请检查你们Runner机器的资源哦@vinkdong choerodon是公司的pass， k8s是自己搭建的可能是短时间内有多个项目要执行CI，你可以稍等一会再重试嗯， 感谢， 现在好像好了Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
外围系统调用swagger接口，怎么认证？你好，swagger是用作测试接口的，请问你是需要外围系统调用猪齿鱼的接口，还是说在swagger上进行测试？需要外围系统调用猪齿鱼的接口可以在平台中的某一组织下创建一个client，然后可以通过接口调用获取token。http://api.example.com/oauth/oauth/token?client_id=client&client_secret=secret&grant_type=password&password=&username=其中password要遵循一定的加密规则这个加密方式有点儿尬，能换成接口形式吗，后面自己实现可以直接使用BCryptPasswordEncoder 作为加密规则是但是外围系统这样搞不太好，外围系统调的时候这个需求我们收集下，后续会提供单独的接口供外围系统调用Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：独立离线环境，不连接外网问题描述：
手动将github上的模板复制放到自定义模板，之后使用该自定义模板生成应用，gitlab中该应用已经生成成功，但里面都是空的 。报错信息(请尽量使用代码块的形式展现)：
查看saga报错日志 有错误信息 io.choerodon.devops.infra.common.util.GitUil.clone( Gi( e( GitUtil.java 190)发现gitlab-service也有报错 报错是 io.choerodon.gitlab.app.service.impl.ProjectServiceImpl.createProject(ProjectServiceImpl .java 40)
Bad Request这是我们测试过程中的失误。
可以在对应模板的gitlab库中手动将代码库设置为公开。
已在0.9.4修复，之前的错误模板需要手动设置为公开Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：我想问下这里的域名地址需要怎么配置呢，我是本地windows虚拟机，按照手册安装了k8s和helm,其他的没有进行安装，是需要自主安装dns服务器吗，是dns服务器里面的那个域名吗，如果是那么下图二的192.168.1.1代表的是我windows本机的ip还是三个node节点中的一个节点的ip呢，麻烦了你好，请问你有在域名运营商处购买域名吗？没有额，不是可以自主搭建dns吗看着一章节，http://choerodon.io/zh/docs/installation-configuration/steps/dns/问： 创建域名时填写的域名地址是从何而来呢
答：楼主没有在域名运营商处购买域名，那么在自主搭建dns时配置的域名地址就是你“创建域名时填写的域名地址”，比如你配置的是test.local，你在“创建域名时填写的域名地址”的时候填写的域名应该是test.local的下一级域名，比如api.test.local。问：图二的192.168.1.1代表的是我windows本机的ip还是三个node节点中的一个节点的ip呢？
答：三个node节点中的任意一个节点的ip小哥哥我弄了个域名，这里域名解析填写的ip应该是哪个ip地址呢，辛苦了辛苦了：
你好，你的域名解析已经通过了。请问一下你的域名对应到的是什么服务。api-gateway吗？还是说自己的服务。如果是自己的服务，请确保一下/v1/demo/hello 这个接口是存在的小哥哥我这个是视频持续交付里边简单的spring boot 项目，是存在的，：Hi, 你的DNS好像配置有问题哦。你的虚拟机ip是221.130.253.135?你好，你的DNS应该是配置有问题的。wuyihuihuang.xin 解析到的ip不是221.130.253.135说出来别笑我，我就是不知道应该指定解析的ip应该是这里面的哪一个:joy:本地ping一下域名。显示的ip就是你域名的ip域名的ip我知道额，可是这里解析不是要添加我虚拟机的IP吗噢， 是这样子的，我是在本地搭的虚拟机不是在这个域名服务器上搭的虚拟机你直接在电脑上访问  192.168.56.11 这个ip 试试你就把域名解析到这个ip就可以了.我试过了额，不行，还可能有其他原因吗
改了DNS要过一会才会生效的，你需要耐心等待一会哦，
在电脑ping一下这个地址看DNS是已经更新了你是否修改了本机的DNS配置呢， 如果使用其他机器也时是同样的结果建议你联系你的DNS服务商哦还是不行额，能ping通了
ChoerodonAuthenticationProvider里面的获取凭证处PasswordDecode.decode里有个数组越界。
oauth-server版本：0.7.0.RELEASE说实话，不太理解这个地方去解码的意思，原谅我。然后又没有注释。。。
获取到凭证不就是明文了么，再去加密匹配不就行了么？
你好，感谢你的建议，代码的不规范我们在持续优化中。关于后端解密的原因，是因为我们的部分用户需要进行ldap校验。后续我们会改进登录的模式和加密方式。好的，感谢答疑。现在麻烦的是，现在oauth-server还不能用你好，请问一下不能用是指？认证的时候输密码，就500了是因为从0.6.0 升级到0.7.0以后引起的？可以清空下浏览器缓存，或者换一个浏览器重新登录下试试清缓OK了大神，我今天在做/oauth/token接口的时候，又出现这个主题的错误了哦，用postman。出现问题是因为我密码传了明文admin，没有传加密内容。然后我们就报数组越界了不知道注意没有，CustomUserDetails继承User之后，创建CustomUserDetails的时候，User的toString执行了，报了个空指针，debug看下就知道了，这个问题可能要看下，虽然不影响后面的认证。。。。还是去实现UserDetails吧。。。。感谢提醒，会及时排查这个问题这个问题现在的版本还是有问题的哦，还是空指针？Choerodon平台版本: 0.6.0遇到问题的执行步骤:
文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/harbor/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
我k8s集群安装之后，没有 kube-lego  然后我想手动申请证书：kubectl create secret tls harbor-cert --key privkey.pem --cert fullchain.pem -n harbor
[root@node1 letsencrypt]# ls
accounts  renewal  renewal-hooks
运行docker之后，有以下三个，文件，所以这里的证书私钥的路径，和证书的路径从哪里获取 --key privkey.pem --cert fullchain.pem提出您对于遇到和解决该问题时的疑问你要申请证书还是导入已有证书？猪齿鱼中的手动申请证书，第一步已经生产证书，现在是创建这一步的，这两个证书路径和，证书私钥的路径不清楚在哪，，我
kubectl create secret tls harbor-cert --key privkey.pem --cert fullchain.pem -n harbor申请过程中有报错吗？申请成功会有如下提示申请过程中有报错吗？申请成功会有如下提示是申请证书的过程 也就是文档中 docker run的那一步Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：服务器启动iam-service服务以及必要的7个服务，本机启动choerodon-front-iam问题描述：启动前端应用，登录完成后显示一直在加载中，浏览器检查结果: 在访问服务器中iam服务的资源时返回Invalid CORS request修改的数据：
choerodon-front-iam 中的config.js文件: 仅改动了server报错信息(请尽量使用代码块的形式展现)：Failed to load
http://192.168.xx.xx:8030/iam/v1/users/self:
Response to preflight request doesn’t pass access control check:
No Access-Control-Allow-Origin header is present on the requested resource.
Origin http://localhost:9090 is therefore not allowed access.疑问：在api-gateway的源码中是有对跨域访问做设置的，但是为什么仍未生效呢？前端server地址配错了，server地址应该是gateway的ip，端口是8080，你这样不经过api-gateway，跨域设置肯定不生效想了一下，确实是这样的问题。感谢，localhost启动choerodon-front-iam没有问题了。
但是问题再延伸一下，在另一台服务器启动choerodon-front-iam时，浏览器检查出现如下错误：
其中config.js的配置如下：不清楚是不是clientId配置错误服务器部署我们建议用npm run build来构建代码，然后部署在nginx上，而不是用npm start， 因为start是开发者模式，会显示一些警告错误信息，其中sockjs-node之类的错误是webpack-dev-server报的错，主要作用是监听文件的变化时热构建js代码然后刷新页面用的。Got it, thanks Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：详细描述
不好意思哈，不知道有没有人发现gateway-helper的0.7.0.RELEASE的代码有问题呢，怎么只有字节码呢，源码呢？如果我没看错的话。辛苦大佬帮忙重新发下哈，急用，感激不尽。

·pom.xml·
你好，gateway-helper是一个服务，而不是作为maven依赖的。我们并没有对外发布gateway-helper 的依赖包。如果需要作为依赖可以通过源码打包的方式到maven私服。



GitHub



choerodon/gateway-helper
Authenticating and limiting the requests from api-gateway, create JWT and return to api-gateway. - choerodon/gateway-helper





Authenticating and limiting the requests from api-gateway, create JWT and return to api-gateway. - choerodon/gateway-helper这样的啊，哈哈，尴尬。那iam-service也没有对外发布吗？对于服务的源码都可以在github 上找到。而服务对应的镜像则可以直接在dockerhub搜索到因为啥呢，我看到这个
你的pom中有直接对iam-service的依赖？没错的，所有我会有这个疑问。gateway-helper依赖出来的，有BOOT-INF了，这个不知道是哪里不对了哦
能方便提供一下依赖的下载链接吗。或者直接通过源码编译？嗯嗯，那没事啦哈。基本明白咋回事了。多谢。iam-service是有人传了私服，麻烦啦哈本地demo可以参考下



GitHub



wmzzh117/choerodon-todo-service
Contribute to wmzzh117/choerodon-todo-service development by creating an account on GitHub.





Contribute to wmzzh117/choerodon-todo-service development by creating an account on GitHub.多谢Choerodon平台版本: 0.8.0遇到问题的执行步骤: 部署SonarQube文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/sonarqube/环境信息(如:节点信息):k8s报错日志:
at org.picocontainer.injectors.AbstractInjectionFactory$LifecycleAdapter.start(AbstractInjectionFactory.java:84)at org.picocontainer.behaviors.AbstractBehavior.start(AbstractBehavior.java:169)at org.picocontainer.behaviors.Stored$RealComponentLifecycle.start(Stored.java:132)at org.picocontainer.behaviors.Stored.start(Stored.java:110)at org.picocontainer.DefaultPicoContainer.potentiallyStartAdapter(DefaultPicoContainer.java:1016)at org.picocontainer.DefaultPicoContainer.startAdapters(DefaultPicoContainer.java:1009)at org.picocontainer.DefaultPicoContainer.start(DefaultPicoContainer.java:767)at org.sonar.core.platform.ComponentContainer.startComponents(ComponentContainer.java:134)at org.sonar.ce.container.ComputeEngineContainerImpl.start(ComputeEngineContainerImpl.java:191)at org.sonar.ce.ComputeEngineImpl.startup(ComputeEngineImpl.java:45)at org.sonar.ce.app.CeServer$CeMainThread.startup(CeServer.java:167)at org.sonar.ce.app.CeServer$CeMainThread.attemptStartup(CeServer.java:154)at org.sonar.ce.app.CeServer$CeMainThread.run(CeServer.java:141)Caused by: java.nio.channels.ClosedByInterruptException: nullat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)at sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:315)at org.apache.commons.io.FileUtils.doCopyFile(FileUtils.java:1142)at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:1091)at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:1038)at org.sonar.ce.container.CePluginJarExploder.explode(CePluginJarExploder.java:52)… 22 common frames omitted2018.08.24 11:23:30 INFO app[][o.s.a.SchedulerImpl] Process [ce] is stopped2018.08.24 11:23:30 INFO web[][o.s.p.StopWatcher] Stopping process2018.08.24 11:23:33 INFO app[][o.s.a.SchedulerImpl] Process [web] is stopped2018.08.24 11:23:33 INFO app[][o.s.a.SchedulerImpl] SonarQube is stopped能否粘贴更完整的日志2018.08.25 02:03:26 INFO  app[][o.s.a.AppFileSystem] Cleaning or creating temp directory /opt/sonarqube/temp
2018.08.25 02:03:26 INFO  app[][o.s.a.es.EsSettings] Elasticsearch listening on /127.0.0.1:9001
2018.08.25 02:03:26 INFO  app[][o.s.a.p.ProcessLauncherImpl] Launch process[[key=‘es’, ipcIndex=1, logFilenamePrefix=es]] from [/opt/sonarqube/elasticsearch]: /opt/sonarqube/elasticsearch/bin/elasticsearch -Epath.conf=/opt/sonarqube/temp/conf/es
2018.08.25 02:03:26 INFO  app[][o.s.a.SchedulerImpl] Waiting for Elasticsearch to be up and running
2018.08.25 02:03:26 INFO  app[][o.e.p.PluginsService] no modules loaded
2018.08.25 02:03:26 INFO  app[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.transport.Netty4Plugin]
2018.08.25 02:03:39 INFO  app[][o.s.a.SchedulerImpl] Process[es] is up
2018.08.25 02:03:39 INFO  app[][o.s.a.p.ProcessLauncherImpl] Launch process[[key=‘web’, ipcIndex=2, logFilenamePrefix=web]] from [/opt/sonarqube]: /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djava.io.tmpdir=/opt/sonarqube/temp -Xmx512m -Xms128m -XX:+HeapDumpOnOutOfMemoryError -Djava.security.egd=file:/dev/./urandom -cp ./lib/common/*:/opt/sonarqube/lib/jdbc/postgresql/postgresql-42.2.1.jar org.sonar.server.app.WebServer /opt/sonarqube/temp/sq-process5200055914547530503properties
2018.08.25 02:03:39 INFO  web[][o.s.p.ProcessEntryPoint] Starting web
2018.08.25 02:03:40 INFO  web[][o.a.t.u.n.NioSelectorPool] Using a shared selector for servlet write/read
2018.08.25 02:03:41 INFO  web[][o.e.p.PluginsService] no modules loaded
2018.08.25 02:03:41 INFO  web[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
2018.08.25 02:03:41 INFO  web[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
2018.08.25 02:03:41 INFO  web[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.transport.Netty4Plugin]
2018.08.25 02:03:42 INFO  web[][o.s.s.e.EsClientProvider] Connected to local Elasticsearch: [127.0.0.1:9001]
2018.08.25 02:03:42 INFO  web[][o.s.s.p.LogServerVersion] SonarQube Server / 7.1.0.11001 / 9f47ce9daecebb16fc777249a418252625ae774a
2018.08.25 02:03:42 INFO  web[][o.sonar.db.Database] Create JDBC data source for jdbc:postgresql://sonarqube-postgresql/sonar
2018.08.25 02:03:43 INFO  web[][o.s.s.p.ServerFileSystemImpl] SonarQube home: /opt/sonarqube
2018.08.25 02:03:43 INFO  web[][o.s.s.u.SystemPasscodeImpl] System authentication by passcode is disabled
2018.08.25 02:03:44 INFO  web[][o.s.s.p.ServerPluginRepository] Deploy plugin Git / 1.4.0.1037 / 05703ae67364e0cc41c6d49f495e0e6977abfb52
2018.08.25 02:03:44 INFO  web[][o.s.s.p.ServerPluginRepository] Deploy plugin SonarC# / 7.0.1.4822 / 6d8cd4ef8c80476eba86efa4d0a1a9416065c304
2018.08.25 02:03:44 INFO  web[][o.s.s.p.ServerPluginRepository] Deploy plugin SonarFlex / 2.4.0.1222 / 68d9cb3b7daccbc4869c4f2b89d09218d26a0829
2018.08.25 02:03:44 INFO  web[][o.s.s.p.ServerPluginRepository] Deploy plugin SonarJS / 4.1.0.6085 / 0a33c1a749126986ce73dd749f64f14ff4c91e33
2018.08.25 02:03:44 INFO  web[][o.s.s.p.ServerPluginRepository] Deploy plugin SonarJava / 5.2.0.13398 / 3e712400303a4017c13b87b7194d3d630c8ff6f3
2018.08.25 02:03:44 INFO  web[][o.s.s.p.ServerPluginRepository] Deploy plugin SonarPHP / 2.13.0.3107 / 9592e5feedc752eddc8f4c15763e80d2bd07ade7
at org.picocontainer.injectors.AbstractInjectionFactory$LifecycleAdapter.start(AbstractInjectionFactory.java:84)at org.picocontainer.behaviors.AbstractBehavior.start(AbstractBehavior.java:169)at org.picocontainer.behaviors.Stored$RealComponentLifecycle.start(Stored.java:132)at org.picocontainer.behaviors.Stored.start(Stored.java:110)at org.picocontainer.DefaultPicoContainer.potentiallyStartAdapter(DefaultPicoContainer.java:1016)at org.picocontainer.DefaultPicoContainer.startAdapters(DefaultPicoContainer.java:1009)at org.picocontainer.DefaultPicoContainer.start(DefaultPicoContainer.java:767)at org.sonar.core.platform.ComponentContainer.startComponents(ComponentContainer.java:134)at org.sonar.ce.container.ComputeEngineContainerImpl.start(ComputeEngineContainerImpl.java:191)at org.sonar.ce.ComputeEngineImpl.startup(ComputeEngineImpl.java:45)at org.sonar.ce.app.CeServer$CeMainThread.startup(CeServer.java:167)at org.sonar.ce.app.CeServer$CeMainThread.attemptStartup(CeServer.java:154)at org.sonar.ce.app.CeServer$CeMainThread.run(CeServer.java:141)Caused by: java.nio.channels.ClosedByInterruptException: nullat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)at sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:315)at org.apache.commons.io.FileUtils.doCopyFile(FileUtils.java:1142)at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:1091)at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:1038)at org.sonar.ce.container.CePluginJarExploder.explode(CePluginJarExploder.java:52)… 22 common frames omitted2018.08.25 02:05:00 INFO app[][o.s.a.SchedulerImpl] Process [ce] is stopped2018.08.25 02:05:00 INFO web[][o.s.p.StopWatcher] Stopping process2018.08.25 02:05:03 INFO app[][o.s.a.SchedulerImpl] Process [web] is stopped2018.08.25 02:05:03 INFO app[][o.s.a.SchedulerImpl] SonarQube is stopped容器一直在重启！！我们无法重现这个问题，建议你删除 sonar和 相关数据 重新安装。数据啥的都删过了，还是一样的问题。
2018.08.25 03:35:38 INFO  app[][o.s.a.SchedulerImpl] Process[web] is up
2018.08.25 03:35:38 INFO  app[][o.s.a.p.ProcessLauncherImpl] Launch process[[key=‘ce’, ipcIndex=3, logFilenamePrefix=ce]] from [/opt/sonarqube]: /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djava.io.tmpdir=/opt/sonarqube/temp -Xmx512m -Xms128m -XX:+HeapDumpOnOutOfMemoryError -cp ./lib/common/*:/opt/sonarqube/lib/jdbc/postgresql/postgresql-42.2.1.jar org.sonar.ce.app.CeServer /opt/sonarqube/temp/sq-process550893160093569493properties
2018.08.25 03:35:41 WARN  app[][o.s.a.p.AbstractProcessMonitor] Process exited with exit value [es]: 137
2018.08.25 03:35:41 INFO  app[][o.s.a.SchedulerImpl] Process [es] is stopped
2018.08.25 03:35:43 INFO  app[][o.s.a.SchedulerImpl] Process [ce] is stopped
2018.08.25 03:35:46 INFO  app[][o.s.a.SchedulerImpl] Process [web] is stopped
2018.08.25 03:35:46 INFO  app[][o.s.a.SchedulerImpl] SonarQube is stopped
cannot exec in a stopped state: unknown你是否是按照我们的教程搭建的kubernetes集群？这个跟K8S 有关系？报错明显是镜像的问题啊这个镜像没有问题，更像网络或者存储有问题。我们没法重新这个问题问题是，别的基础组件都可以，就这个运行一段时间就报错了。。然后就挂了。你可以选择一台主机执行看是否有同样的报错2018.08.25 04:34:43 INFO  ce[][o.s.p.ProcessEntryPoint] Starting ce
2018.08.25 04:34:43 INFO  ce[][o.s.ce.app.CeServer] Compute Engine starting up…
2018.08.25 04:34:44 INFO  ce[][o.e.p.PluginsService] no modules loaded
2018.08.25 04:34:44 INFO  ce[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.join.ParentJoinPlugin]
2018.08.25 04:34:44 INFO  ce[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]
2018.08.25 04:34:44 INFO  ce[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.transport.Netty4Plugin]
2018.08.25 04:34:45 INFO  ce[][o.s.s.e.EsClientProvider] Connected to local Elasticsearch: [127.0.0.1:9001]
2018.08.25 04:34:45 INFO  ce[][o.sonar.db.Database] Create JDBC data source for jdbc:h2:tcp://127.0.0.1:9092/sonar
2018.08.25 04:34:45 WARN  ce[][o.s.d.DatabaseChecker] H2 database should be used for evaluation purpose only
2018.08.25 04:34:47 INFO  ce[][o.s.s.p.ServerFileSystemImpl] SonarQube home: /opt/sonarqube
2018.08.25 04:34:47 INFO  ce[][o.s.c.c.CePluginRepository] Load plugins
2018.08.25 04:34:48 INFO  ce[][o.s.c.q.PurgeCeActivities] Delete the Compute Engine tasks created before 1519619688595
2018.08.25 04:34:48 INFO  ce[][o.s.c.q.PurgeCeActivities] Delete the Scanner contexts tasks created before 1532752488603
2018.08.25 04:34:48 INFO  ce[][o.s.ce.app.CeServer] Compute Engine is operational
2018.08.25 04:34:48 INFO  app[][o.s.a.SchedulerImpl] Process[ce] is up
2018.08.25 04:34:48 INFO  app[][o.s.a.SchedulerImpl] SonarQube is up这样执行确实不报错了，K8S里面执行有问题，这就尴尬了2018.08.25 18:53:22 INFO ce[][o.s.p.ProcessEntryPoint] Starting ce2018.08.25 18:53:22 INFO ce[][o.s.ce.app.CeServer] Compute Engine starting up…2018.08.25 18:53:22 INFO ce[][o.e.p.PluginsService] no modules loaded2018.08.25 18:53:22 INFO ce[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.join.ParentJoinPlugin]2018.08.25 18:53:22 INFO ce[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.percolator.PercolatorPlugin]2018.08.25 18:53:22 INFO ce[][o.e.p.PluginsService] loaded plugin [org.elasticsearch.transport.Netty4Plugin]2018.08.25 18:53:24 INFO ce[][o.s.s.e.EsClientProvider] Connected to local Elasticsearch: [127.0.0.1:9001]2018.08.25 18:53:24 INFO ce[][o.sonar.db.Database] Create JDBC data source for jdbc:postgresql://sonarqube-postgresql/sonar2018.08.25 18:53:25 WARN app[][o.s.a.p.AbstractProcessMonitor] Process exited with exit value [es]: 1372018.08.25 18:53:25 INFO app[][o.s.a.SchedulerImpl] Process [es] is stopped2018.08.25 18:53:25 INFO ce[][o.s.p.StopWatcher] Stopping process2018.08.25 18:53:26 INFO ce[][o.s.s.p.ServerFileSystemImpl] SonarQube home: /opt/sonarqube2018.08.25 18:53:26 INFO ce[][o.s.c.c.CePluginRepository] Load plugins2018.08.25 18:53:26 ERROR ce[][o.s.ce.app.CeServer] Compute Engine startup failedjava.lang.IllegalStateException: Fail to unzip plugin [python] /opt/sonarqube/extensions/plugins/sonar-python-plugin-1.9.1.2080.jar to /opt/sonarqube/temp/ce-exploded-plugins/pythonat org.sonar.ce.container.CePluginJarExploder.explode(CePluginJarExploder.java:56)at org.sonar.core.platform.PluginLoader.defineClassloaders(PluginLoader.java:92)at org.sonar.core.platform.PluginLoader.load(PluginLoader.java:72)at org.sonar.ce.container.CePluginRepository.start(CePluginRepository.java:71)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:498)at org.picocontainer.lifecycle.ReflectionLifecycleStrategy.invokeMethod(ReflectionLifecycleStrategy.java:110)at org.picocontainer.lifecycle.ReflectionLifecycleStrategy.start(ReflectionLifecycleStrategy.java:89)at org.picocontainer.injectors.AbstractInjectionFactory$LifecycleAdapter.start(AbstractInjectionFactory.java:84)at org.picocontainer.behaviors.AbstractBehavior.start(AbstractBehavior.java:169)at org.picocontainer.behaviors.Stored$RealComponentLifecycle.start(Stored.java:132)at org.picocontainer.behaviors.Stored.start(Stored.java:110)at org.picocontainer.DefaultPicoContainer.potentiallyStartAdapter(DefaultPicoContainer.java:1016)at org.picocontainer.DefaultPicoContainer.startAdapters(DefaultPicoContainer.java:1009)at org.picocontainer.DefaultPicoContainer.start(DefaultPicoContainer.java:767)at org.sonar.core.platform.ComponentContainer.startComponents(ComponentContainer.java:134)at org.sonar.ce.container.ComputeEngineContainerImpl.start(ComputeEngineContainerImpl.java:191)at org.sonar.ce.ComputeEngineImpl.startup(ComputeEngineImpl.java:45)at org.sonar.ce.app.CeServer$CeMainThread.startup(CeServer.java:167)at org.sonar.ce.app.CeServer$CeMainThread.attemptStartup(CeServer.java:154)at org.sonar.ce.app.CeServer$CeMainThread.run(CeServer.java:141)Caused by: java.nio.channels.ClosedByInterruptException: nullat java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)at sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:315)at org.apache.commons.io.FileUtils.doCopyFile(FileUtils.java:1142)at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:1091)at org.apache.commons.io.FileUtils.copyFile(FileUtils.java:1038)at org.sonar.ce.container.CePluginJarExploder.explode(CePluginJarExploder.java:52)… 22 common frames omitted2018.08.25 18:53:26 INFO app[][o.s.a.SchedulerImpl] Process [ce] is stopped2018.08.25 18:53:26 INFO web[][o.s.p.StopWatcher] Stopping process2018.08.25 18:53:29 INFO app[][o.s.a.SchedulerImpl] Process [web] is stopped2018.08.25 18:53:29 INFO app[][o.s.a.SchedulerImpl] SonarQube is stopped镜像少了包 麻烦有时间协助处理一下。。。这个镜像是sonarqube官方提供的，我们并没有修改它 试下不使用持久化就是这个镜像 不用安装也可以吗？Sonarqube如果你的代码无需sonar检查就不用安装，你可以将 --set persistence.enabled=true设为false 先排除NFS的问题。麻烦把 2018.08.26 09:10:17 INFO  app[][o.s.a.p.ProcessLauncherImpl] Launch process[[key=‘ce’, ipcIndex=3, logFilenamePrefix=ce]] from [/opt/sonarqube]: /usr/lib/jvm/java-1.8-openjdk/jre/bin/java -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djava.io.tmpdir=/opt/sonarqube/temp -Xmx512m -Xms128m -XX:+HeapDumpOnOutOfMemoryError -cp ./lib/common/*:/opt/sonarqube/lib/jdbc/postgresql/postgresql-42.2.1.jar org.sonar.ce.app.CeServer /opt/sonarqube/temp/sq-process5270650338536645896properties把镜像的-Xmx512m  这个参数改成 1024M啊。512太小了起不来。。。官方都推荐 768M了啊您好，这是官方镜像的默认值。你好 sonarqube不停重启的问题解决了吗？Choerodon平台版本：0.9.0运行环境：一键安装问题描述：使用敏捷管理，如何创建一个自定义搜索类似：仅我的问题，这种搜索功能。
执行的操作：报错信息(请尽量使用代码块或系统截图的形式展现)：建议：截图中的功能就是自定义的功能，可以选择字段、操作类型和值进行关联筛选，但是仅我的问题之类的“泛定义”的值目前系统还不支持，只能定义确定的值生成对应界面的快速搜索选项额，我发现项目成员 没权限去自己定义参数。。。管理员定义的 搜索就是针对全部成员的。给个人使用没有什么意义啊确实是这样，这种泛参数的定义支持我们记录一下，会在后续的版本进行优化，谢谢反馈Choerodon平台版本：0.9.0UI 组件版本：0.9.4运行环境：一键安装问题描述：不明白dockerfile相关配置：
如果自己开发使用，是否需要修改成自己的域名地址？values.yamlenv:
部分是否需要修改自己的域名？执行的操作：
前端开发报错信息(请尽量使用代码块的形式展现)：建议：文档写得 不够清楚。。没有写重点你好，你可以参照


github.com


choerodon/choerodon-front-template/blob/master/charts/model-service/README.md
# Quick start

部署文件的渲染模板，我们下文将定义一些变量，helm执行时会将变量渲染进模板文件中。

## _helpers.tpl

这个文件我们用来进行标签模板的定义，以便在上文提到的位置进行标签渲染。

此项目标签总共分为两个部分: 平台、日志。

### 平台标签

#### deployment 级:

```
{{- define "service.labels.standard" -}}
choerodon.io/release: {{ .Release.Name | quote }}
{{- end -}}
```
平台管理实例需要的实例ID。


  This file has been truncated. show original





将values里面的值替换成实际的值http://choerodon.io/zh/docs/development-guide/basic/helm-chart/
这里是相关文档。
各项目的详细配置自定义配置。
dockerfile 和 values.yaml 中均是配置默认值（部署未修改时使用的值）。
values.yaml里面是部署时候展示的可修改默认配置字段
chart 的values.yaml 写了 dockerfile的可以不必写了主要作为一个开发，不知道这些值从哪来的。。？ENV PRO_CLIENT_ID
ENV PRO_TITLE_NAME
ENV PRO_HEADER_TITLE_NAME
ENV PRO_COOKIE_SERVER
比如我这些参数是要根据我实际地址修改吧，这些参数哪来不太懂明白意思。。这个是你根据项目实际情况需要，部署时候配置信息给出的默认环境变量 。
部署时候不关心那就使用默认给的。
比如 Client_ID 浏览器的HEADER_TITLE_NAME 等。
对日常开发者，不关心部署默认值可以不必修改，理解就好。嗯，ENV PRO_COOKIE_SERVER 这个地址也不需要修改？这个是存cookie的，部署的时候根据你创建的域名做修改就好。Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：参考猪齿鱼最新的分布式事务一致性解决方案（Saga）http://choerodon.io/zh/docs/development-guide/backend/framework/saga/（如何使用Saga）http://choerodon.io/zh/blog/event-consistent/（Saga简介）在合同系统进行了相关程序的集成以及参考demo程序逻辑编写，在github上所获取到的saga相关的阿斯加德服务 版本为0.8.0.RELEASE
合同系统相关的服务均按照文档所说对 猪齿鱼开发包版本升级至0.6.0RELEASE <choerodon.starters.version>0.6.0.RELEASE</choerodon.starters.version>并在消费端开启saga相关配置    return new ResponseEntity<>(startInstanceAndTask(dto, sagaTasks), HttpStatus.OK);
}public ResponseEntity<SagaInstanceDTO> start(final StartInstanceDTO dto) {
    final String code = dto.getSagaCode();
    if (!sagaMapper.existByCode(code)) {
        throw new FeignException(“error.saga.notExist”);
    }原因分析：查询到相关的表为如下表 asgard_orch_saga疑问：问题：请问saga如何在这张表（asgard_orch_saga）中维护数据，是需要手动维护还是在其他地方？在博客中提及的猪齿鱼事务定义界面如何访问？本地启动我们的eureka-server，线上注册中心用go-register-server，服务启动后注册中心发送事件到kafka，然后asgard-service通过kafka的消息监听微服务启动，在服务启动后向具体服务拉去@Saga和@SagaTask的事务定义实现自动维护。
可通过root用户登录访问事务定义页面。
Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：容器日志查看这里可以考虑一下openshift上面的save、expand、stop following和go to top，这几个功能有很多人在使用过程中有反馈。好，这个迭代正在优化！Choerodon平台版本：0.9.0运行环境：一键安装问题描述：执行的操作：报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作原因是在看板配置中设置了列最大数量限制
1、打开看板的配置页面：
如上操作即可取消列约束限制好的，我试一下。。Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：自主搭建问题描述：
修改域名里面，路径不能加 “/” ,"/"在路径中应该是可以出现的吧。
您好，要在服务器部署项目，是不是需要根据平台手册搭建一整套k8s集群环境呢，http://choerodon.io/zh/docs/installation-configuration/steps/nfs/http://choerodon.io/zh/docs/concept/platform-concept/
具体你可以看一下这个里面描述的环境 。
但一般来说需要有一个K8S集群，恩，这版本已经解决，下次发布新版本就可以建这种格式的路径了Choerodon平台版本: 0.9.0遇到问题的执行步骤:
一键部署
gitlab这步一直过不去。这个问题网上也有好多人遇到。但是没找到解决方案。
https://gitlab.com/gitlab-org/omnibus-gitlab/issues/1601
https://zhuanlan.zhihu.com/p/32248933环境信息(如:节点信息):
k8s 1.11报错日志:请尝试给Gitlab挂载目录添加777权限试一试比如你在values.sh设置的参数如下那么执行这步还是不行。完整的日志。
NFS已经授权777了  chmod +777 -R /u01/io-choerodon/gitlab建议在我们推荐的1.8.5版本k8s上进行部署哈 。目前想用猪齿鱼的分布式事务，请问有相关设计文档，或者使用文档吗？我们没有分布式事务，有基于saga的最终一致性的实现，目前正在内部测试使用，即将发布。这里有个如何使用的小demo：https://github.com/flyleft/spring-cloud-base/tree/master/asgard-saga-demo


GitHub



choerodon/event-store-service
event-store-service - Event Store Service implements data consistency,and the message queue kafka is supported.






event服务不能进行分布式事务？event-store-service - Event Store Service implements data consistency,and the message queue kafka is supported.event服务是实现最终一致性的一种方式，不是分布式事务，无法实现回滚操作最近我们部门要使用猪齿鱼开发，请问您们用了哪些手段来来保证数据一致性呢？都可以从哪里找到demo啊。想系统学习下你们的数据一致性解决方案。@shirayner谢谢大佬回复，还有个问题。网上关于分布式事务数据一致性的解决方案有很多种：2PC、TCC、本地消息表、可靠消息服务、 Event Sourcing、Saga等。猪齿鱼是如何对这些方案做技术选型的？另外在哪些场景使用什么方案？选型的话主要结合CAP理论，考虑到方案的可实现和拓展性的。具体的可以看下微信公众号的那篇文章。好的谢谢。您们新上的Saga将来是会要取代  Event Sourcing 吗？还是说两者并存？两者并存的话，各用在什么场景中呢？Choerodon有关于分布式数据一致性的都已经迁移到了asgard上。之前的不会再做版本的迭代了。同时asgard服务配备了对应的页面。可以在页面上检索到更多的信息。好的，谢谢了Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：持续集成这里的请求响应都比较慢，不知道是不是每次切换应用的时候，都是去gitlab那边实时查询的，麻烦看看有什么可以优化的地方对的，现在是每次切换应用的时候，都会去gilab那边实时查询，我们将在这个迭代优化Choerodon平台版本：Choerodon官网：https://api.choerodon.com.cn/运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：使用在 汉得hzero的基础部署k8s服务在组织下部署实例 hzero-iam 出现错误：
需要执行 io/choerodon/config/mapper/ServiceMapper.java 下的SQL，
缺少mgmt_service表修改的数据：报错信息(请尽量使用代码块的形式展现)：
Network:Error querying database.  Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table ‘hzero_platform.mgmt_service’ doesn’t exist
The error may exist in io/choerodon/config/mapper/ServiceMapper.java (best guess)
The error may involve io.choerodon.config.mapper.ServiceMapper.selectOne-Inline
The error occurred while setting parameters
SQL: SELECT creation_date,created_by,last_update_date,last_updated_by,object_version_number,id,name  FROM mgmt_service  WHERE  name = ?
Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table ‘hzero_platform.mgmt_service’ doesn’t exist
; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table ‘hzero_platform.mgmt_service’ doesn’t exist
2018-08-31 15:49:06.363  INFO [-,] 6 — [       Thread-4] s.c.a.AnnotationConfigApplicationContext : Closing org.springframework.context.annotation.AnnotationConfigApplicationContext@15327b79: startup date [Fri Aug 31 15:49:00 CST 2018]; root of context hierarchy
2018-08-31 15:49:06.364  INFO [-,] 6 — [       Thread-4] o.s.j.e.a.AnnotationMBeanExporter        : Unregistering JMX-exposed beans on shutdownmanager-service



GitHub



choerodon/manager-service
This service is the management center of the choerodon microservices framework. - choerodon/manager-service





This service is the management center of the choerodon microservices framework. - choerodon/manager-serviceChoerodon平台版本：0.8.0.RELEASE运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：问题描述在gateway-helper鉴权的时候，这个passProjectOrOrgPermission方法中，有个matcher.extractUriTemplateVariables，这个会返回路径中的organization id对吧，不知道有没有理解错哈，但是我们现在开发的服务的路由的组织id都是驼峰来着，但是我看到常量中配置的是下划线，导致现在这段鉴权解析的时候就过不了，这个东西能不能做成配置的呀？
你好。choerodon 中的mapping都是使用的小写字母+“_" 的格式。这个也是choerodon的规范之一。我们会考虑将变量抽离出来作为可配置的。但是建议使用这种格式谢谢大佬Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
Choerodon后端小白一只，只是想先搭一个SpringBoot的demo先跑起来。克隆了新建的项目，先把有关的插件先注释了，但是启动还是报错
先在pom文件里修改了版本，进行了重新构建

然后在application里面注释掉了插件有关的
# application.yml
spring:
#  sleuth:
#    integration:
#      enabled: false
#    scheduled:
#      enabled: false
#    sampler:
#      percentage: 1.0
#    stream:
#      enabled: true
  datasource:
    url: jdbc:mysql://localhost:3306/demo_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
    username: root
    password: root
#  kafka: #使用原生spring kafka需要配置
#    bootstrap-servers: localhost:9092
#    consumer:
#      group-id: iam-service
#      auto-offset-reset: earliest
#  cloud:
#    bus:
#      enabled: true
#    stream:
#      kafka:
#        binder:
#          brokers: 127.0.0.1:9092
#          zkNodes: 127.0.0.1:2181
#event:
#  store:
#    service:
#      name: event-store-service
#choerodon:
#  ldap:
#    userNameType: 0
#  devops:
#    message: true
#  event:
#    consumer:
#      enabled: true # 是否开启，不设置默认开启
#      queue-type: kafka # 消息队列类型,目前仅支持kafka
#      enable-duplicate-remove: true # 是否开启去重功能
#      failed-strategy: nothing # 消息失败策略
#      retry:
#        enabled: true # 是否开启重试功能
#      kafka:
#        bootstrap-servers: localhost:9092
eureka:
  instance:
    preferIpAddress: true
    leaseRenewalIntervalInSeconds: 10
    leaseExpirationDurationInSeconds: 30
    metadata-map:
      VERSION: v1
  client:
    serviceUrl:
      defaultZone: http://localhost:8000/eureka/
    registryFetchIntervalSeconds: 10
#hystrix:
#  command:
#    default:
#      execution:
#        isolation:
#          thread:
#            timeoutInMilliseconds: 15000
#  stream:
#    queue:
#      enabled: false
#ribbon:
#  ReadTimeout: 5000
#  ConnectTimeout: 5000
#file-service:
#  ribbon:
#    ReadTimeout: 15000
#    ConnectTimeout: 15000
mybatis:
  mapperLocations: classpath*:/mapper/*.xml
  configuration: # 数据库下划线转驼峰配置
    mapUnderscoreToCamelCase: true
在values.yaml中注释了kafka相关的
# Default values for manager-service.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.replicaCount: 1image:
  repository: registry.cn-hangzhou.aliyuncs.com/ora-xadc/xa-todo-servie
  pullPolicy: AlwayspreJob:
  preConfig:
    configFile: application.yml
    mysql:
      host: 192.168.12.175
      port: 3306
      database: manager_service
      username: root
      password: choerodon
  preInitDB:
    mysql:
      host: 192.168.12.175
      port: 3306
      database: demo_service
      username: root
      password: choerodondeployment:
  managementPort: 18088env:
  open:
    ## register-server
    EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://register-server.io-choerodon:8000/eureka/
    ## config-server
    SPRING_CLOUD_CONFIG_ENABLED: true
    SPRING_CLOUD_CONFIG_URI: http://config-server.framework:8010/
    ## mysql
    SPRING_DATASOURCE_URL: jdbc:mysql://localhost/demo_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
    SPRING_DATASOURCE_USERNAME: root
    SPRING_DATASOURCE_PASSWORD: choerodon
    ## kafka
#    CHOERODON_EVENT_CONSUMER_KAFKA_BOOTSTRAP_SERVERS: kafka-0.kafka-headless.kafka.svc.cluster.local:9092,kafka-1.kafka-headless.kafka.svc.cluster.local:9092,kafka-2.kafka-headless.kafka.svc.cluster.local:9092
#    SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS: kafka-0.kafka-headless.kafka.svc.cluster.local:9092,kafka-1.kafka-headless.kafka.svc.cluster.local:9092,kafka-2.kafka-headless.kafka.svc.cluster.local:9092
#    SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES: zookeeper-0.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.zookeeper.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.zookeeper.svc.cluster.local:2181
#    SPRING_KAFKA_PRODUCER_VALUE_SERIALIZER: org.apache.kafka.common.serialization.ByteArraySerializermetrics:
  path: /prometheus
  group: spring-bootlogs:
 parser: spring-bootpersistence:
  enabled: false
  ## A manually managed Persistent Volume and Claim
  ## Requires persistence.enabled: true
  ## If defined, PVC must be created manually before volume will be bound
  # existingClaim:
  # subPath:service:
  enabled: false
  type: ClusterIP
  port: 18080

ingress:
  enabled: falseresources:
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources,such as Minikube. If you do want to specify resources,uncomment the following
  # lines,adjust them as necessary,and remove the curly braces after ‘resources:’.
  limits:
    # cpu: 100m
    memory: 2Gi
  requests:
    # cpu: 100m
    memory: 1.5Gi
由于在跑的时候有提示18081端口已被占用，所以我改成18088了
注释了demo类中的引用
java.net.ConnectException: Connection refused: no further information
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[na:1.8.0_77]
at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[na:1.8.0_77]
at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.8.jar:3.4.8–1]
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) ~[zookeeper-3.4.8.jar:3.4.8–1]原因分析：报了zookeeper连接异常的错误，因为我的kafka装在虚拟机的docker中，暂时没有连接，demo是本地跑的。所以我想注释了插件，先把demo跑通，我只是想先跑通demo啊疑问：提出您对于遇到和解决该问题时的疑问本地启动异常？在resources 文件夹下创建application-default.yml 文件。添加配置这个文件在我项目里是application.yml，我把原来的都注释了，然后添加了你说的代码，还是报错，应用启动失败。
clean 一下，然后重新import下依赖，再启动在每次运行之前，我都会clean一下，重新构建以后再跑，然而并没有什么卵用麻烦贴一下pom文件和错误的堆栈信息。还有你的的demo项目是从https://github.com/wmzzh117/choerodon-todo-service 这里clone的吗代码克隆地址为https://code.choerodon.com.cn/ora-xadc/xa-todo-servie.git，看了论坛前面的帖子，我把pom文件里Choerodon的版本号改了，重新构建了。看了一下代码。里面连接的各种插件都以写好。但是刚接触这个，我是想先跑通demo。所以我放弃了直接跑那个工程了，现在按照官网文档新建项目，一步一步来。初步还是可以跑起来的。慢慢的再添加那些插件，对学习有好处。谢谢耐心的解答。我拉了下代码本地是可以跑通过的。你可以检索下错误信息，看起来并不像是代码的错误，而像是IDE或者jdk的问题。Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):
2台4H16G的机器，mysql和kafka都是用的外部的
报错日志:
Error: Job failed: BackoffLimitExceeded
?Success of install Choerodon iam service.
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问如何解决这个错误呢你这里配的全是外网ip?Choerodon平台版本：0.9.0运行环境：一键安装问题描述：
应用部署报错执行的操作：
部署完提示失败报错信息(请尽量使用代码块或系统截图的形式展现)：请确保应用chart路径下，文件夹名与应用名一致
nginx示例
http://choerodon.io/zh/docs/quick-start/nginx-demo/
创建文档 并没有说 跟yaml文件名一样啊。抱歉，我们会对文档做出相应修改Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：部署实例之后实例显示运行中，但是没有容器，容器状态为0这个问题在上一篇帖子已经回复过了。小哥哥记性真好Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：我们自己创建了一个微服务，并写了接口，但是经常会遇到一个情况，就是写好的接口没有自动插入到iam_permission这张表里，从而无法去配置权限执行的操作：
如:代码提交合并以后发布，但是iam_permission表中没有这个接口，通过path模糊查询的@API，这个注解必须要有吗
这个类下所有的接口都进不去吗，还是只有这一个接口这个类下面就写了这一个接口1.看下数据库有没有重复的code，code生成规则是  服务名.organization.list
2.看下该服务其他类的接口能不能刷进去， 都刷不进去的话可能是kafka消息没发过来，需要排查manager->iam发消息部分不一定要继承basecontroller把？那个没影响Choerodon平台版本：0.9.0运行环境：一键安装问题描述：
部署实例一直在处理中。。。几个小时了。
客户端一直报：
执行的操作：
就点了部署操作，，一直在处理中，也无法删除报错信息(请尽量使用代码块或系统截图的形式展现)：建议：现在又报这个错了：
你好，你可以点击环境总览的配置库，查看对应环境中是否有该yaml文件请问搭建时这一步是否操作？http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/#启用SSH协议有操作的：
K8S 的管理服务器，gitlab域名就是指到这台服务器上的呢你好，请尝试在本地使用SSH协议进行克隆项目试一试呢   看看会有什么结果
配置文件是这样的吧？嗯   是对的    那你DNS上配置的ip是这个内网ip还是这个节点对应的公网ip呢DNS上是配的 节点对应的公网ip咋解决哦，全部都部署好了。就这个问题。。在线等亲   我们部署还没有遇到这个情况   正在排查问题哈   请耐心等待需要我这边提供 服务器环境，可以提供给你们看看的。可以查看一下环境总览中，日志那里有没有错误日志。以及配置库、已解析、已执行的commit信息是否一致。如果一致的话。可以进到配置库中，选择要部署的配置文件进行编辑。可以添加空格或换行然后保存文件，再看一下部署的状态有没有更新Choerodon平台版本：0.8.0.RELEASE运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：问题描述最新版本github的oauth-service跟manager-service根本跑不起来，像Swagger版本报错啥的就不说了，choerodon.starters.version调整也没用，这个有人关注吗。。。无奈
知道套路，贴个图。
你好，你访问的是master分支的吗？还是0.9.0 tag的代码直接master的
oauth-service跟manager-service根本跑不起来oauth-service跟manager-service跑不起来报的什么异常？贴一下不好意思，oauth没毛病。
manager的
你好，master分支的代码是我们在持续更新的代码，不是一个稳定可用的版本。你可以拉取最新tag下的代码，比如0.9.0好嘞，我试试，感谢。升级之后，现在用oauth，认证不了，在循环认证。这个有碰到过吗，大神
你好，可以配置一下同时确保一下你本地的redis是启动没有问题的。好的呢，我先试试。
大神，刚还碰到一个问题。
自定义的路径，比如说/custom/，这种接口配置为啥不会被JwtTokenFilter过滤呢，
路由规则也配置了，初始化的时候也生效了。
choerodon:
resource:
  pattern: /v1/, /custom/* 你好，过滤不生效是因为目前只支持单个path的配置。我们这边修改一下，下一个版本会加上多条记录的过滤Choerodon平台版本：0.9.0运行环境：公司提供请问使用猪齿鱼平台，可不可以使用自己的gitlab、harbor和chart仓库不可以Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：自主搭建问题描述：新部署了一个应用，有些接口没有出现来权限表里面，但swagger里面可以查询到。使用manager-service里面的手动刷新权限的接口也没有效果你好，你是怎么解决的？我在swaager手动调用了 manager-service 里面的 手动刷新权限接口  ，iam收到消息，同一个微服务，本地的权限全部进去了，服务其上的部分权限刷不进去，怎么解决，数据库里没有， swagger json 获取地址是哪里？
，
swagger json的文档生成策略在哪里？刷入permission规则包含有:
1.所有的controller都要以controller结尾
2.同一个controller下不能有相同的方法名
3.接口上要有@Permission注解
4.如果在controller上自定义swagger tag，至少要包含一个当前controller名驼峰转中划线格式的tag，如下：
Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):报错日志:
执行  kubectl logs choerodon-manager-service-5df4dbbb49-lbgf8 -n choerodon-devops-prod后报错日志如下
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问重新执行过很多次了，一直这个地方过不去，有什么解决办法么你是否启用了防火墙或者其他的安全策略?Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：环境流水线创建了，去哪运行生成的指令，看视频和手册都说是去集群中运行，我要去运行这个命令，需要在服务器上安装些什么呢，是手册里面写的这些都要按照步骤来安装吗？
http://choerodon.io/zh/docs/installation-configuration/steps/报错信息(请尽量使用代码块的形式展现)：
@sssnow 你可以选择在已有的k8s上执行或者搭建一个新的k8s集群并安装helm然后执行。麻烦再问一下，这里的容器状态为0是为什么呢，我是按照持续集成的那个视频一步步来的
您好,请问您部署的choerodon平台是0.9.0吗？额，我只知道是汉得员工用的版本你好这边需要在k8s环境客户端环境中中执行一个创建自定义K8s对象的文件，http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-devops/
，Choerodon平台版本：0.9.0运行环境：一键安装问题描述：创建一个nginx-demo示例报错：
http://choerodon.io/zh/docs/quick-start/nginx-demo/执行的操作：报错信息(请尽量使用代码块或系统截图的形式展现)：
Sorry, 在nginx-demo 中指定了镜像仓库的用户名和密码，如果你修改了密码则会发生这个错误。在gitlab项目或组中添加相应的环境变量指定镜像库的用户名DOCKER_USER和密码DOCKER_PASSWORD
然后修改 .gitlab-ci.yaml文件gitlab数据库只支持mysql吗？
可以用PG吗？我看官方对gitlab数据库的建议：我们的平台目前以mysql为主，为了减少运维成本选用了mysql，如果你们有专业postgresql的运维人员，可以尝试切换为postgersql数据库。Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。Choerodon客户端 的部署环境必须要使用 1.8.5？理论上可以在1.7以上集群部署，如果你使用自己搭建的k8s，需要特别注意网络和存储权限，如在某些网络组件上默认禁止了不同namespace间的网络通信。在高版本的k8s中已默认禁止了configmap的写权限等。按正常 的，肯定很多的已经有自己在使用的K8S集群。。用choerodon做K8S集群管理，发布各基于K8S集群的环境里面。不知道会有多少问题。。。不会有问题，我们这边K8S集群全部自建，版本也不一致，都可以用猪齿鱼。嗯，我先测试一下。有问题在请教。。谢谢本文是Kubernetes系列的第一篇，将介绍Docker和Kubernetes两大热门开源产品，主要内容包括：基本概念、基础组件、Kubernetes架构。Docker 起初是 dotCloud 公司创始人 Solomon Hykes 在法国的时候发起的一项公司内部项目，Docker是基于 dotCloud 公司多年云服务技术的一次革新，在 2013 年 3 月以 Apache 2.0 授权协议进行开源，其项目主要代码在 GitHub 上进行维护，自从Docker 开源之后，就一直受到了广泛讨论和关注。Docker 进行开发实现使用的是Google 公司推出的 Go 语言，对进程进行封装隔离是基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，这属于操作系统层面的虚拟化技术。因为隔离的进程独立于宿主与其它隔离的进程，所以也称其为容器（后文会对“容器”的概念进行详细介绍）。Docker 在容器的基础上，进行了进一步的封装，从网络互联、文件系统到进程隔离等，大大地简化了容器的创建和维护，让 Docker 技术比虚拟机技术更加轻便、快捷。以下两张图片对比了 Docker 与传统虚拟化方式的不同之处。Docker 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，没有进行硬件虚拟；而传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程。因此容器要比传统虚拟机更为轻便。Docker是一种新兴的虚拟化方式，跟传统的虚拟化方式相比具有众多优势。因为容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，所以Docker 对系统资源的利用率更高。Docker 容器应用由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。极大地节省了开发、测试，部署的时间。开发过程中比较常见的问题就是环境一致性问题。因为开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。对开发和运维人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合持续集成(Continuous Integration)系统进行集成测试，而运维人员则可以直接在各种环境中快速部署该镜像，甚至结合持续部署(Continuous Delivery/Deployment) 系统进行自动部署。而且使用 Dockerfile 使镜像构建透明化，不仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好地在生产环境中部署该镜像。由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻松地将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境变化导致应用无法正常运行的情况。Docker 使用的分层存储以及镜像技术，使得应用重复部分的复用更为容易，也使得应用的维护更新和基于基础镜像进一步扩展镜像变得非常简单。此外，Docker 团队同各个开源项目团队一起维护了一大批高质量的官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大降低了应用服务的镜像制作成本。Docker的口号是“Build, Ship and Run Any App, Anywhere.”，大意是编译好一个应用后，可以在任何地方运行，不会像传统的程序一样，一旦换了运行环境，往往就会出现缺这个库，少那个包的问题。那么Docker是怎么做到这点的呢？简单说就是它在编译应用的时候把这个应用依赖的所有东西都构建到镜像里面（有点像程序的静态编译——只是像而已）。我们把这个编译构建好的东西叫Docker镜像（Image），然后当Docker deamon（Docker的守护进程/服务进程）运行这个镜像的时候，我们称其为Docker容器（Container）。可以简单理解Docker镜像和Docker容器的关系就像是程序和进程的关系一样(当然实质是不一样的)。每个Docker镜像（Image）都引用了一些只读的（read-only）层（layer），不同的文件系统layer也不同。这些layer堆叠在一起构成了容器（Container）的根文件系统（root filesystem）。下图是Ubuntu 15.04的镜像，共由4个镜像层（image layer）组成：容器和镜像的主要区别就是顶部的那个可写层（即之前说的那个“container layer”）。容器运行时做的所有操作都会写到这个可写层里面，当容器删除的时候，这个可写层也会被删掉，但底层的镜像依旧保持不变。所以，不同的容器都有自己的可写层，但可以共享同一个底层镜像。下图展示了多个容器共享同一个Ubuntu 15.04镜像。Docker的storage driver负责管理只读的镜像层和可写的容器层，当然不同的driver实现的方式也不同，但其后都有两项关键技术：可堆叠的镜像层（stackable image layer）和写时拷贝技术（copy-on-write, CoW）。刚开始的时候，Docker一般只适用于无状态的计算场景使用。但随着发展，Docker通过data volume技术也可以做到数据持久化了。Data volume就是我们将主机的某个目录挂载到容器里面，这个data volume不受storage driver的控制，所有对这个data volume的操作会绕过storage driver直接其操作，其性能也只受本地主机的限制。而且我们可以挂载任意多个data volume到容器中，不同容器也可以共享同一个data volume。下图展示了一个Docker主机上面运行着两个容器.每一个容器在主机上面都有着自己的地址空间（/var/lib/docker/…），除此以外，它们还共享着主机上面的同一个/data目录。参考文档：https://yeasy.gitbooks.io/docker_practice/content/introduction/what.html
https://yeasy.gitbooks.io/docker_practice/content/introduction/why.html
https://docs.docker.com/storage/storagedriver/Kubernetes 是谷歌开源的容器集群管理系统，是 Google 多年大规模容器管理技术 Borg 的开源版本，主要功能包括：Kubernetes 发展非常迅速，已经成为容器编排领域的领导者。Kubernetes 提供了很多的功能，它可以简化应用程序的工作流，加快开发速度。通常，一个成功的应用编排系统需要有较强的自动化能力，这也是为什么 Kubernetes 被设计作为构建组件和工具的生态系统平台，以便更轻松地部署、扩展和管理应用程序。用户可以使用 Label 以自己的方式组织管理资源，还可以使用 Annotation 来自定义资源的描述信息，比如为管理工具提供状态检查等。此外，Kubernetes 控制器也是构建在跟开发人员和用户使用的相同的 API 之上。用户可以编写自己的控制器和调度器，也可以通过各种插件机制扩展系统的功能。这种设计使得用户可以方便地在 Kubernetes 之上构建各种应用系统。Kubernetes 不是一个传统意义上，包罗万象的 PaaS (平台即服务) 系统。它给用户预留了选择的自由。另外，已经有很多 PaaS 系统运行在 Kubernetes 之上，如 Openshift, Deis 和 Eldarion 等。 你也可以构建自己的 PaaS 系统，或者只使用 Kubernetes 管理你的容器应用。当然了，Kubernetes 不仅仅是一个 “编排系统”，它消除了编排的需要。Kubernetes 通过声明式的 API 和一系列独立、可组合的控制器保证了应用总是在期望的状态，而用户并不需要关心中间状态是如何转换的。这使得整个系统更容易使用，而且更强大、更可靠、更具弹性和可扩展性。参考文档：https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/Kubernetes 主要由以下几个核心组件组成：除了核心组件，还有一些推荐的 Add-ons：Etcd是CoreOS基于Raft开发的分布式key-value存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。Etcd主要功能：kube-apiserver 是 Kubernetes 最重要的核心组件之一，主要提供以下的功能：Controller Manager由kube-controller-manager和cloud-controller-manager组成，是Kubernetes的大脑，它通过apiserver监控整个集群的状态，并确保集群处于预期的工作状态。kube-controller-manager由一系列的控制器组成在Kubernetes启用Cloud Provider的时候才需要，用来配合云服务提供商的控制，也包括一系列的控制器，如：kube-scheduler 负责分配调度 Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配 Node 的 Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod的 NodeName 字段）。调度器需要充分考虑诸多的因素：每个节点上都运行一个 kubelet 服务进程，默认监听 10250 端口，接收并执行 master 发来的指令，管理 Pod 及 Pod 中的容器。每个 kubelet 进程会在 API Server 上注册节点自身信息，定期向 master 节点汇报节点的资源使用情况，并通过 cAdvisor 监控节点和容器的资源。容器运行时（Container Runtime）是 Kubernetes 最重要的组件之一，负责真正管理镜像和容器的生命周期。Kubelet 通过 Container Runtime Interface (CRI) 与容器运行时交互，以管理镜像和容器。每台机器上都运行一个 kube-proxy 服务，它监听 API server 中 service 和 endpoint 的变化情况，并通过 iptables 等来为服务配置负载均衡（仅支持 TCP 和 UDP）。kube-proxy 可以直接运行在物理机上，也可以以 static pod 或者 daemonset 的方式运行。kube-proxy 当前支持一下几种实现：K8s设置由几个部分组成，其中一些是可选的，一些是整个系统运行所必需的。下面是k8s的全局架构图Master有三个组件：API Server、Scheduler、Controller。API Server提供了友好易用的API供外部调用，同时有很多强大的工具使得API调用更加简单，如kubectl封装了大量API调用，使得部署、配置更加简单。Kubernetes-dashboard可以让用户在界面上操作Kubernetes，而无需手动输入各个API的调用地址参数等信息。当API Server收到部署请求后，Scheduler会根据所需的资源，判断各节点的资源占用情况分配合适的Node给新的容器。判断依据包括内存、CPU、磁盘等。Controller负责整个集群的整体协调和健康，保证每个组件以正确的方式运行。在图的最下边是ETCD数据库。如前文所述ETCD是分布式存储数据库，其作为Kubernetes的中央数据库，存储了集群的状态，组件可以通过查询ETCD了解集群的状态。Kubernetes Master分配容器到Node执行，Node将会承受压力，通常情况下新容器不会运行在Master上。或者说Master是不可调度的，但是你也可以选择把Master同时也作为Node,但是这并不是地道的用法。下面的为Node的架构图:Kube-proxy在Node中管理网络，其左右至关重要。Kube-proxy通过管理iptables等方式使得pod到pod之间，和pod到node之间网络能够互通。实质上在跨主机的pod之间网络也能够互通。Kubelet负责向api server报告信息，并把健康状态、指标和节点状态信息存入ETCD中。Docker上文已详细介绍这里就不多做阐述。Supervisord保证Docker和kubelet一直在运行中，supervisord并不是必须组件，可以使用其他类似组件替换。Pod是可以在Kubernetes中创建和管理的最小可部署计算单元。一个POD中可以包含多个容器，但Kubernetes仅管理pod。如果多个容器运行在一个POD中，就相当于这些容器运行在同一台主机中，需要注意端口占用问题。参考资料:http://k8s.docker8.com/
https://www.youtube.com/watch?v=zeS6OyDoy78Choerodon猪齿鱼是一个开源企业服务平台，是基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理的开源平台，同时提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，致力帮助企业聚焦于业务，加速数字化转型。欢迎大家关注Choerodon，支持开源…大家也可以扫描二维码关注Choerodon的微信和微博：Choerodon平台版本: 0.8.0遇到问题的执行步骤:
sh choerodon-install.sh values.sh文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):
部署到一台测试服务器报错日志:
[Step 9]: create database for Choerodon …
Error: timed out waiting for the condition
?create database failed原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
一键安装choerodon,执行脚本的时候第九步一直超时，我重试了无数次，还是这个错误，是否要设置mysql超时连接时间，需要到怎么设置。疑问:提出您对于遇到和解决该问题时的疑问
超时，要如何设置呢？
如果你之前安装失败 或执行过分布安装 请根据文档中关于安装失败的步骤进行数据清理和还原。执行过无所次了，也删除过数据重新执行，还是报这个错同样的错误。试过很多遍了。错误依然存在。请问下有什么解决办法？是否已经创建了 MYSQL_DIR 中定义的目录？创建了。
NAME:   choerodon-mysql
LAST DEPLOYED: Wed Aug 29 15:44:26 2018
NAMESPACE: liantai-devops-ofc
STATUS: DEPLOYEDRESOURCES:
==> v1/Pod(related)
NAME                              READY  STATUS   RESTARTS  AGE
choerodon-mysql-7ff6f6d496-pvg6p  0/2    Pending  0         9m==> v1/ConfigMap
NAME                    DATA  AGE
choerodon-mysql-config  1     9m==> v1/Service
NAME             TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)   AGE
choerodon-mysql  ClusterIP  10.68.81.143         3306/TCP  9m==> v1beta1/Deployment
NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
choerodon-mysql  1        0        0           0          9m Success of install Mysql for Choerodon.[Step 8]: checking Mysql ready …
➜ choerodon-mysql not ready,sleep 5s,check it.
➜ choerodon-mysql not ready,sleep 5s,check it.
 Mysql is ready.[Step 9]: create database for Choerodon …
Error: Job failed: DeadlineExceeded
 create database failed[root@ofcdeploy choerodon]# kubectl get all -n choerodon-devops-prod
NAME                                   READY     STATUS    RESTARTS   AGE
pod/choerodon-mysql-7ff6f6d496-pvg6p   2/2       Running   0          23mNAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/choerodon-mysql   ClusterIP   10.68.81.143           3306/TCP   23mNAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/choerodon-mysql   1         1         1            1           23mNAME                                         DESIRED   CURRENT   READY     AGE
replicaset.apps/choerodon-mysql-7ff6f6d496   1         1         1         23mNAME                                   DESIRED   SUCCESSFUL   AGE
job.batch/create-choerodon-databases   1         0            23m[root@ofcdeploy choerodon]# kubectl get pvc -n choerodon-devops-prod
NAME                  STATUS    VOLUME               CAPACITY   ACCESS MODES   STORAGECLASS   AGE
choerodon-mysql-pvc   Bound     choerodon-mysql-pv   3Gi        RWO看job日志。最后一条有关系吗？Volumes:         
Events:
Type     Reason            Age                From            MessageNormal   SuccessfulCreate  25m                job-controller  Created pod: create-choerodon-databases-2jqj9
Normal   SuccessfulDelete  23m                job-controller  Deleted pod: create-choerodon-databases-2jqj9
Warning  DeadlineExceeded  23m (x2 over 23m)  job-controller  Job was active longer than specified deadline执行过无所次了，也删除过数据重新执行，还是报这个错？你的服务器是哪个服务商？ 使用哪种方式部署的k8s我是在自己的服务器上部署的。k8s用二进制方式部署的。1.11.1版本我们暂时仅在 1.8.5 和 1.9.9中测试，1.11暂未测试，你在部署的时候 可以另起一个窗口 跟踪pod信息, 当出现 create-choerodon-databases-xxx 这个pod时跟踪它的日志。2018-08-29T07:35:08.036734Z 0 [Note] mysqld (mysqld 5.7.22) starting as process 1 …
2018-08-29T07:35:08.056832Z 0 [Warning] InnoDB: Using innodb_file_format is deprecated and the parameter may be removed in future releases. See http://dev.mysql.com/doc/refman/5.7/en/innodb-file-format.html
2018-08-29T07:35:08.057328Z 0 [Note] InnoDB: PUNCH HOLE support available
2018-08-29T07:35:08.057367Z 0 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins
2018-08-29T07:35:08.057382Z 0 [Note] InnoDB: Uses event mutexes
2018-08-29T07:35:08.057408Z 0 [Note] InnoDB: GCC builtin __atomic_thread_fence() is used for memory barrier
2018-08-29T07:35:08.057420Z 0 [Note] InnoDB: Compressed tables use zlib 1.2.3
2018-08-29T07:35:08.057432Z 0 [Note] InnoDB: Using Linux native AIO
2018-08-29T07:35:08.059387Z 0 [Note] InnoDB: Number of pools: 1
2018-08-29T07:35:08.060658Z 0 [Note] InnoDB: Using CPU crc32 instructions
2018-08-29T07:35:08.067729Z 0 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M
2018-08-29T07:35:08.090888Z 0 [Note] InnoDB: Completed initialization of buffer pool
2018-08-29T07:35:08.099249Z 0 [Note] InnoDB: If the mysqld execution user is authorized, page cleaner thread priority can be changed. See the man page of setpriority().
2018-08-29T07:35:08.220479Z 0 [Note] InnoDB: Highest supported file format is Barracuda.
2018-08-29T07:35:08.439140Z 0 [Note] InnoDB: Creating shared tablespace for temporary tables
2018-08-29T07:35:08.450553Z 0 [Note] InnoDB: Setting file ‘./ibtmp1’ size to 12 MB. Physically writing the file full; Please wait …
2018-08-29T07:35:08.948882Z 0 [Note] InnoDB: File ‘./ibtmp1’ size is now 12 MB.
2018-08-29T07:35:08.975180Z 0 [Note] InnoDB: 96 redo rollback segment(s) found. 96 redo rollback segment(s) are active.
2018-08-29T07:35:08.975275Z 0 [Note] InnoDB: 32 non-redo rollback segment(s) are active.
2018-08-29T07:35:08.979403Z 0 [Note] InnoDB: 5.7.22 started; log sequence number 12360107
2018-08-29T07:35:08.981170Z 0 [Note] InnoDB: Loading buffer pool(s) from /var/lib/mysql/ib_buffer_pool
2018-08-29T07:35:08.981351Z 0 [Note] Plugin ‘FEDERATED’ is disabled.
2018-08-29T07:35:09.179995Z 0 [Note] Found ca.pem, server-cert.pem and server-key.pem in data directory. Trying to enable SSL support using them.
2018-08-29T07:35:09.185626Z 0 [Warning] CA certificate ca.pem is self signed.
2018-08-29T07:35:09.189307Z 0 [Note] InnoDB: Buffer pool(s) load completed at 180829  7:35:09
2018-08-29T07:35:09.193628Z 0 [Note] Server hostname (bind-address): ‘*’; port: 3306
2018-08-29T07:35:09.193802Z 0 [Note] IPv6 is available.
2018-08-29T07:35:09.193866Z 0 [Note]   - ‘::’ resolves to ‘::’;
2018-08-29T07:35:09.194613Z 0 [Note] Server socket created on IP: ‘::’.
2018-08-29T07:35:09.199889Z 0 [Warning] Insecure configuration for --pid-file: Location ‘/var/run/mysqld’ in the path is accessible to all OS users. Consider choosing a different directory.
2018-08-29T07:35:09.253221Z 0 [Warning] ‘user’ entry ‘root@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.253366Z 0 [Warning] ‘user’ entry ‘mysql.session@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.253407Z 0 [Warning] ‘user’ entry ‘mysql.sys@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.253552Z 0 [Warning] ‘db’ entry ‘performance_schema mysql.session@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.253587Z 0 [Warning] ‘db’ entry ‘sys mysql.sys@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.253674Z 0 [Warning] ‘proxies_priv’ entry ‘@ root@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.337764Z 0 [Warning] ‘tables_priv’ entry ‘user mysql.session@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.337853Z 0 [Warning] ‘tables_priv’ entry ‘sys_config mysql.sys@localhost’ ignored in --skip-name-resolve mode.
2018-08-29T07:35:09.513301Z 0 [Note] Event Scheduler: Loaded 0 events
2018-08-29T07:35:09.514038Z 0 [Note] mysqld: ready for connections.
Version: ‘5.7.22’  socket: ‘/var/run/mysqld/mysqld.sock’  port: 3306  MySQL Community Server (GPL)从日志上看还是比较正常的。不是mysql的 是create-choerodon-databases-xxx  这个应该跟istio自动注入有关系。关掉就可以了。 启用istio自动注入，job就不跑了。。。Choerodon里有分布式锁的实现吗？有关于分布式下的事务可以参考下这两篇文章。在choerodon中提供了asgard 服务作为对应的解决方案。http://choerodon.io/zh/docs/development-guide/backend/framework/saga/ Choerodon猪齿鱼平台中的微服务数据一致性解决方案您好，看介绍觉得你们的平台很强大，功能也比较丰富。
请问，你们有在线试用的环境吗？您好！目前平台没有提供在线的使用环境，您可以通过视频等详细的了解Choerodon猪齿鱼的功能。http://choerodon.io/zh/docs/quick-start/video-tutorial/Choerodon平台版本: 0.9.0遇到问题的执行步骤:文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/
https://github.com/choerodon/kubeadm-ansible/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请提供一下inventory/vars文件内容inventory/vars 内容如下：eth0 网卡是我的机器的正确的网卡：我自己解决了，我按照官方文档上的部署章节，在上一次执行失败之后，运行了 reset 之后再次部署就正常了。Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
通过choerodon —》 应用管理 -----》创建的“应用”项目。gitlab地址无法下载fatal: Authentication failed for默认密码是 password， 你试一下使用这个密码password随便哪个账户？用password 密码和自己的密码试了，无法拉取gitlab里的代码账户需要有那个 库的权限安装脚本里面改了这个密码吗？
GITLAB_USER_PASSWORD这个修改了，所有choerodon应用管理创建的 账户默认这个密码吗？是的好的，已经可以克隆了。谢谢Choerodon平台版本：0.9.0运行环境：自主搭建问题描述：我根据官方文档的快速入门章节部署nginx-demo，但是发现猪齿鱼平台中的应用部署状态和K8S中的实际资源状态不一致，实际上K8S环境中未生成任何的Pod或者Deployment对象。
可否帮忙诊断一下是什么问题么？
谢谢麻烦在 环境总览界面截图，环境状态是否在同步中。还有提供一下devops-service和Choerodon-agent的版本可能是你这个nginx-demo应用就没有deployment，你在环境中执行 helm get nginx-demo-2c294看看这个应用中是否真的包含deployment文件。K8S 或者 Helm 没找到它执行一下kubectl get c7nhelmrelease -n dev-env你用的是我们的这套pass吗，升级之后要在环境中装crd文件，猪齿鱼平台使用的是公司的https://choerodon.com.cn/K8S 是我本地搭建的。您发的那个 helmrelease 的资源我没执行过。了解，执行一下就好了，之后我们会解决这个问题。执行的时候 namespace 有要求么？c7nhelmrelease 我已经执行了，不过看来没什么效果，而且c7n中的部署实例状态都卡在那里：
由这个实例创建在执行crd之前。并没有有创建成功。新创建的实例是不会卡住了。用spinnaker已经有段时间了。对devops流程做了很好的融合。
包括对多云的支持，以及非常强大的pipeline。
平台也是基于spring开发的。目前已经开源。可以考虑借鉴下。官网：



Spinnaker


Spinnaker
Global Continuous Delivery





Global Continuous Deliverygithub：



GitHub



spinnaker
Spinnaker is an open source, multi-cloud continuous delivery platform for releasing software changes with high velocity and confidence. - spinnaker





Spinnaker is an open source, multi-cloud continuous delivery platform for releasing software changes with high velocity and confidence. - spinnakerChoerodon平台版本：0.9.0运行环境：自主搭建问题描述：升价到0.9.0以后，重新部署应用的时候？报了个gitOPS什么不存在的问题。去数据库表里面看了下devops-project表的env_group_id为空想问下这个字段是指什么？env_group_id 是我们0.9.0 gitops的新功能，0.9.0新版本创建项目之后会多新建一个gitops的gitlab库，然后在这个项目下创环境会创一个环境的gitlab project, 然后部署实例，网络，域名都是操作这个gitlab project 里面的文件。env_group组：(创项目)
env_project项目：（创环境）
object_file: (部署实例，域名，网络)
gitops就是新建，更新，删除文件，既可以在界面上操作实例，网络，域名，也可以直接在gitlab库里面操作实例，网络，域名（按照对象的格式写对象yaml文件即可）那0.8版本创建的项目并没有创建gitOps相关的东西，怎么搞呢？删除已经运行的实例，还会报找不到gitOps的信息这是平台升级0.8-0.9,可以看下devops-service的部分
http://choerodon.io/zh/docs/installation-configuration/update/0.8-to-0.9/然后升级设计到的项目，环境很多的话，可能会造成一些数据不一致的情况，如果升级之后发现问题，及时反馈，我们这边协助您解决！这边我们有更新了一下版本，并优化了一些问题，可以先升级我们最新的版本，调取升级接口前，停掉Agent。等升级完之后，确认配置库中的生成出来的部署文件没有问题，再启动agent。停掉agent，就要停掉在环境里运行的实例，停掉实例，相应的网络配置也失效了。
然后网络的东西删不了。
就是按照这个来升级的。升级的过程中，因为要设计的到操作gitlab去新建group,新建project,新建文件，可能速度有点慢。可以通过查询devops-service数据库的devops_check_log表是否新增记录来确保是否平滑升级成功。确保升级成功之后在进行后续的操作可以直接删掉Agent环境客户端，界面上的先不用管Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：自主搭建疑问：
请问猪齿鱼后续会加入流程控制的功能 ，如引入 activity等开源的工作流引擎你好，感谢您的建议，相关的需求我们会进行整理。根据功能开发计划可能会在稍晚的版本进行开发Choerodon平台版本：0.9.0运行环境：一键安装问题描述：
应用管理----创建应用—状态一直在“创建中”choerodon-devops-service 应用报错日志：
gitlab已经生成项目：
请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：使用了一下迭代管理，到冲刺发布以后就结束了。 之前还想着是需求和feature相关之类的。迭代发布会和release分支关联在一起执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作请修改部署devops-service的环境变量值,在搭建的集群的里面使用kubectl命令编辑devops-service的deployment文件。 修改SERVICES_GATEWAY_URL变量的值，按照一键部署脚本部署的devops-service服务这个变量的值没有加上http的前缀，导致创建webhook不成功，所以创建应用一直创建中，修改这个变量值，加上http前缀即可我们将立即修改一键部署devops-service的环境变量值，为此对您造成的麻烦深感歉意可以把修改步骤发出来吗？先进到一键部署choerodon集群内, 使用ssh或者其它的方式，
然后使用kubectl 命令kubectl get namespaces嗯，进来了，执行了１.然后使用kubectl get deployment -n  xxxxx(xxxx是具体命名空间) 找到devops-service部署在那个命名空间内 例如：2 .然后使用kubectl edit deployment  a   -n  b(a代表devops-service的deployment文件名，b代表的是那个具体的命名空间)3.进去之后找到SERVICES_GATEWAY_URL变量所在的位置，变量都存放在
4.然后在按照我上面提的解决方法，修改SERVICES_GATEWAY_URL变量，加上http的前缀，修改完之后保存文件即可
SERVICES_GATEWAY_URL嗯，不需要重启容器吧？你修改了变量值之后 容器会自动重启的，会删掉以前的pod，新建新的pod嗯,已经可以了！现在创建应用可以了吗？现在创建 已经可以了
应用版本   菜单里面看不到创建的吗应用版本不是界面创建的，应用版本是需要应用代码库做了相关修改，然后触发gitlab-ci，gitlab-ci各阶段（stage）跑完之后，才会生成的, 此时choerodon平台的应用版本界面上就会显示版本了嗯，好的。这个好像有相关视频讲过。我改天在看看。谢谢你是自主搭建的choerodon环境吗你好，请提供下报错的信息
也麻烦参考下。一张截图无法提供太多有用的信息




Choerodon猪齿鱼 论坛提问规范：怎样让自己的问题及时得到解答 General


    猪齿鱼社区朋友： 
规范准确地描述问题，有利于社区成员快速地理解问题，并给出解决方案！

当您在Choerodon猪齿鱼论坛提问的时候请注意以下几点： 
1. 选择正确的“主题分类” 
目前在论坛发布“主题”，必须选择“主题分类”，请务必选择正确的“主题分类”，以便获得及时的解答。

论坛有如下分类： 
A.Installation management 

您可以提交关于Choerodon平台的…
  

Running with gitlab-runner 10.7.2 (b5e03c94)
on choerodon-runner 9b204dac
Using Kubernetes namespace: tools
Using Kubernetes executor with image registry.saas.hand-china.com/tools/pynode:test …
Waiting for pod tools/runner-9b204dac-project-334-concurrent-0bcwcw to be running, status is Pending
Running on runner-9b204dac-project-334-concurrent-0bcwcw via runner-gitlab-runner-7b87d57488-g2gbl…
Cloning repository…
Cloning into ‘/hzero-hpay/hpay-front’…
Checking out 8f986f58 as develop…
Updating/initializing submodules recursively…
$ curl -o .auto_devops.sh \ # collapsed multi-line command
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Dload  Upload   Total   Spent    Left  Speed0     0    0     0    0     0      0      0 --:–:-- --:–:-- --:–:--     0
100  3097  100  3097    0     0   130k      0 --:–:-- --:–:-- --:–:--  131k
$ node_config
/usr/local/bin/yarnpkg -> /usr/local/lib/node_modules/yarn/bin/yarn.js
/usr/local/bin/yarn -> /usr/local/lib/node_modules/yarn/bin/yarn.js希望帮忙看一下，一直重复构建都是 这个结果构建卡在 Building fresh packages… 导致超时。请检查各个依赖、代码或尝试优化构建步骤。重复尝试了接近10次，不断的retry,刚刚构建成功了，请问有什么优化方案？请根据日志信息，比如构建失败，查看失败的原因。如某个方法加载缓慢或使用了一些存在缺陷的依赖库等，具体应用你需要具体分析。麻烦看看这个构建失败的结果，重复构建几次都报这个错如果您使用了插件，第三方软件等，我们CI基础镜像中并无集成此类应用，请根据报错信息自行在CI中添加相关安装步骤。像这种情况该怎么解决提示文件或路径不存在，请检查下文件或路径可能是 https://registry.yarnpkg.com 这个域名暂时无法访问，你可以自行确定这个域名可以访问且没被GFW，确认可以正常访问后再重试。Choerodon平台版本: 0.9.0遇到问题的执行步骤: 配置Choerodon Oauth认证文档地址: http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/环境信息(如:节点信息):报错日志:
gitlab web界面无法登陆了？原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问客户端重定向地址是否填写了真实的gitlab地址？{
“accessTokenValidity”: 60,
“additionalInformation”: “”,
“authorizedGrantTypes”: “implicit,client_credentials,authorization_code,refresh_token”,
“autoApprove”: “default”,
“name”: “gitlab”,
“objectVersionNumber”: 0,
“organizationId”: 1,
“refreshTokenValidity”: 60,
“resourceIds”: “default”,
“scope”: “default”,
“secret”: “secret”,
“webServerRedirectUri”: “http://gitlab.example.choerodon.io”
}是的，改成我的真实地址了，我想问一下，重定向这个在哪保存？用一键安装git是不是已经设置好了一键安装已经设置了这个client，你可以在choerodon界面中修改client的相关信息选择一个组织–>组织设置–>客户端是的  在这里新创建一个应用一直在创建中，还不能删除。。你的 界面跟社区版 不一样啊，还多了很多功能。。。可以企业定制？这些是我们正在开发的功能，敬请关注Choerodon后续版本Choerodon平台版本: 0.9.0遇到问题的执行步骤:gitlab 认证文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/环境信息(如:节点信息):k8s报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问gitlab模板库是在创建choerodon组织的时候创建的，创建组织的人就自动会成为gitlab模板库的owner，如果想让其它用户有可以访问模板库，给其它用户在该组织下分配组织管理员即可，此时被分配的用户也成为了模板库的owner我是在之前 用gitlab用户创建的！！！现在gitlab管理员不能登录了。。Choerodon平台版本：0.6.0运行环境：一键安装问题描述：创建应用报错，一直显示在创建中请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：使用了一下迭代管理，到冲刺发布以后就结束了。 之前还想着是需求和feature相关之类的。迭代发布会和release分支关联在一起执行的操作：
创建应用报错信息(请尽量使用代码块或系统截图的形式展现)：
16:56:37.591 [Asgard-saga-consumer-1] WARN io.choerodon.asgard.saga.SagaMonitor - sagaMonitor invoke method error, transaction rollback, msg SagaTaskInstanceDTO{id=17, taskCode=‘devopsOperationGitlabProject’, sagaCode=‘devops-create-gitlab-project’, instanceLock=‘10.233.65.213:8060’}, causejava.lang.IllegalArgumentException: Illegal character in path at index 34: http://gitlab-service/v1/projects/{projectId}/protected_branches?name=master&mergeAccessLevel=40&pushAccessLevel=40&userId=3at java.net.URI.create(URI.java:852)at org.springframework.cloud.netflix.feign.ribbon.LoadBalancerFeignClient.execute(LoadBalancerFeignClient.java:56)at feign.SynchronousMethodHandler.executeAndDecode(SynchronousMethodHandler.java:97)at feign.SynchronousMethodHandler.invoke(SynchronousMethodHandler.java:76)at feign.ReflectiveFeign$FeignInvocationHandler.invoke(ReflectiveFeign.java:103)at com.sun.proxy.$Proxy212.createProtectedBranches(Unknown Source)at io.choerodon.devops.infra.persistence.impl.GitlabRepositoryImpl.createProtectBranch(GitlabRepositoryImpl.java:119)at io.choerodon.devops.app.service.impl.ApplicationServiceImpl.operationApplication(ApplicationServiceImpl.java:302)at io.choerodon.devops.api.eventhandler.DevopsSagaHandler.createApp(DevopsSagaHandler.java:88)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)at java.lang.reflect.Method.invoke(Method.java:498)at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.invoke(SagaMonitor.java:197)at io.choerodon.asgard.saga.SagaMonitor$InvokeTask.run(SagaMonitor.java:169)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)at java.lang.Thread.run(Thread.java:745)Caused by: java.net.URISyntaxException: Illegal character in path at index 34: http://gitlab-service/v1/projects/{projectId}/protected_branches?name=master&mergeAccessLevel=40&pushAccessLevel=40&userId=3at java.net.URI$Parser.fail(URI.java:2848)at java.net.URI$Parser.checkChars(URI.java:3021)at java.net.URI$Parser.parseHierarchical(URI.java:3105)at java.net.URI$Parser.parse(URI.java:3053)at java.net.URI.<init>(URI.java:588)at java.net.URI.create(URI.java:850)… 17 more提出您认为不合理的地方，帮助我们优化用户操作一直创建中…
GITLAB 已经有项目了：2018-08-27 19:43:49.358  INFO [gitlab-service,aa7dd970a486b832,aa7dd970a486b832,true] 1 — [ XNIO-3 task-22] i.c.r.h.ControllerExceptionHandler       : exception info io.choerodon.core.exception.CommonException: Bad Request
at io.choerodon.gitlab.app.service.impl.ProjectServiceImpl.createProject(ProjectServiceImpl.java:40)
at io.choerodon.gitlab.api.controller.v1.ProjectsController.create(ProjectsController.java:47)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133)
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738)
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967)
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901)
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.choerodon.resource.filter.JwtTokenFilter.doFilter(JwtTokenFilter.java:101)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:111)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:208)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:64)
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132)
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:336)
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)gitlab-service 工程报错Choerodon平台版本: 0.9.0遇到问题的执行步骤:执行test-manager-service-init-db报InvalidImageName不管是重新安装，还是升级都是你好请执行以下命令 反馈一下结果  谢谢kubectl get poimage:    //:0.9.2请问你执行安装的命令还有记录吗 ？   可以提供一下吗   方便我们排查错误   谢谢刚开始，就是根据0.8升级到0.9一步步来的，命令直接复制执行。
执行的时候，先删除这个init-db的job。
后来，升级的方式不行，又想通过直接install来做。命令也是拷贝的文档上的，改改一些域名。我觉得这个问题很明显了吧，image这个字段填的值不对你好，此BUG已修复，请执行helm repo update后再次进行升级或安装，给你带来不便，敬请谅解。helm repo update好了，谢谢Choerodon平台版本：0.9.0运行环境：一键安装问题描述：在组织中 创建用户报错(包括创建项目，用户等)请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：管理页面怎么进入，进行系统配置执行的操作：
如:已经搭建完毕,成功登陆报错信息(请尽量使用代码块或系统截图的形式展现)：提出您认为不合理的地方，帮助我们优化用户操作这个为发送事件的feign调用失败。已经好了，阿里云的 安全组 忘记加18081端口了。为啥admin 不能创建组织？在管理界面----》组织管理–界面没有创建按钮？Choerodon平台版本: 0.9.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:无原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问1、 编辑kubernetes-dashboard的service配置，添加externalIPs，并修改绑定到任意节点的ip上。修改端口为8443,并将<YOUR_HOST_IP>替换为任意节点的ip通过https://<YOUR_HOST_IP>:8443访问。如果你不在乎浏览器检测不安全情况,可以不配置证书，使用ingress-nginx-controller默认自签名证书。可以这样部署,dashboard.example.com替换为访问的域名:通过域名https://dashboard.example.com访问。集群管理员：如果只针对某个命令空间授权：如果只给某个命令空间的读pod的权限：token好的。谢谢Choerodon平台版本: 0.9.0遇到问题的执行步骤:一键安装文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):k8s报错日志:无原因分析:
一键安装脚本里面，没有看到  知识管理相关部署疑问:提出您对于遇到和解决该问题时的疑问你好，一键部署中确实没有知识管理相关部署。这是因为部署知识管理时会花费很多时间也并非Choerodon必须组件，且安装时由于服务器性能差异存在安装失败的风险。为了用户体验，我们暂时将知识管理相关部署从一键部署脚本中移除。请使用一键部署脚本部署完毕后，手动安装知识管理相关功能，在此给你带来不便，敬请谅解。额，好的。一键安装完。手动部署 就OK了。扩展master节点需要重新签发证书，重启master节点，这可能会导致整个集群进入空窗期，是十分危险的，故现在没有做这一功能的实现。可不可以告诉我具体怎么做？或者有没有什么文档可以做参考？扩展master需要对k8s集群组件有足够的了解，如果您不熟悉k8s集群不建议你扩展，下面这些文章你可以参考。Set up High-Availability Kubernetes Masters这个是daemonset
但是这个命名空间下并没有这个daemoset控制器，而且主节点没有这个pod，只有从节点有sorry, nginx-proxy 是系统应用。如果你有类似需求请使用daemonsetkubernetes有这个概念吗？怎么实现的呢？启动kubelet时会启动这个容器。在阿里云上部署的kubernetes，使用阿里云的flannel网络时出现这个错误？总共三台新的节点，两台会这样，一台不会。请问这个要怎么解决这个可能是容器网段与你呢VPC网段或路由表冲突了。
应该不是这个问题，容器网段是这个，我的vpc网段是198开头的，而且有一台没有报这样的错误
你好，根据报错信提示，你在当前VPC环境下设置的容器网段与现有路由表网段或者VPC下交换机网段确实存在冲突，请检查路由表网段或者VPC下交换机网段。如果排查后确实发现没有冲突，那么可能是阿里云的BUG，请直接与阿里云客服取得联系，让他们帮助你进行操作，谢谢。https://yq.aliyun.com/articles/74615Choerodon平台版本: 0.9.0遇到问题的执行步骤:   部署notify service 以下全部报错。。文档地址: http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/环境信息(如:节点信息):报错日志:2018-08-26 20:27:46.674  INFO [config-server,9eec3d4561a0a22e,9eec3d4561a0a22e,false] 1 — [ XNIO-3 task-32] i.c.config.DbEnvironmentRepository       : api-gateway 获取配置
2018-08-26 20:27:50.684 ERROR [config-server,] 1 — [ XNIO-3 task-32] io.undertow.request                      : UT005023: Exception handling request to /api-gateway/defaultorg.springframework.web.util.NestedServletException: Request processing failed; nested exception is feign.RetryableException: connect timed out executing GET http://manager-service/v1/services/api-gateway/configs/default
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:982) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:687) ~[javax.servlet-api-3.1.0.jar!/:3.1.0]
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) ~[javax.servlet-api-3.1.0.jar!/:3.1.0]
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:74) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) ~[spring-boot-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:111) ~[spring-boot-actuator-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:208) ~[spring-security-web-4.2.7.RELEASE.jar!/:4.2.7.RELEASE]
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177) ~[spring-security-web-4.2.7.RELEASE.jar!/:4.2.7.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:347) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:263) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:109) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:93) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186) ~[spring-cloud-sleuth-core-1.2.5.RELEASE.jar!/:1.2.5.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106) ~[spring-boot-actuator-1.5.14.RELEASE.jar!/:1.5.14.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106) ~[micrometer-spring-legacy-1.0.2.jar!/:1.0.2]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletChain$1.handleRequest(ServletChain.java:64) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:132) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46) ~[undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60) ~[undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43) ~[undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) ~[undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48) [undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43) [undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81) ~[undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104) [undertow-servlet-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:336) [undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:830) [undertow-core-1.4.25.Final.jar!/:1.4.25.Final]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]
Caused by: feign.RetryableException: connect timed out executing GET http://manager-service/v1/services/api-gateway/configs/default
at feign.FeignException.errorExecuting(FeignException.java:67) ~[feign-core-9.5.0.jar!/:na]
at feign.SynchronousMethodHandler.executeAndDecode(SynchronousMethodHandler.java:104) ~[feign-core-9.5.0.jar!/:na]
at feign.SynchronousMethodHandler.invoke(SynchronousMethodHandler.java:76) ~[feign-core-9.5.0.jar!/:na]
at feign.ReflectiveFeign$FeignInvocationHandler.invoke(ReflectiveFeign.java:103) ~[feign-core-9.5.0.jar!/:na]
at com.sun.proxy.$Proxy131.getConfig(Unknown Source) ~[na:na]
at io.choerodon.config.service.PullConfigServiceImpl.getConfig(PullConfigServiceImpl.java:31) ~[classes!/:0.8.0.RELEASE]
at io.choerodon.config.DbEnvironmentRepository.findOne(DbEnvironmentRepository.java:68) ~[classes!/:0.8.0.RELEASE]
at org.springframework.cloud.config.server.environment.EnvironmentEncryptorEnvironmentRepository.findOne(EnvironmentEncryptorEnvironmentRepository.java:53) ~[spring-cloud-config-server-1.3.3.RELEASE.jar!/:1.3.3.RELEASE]
at org.springframework.cloud.config.server.environment.EnvironmentController.labelled(EnvironmentController.java:107) ~[spring-cloud-config-server-1.3.3.RELEASE.jar!/:1.3.3.RELEASE]
at org.springframework.cloud.config.server.environment.EnvironmentController.defaultLabel(EnvironmentController.java:96) ~[spring-cloud-config-server-1.3.3.RELEASE.jar!/:1.3.3.RELEASE]
at sun.reflect.GeneratedMethodAccessor191.invoke(Unknown Source) ~[na:na]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_121]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_121]
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) ~[spring-web-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:967) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:901) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) ~[spring-webmvc-4.3.18.RELEASE.jar!/:4.3.18.RELEASE]
… 75 common frames omitted
Caused by: java.net.SocketTimeoutException: connect timed out
at java.net.PlainSocketImpl.socketConnect(Native Method) ~[na:1.8.0_121]
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[na:1.8.0_121]
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[na:1.8.0_121]
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[na:1.8.0_121]
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[na:1.8.0_121]
at java.net.Socket.connect(Socket.java:589) ~[na:1.8.0_121]
at sun.net.NetworkClient.doConnect(NetworkClient.java:175) ~[na:1.8.0_121]
at sun.net.www.http.HttpClient.openServer(HttpClient.java:432) ~[na:1.8.0_121]
at sun.net.www.http.HttpClient.openServer(HttpClient.java:527) ~[na:1.8.0_121]
at sun.net.www.http.HttpClient.(HttpClient.java:211) ~[na:1.8.0_121]
at sun.net.www.http.HttpClient.New(HttpClient.java:308) ~[na:1.8.0_121]
at sun.net.www.http.HttpClient.New(HttpClient.java:326) ~[na:1.8.0_121]
at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202) ~[na:1.8.0_121]
at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138) ~[na:1.8.0_121]
at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032) ~[na:1.8.0_121]
at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966) ~[na:1.8.0_121]
at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1546) ~[na:1.8.0_121]
at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474) ~[na:1.8.0_121]
at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_121]
at feign.Client$Default.convertResponse(Client.java:152) ~[feign-core-9.5.0.jar!/:na]
at feign.Client$Default.execute(Client.java:74) ~[feign-core-9.5.0.jar!/:na]
at org.springframework.cloud.sleuth.instrument.web.client.feign.TraceFeignClient.execute(TraceFeignClient.java:92) ~[spring-cloud-sleuth-core-1.2.5.RELEASE.jar!/:1.2.5.RELEASE]
at org.springframework.cloud.netflix.feign.ribbon.RetryableFeignLoadBalancer$1.doWithRetry(RetryableFeignLoadBalancer.java:92) ~[spring-cloud-netflix-core-1.3.5.RELEASE.jar!/:1.3.5.RELEASE]
at org.springframework.cloud.netflix.feign.ribbon.RetryableFeignLoadBalancer$1.doWithRetry(RetryableFeignLoadBalancer.java:77) ~[spring-cloud-netflix-core-1.3.5.RELEASE.jar!/:1.3.5.RELEASE]
at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:287) ~[spring-retry-1.2.2.RELEASE.jar!/:na]
at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:164) ~[spring-retry-1.2.2.RELEASE.jar!/:na]
at org.springframework.cloud.netflix.feign.ribbon.RetryableFeignLoadBalancer.execute(RetryableFeignLoadBalancer.java:77) ~[spring-cloud-netflix-core-1.3.5.RELEASE.jar!/:1.3.5.RELEASE]
at org.springframework.cloud.netflix.feign.ribbon.RetryableFeignLoadBalancer.execute(RetryableFeignLoadBalancer.java:48) ~[spring-cloud-netflix-core-1.3.5.RELEASE.jar!/:1.3.5.RELEASE]
at com.netflix.client.AbstractLoadBalancerAwareClient$1.call(AbstractLoadBalancerAwareClient.java:109) ~[ribbon-loadbalancer-2.2.2.jar!/:2.2.2]
at com.netflix.loadbalancer.reactive.LoadBalancerCommand$3$1.call(LoadBalancerCommand.java:303) ~[ribbon-loadbalancer-2.2.2.jar!/:2.2.2]
at com.netflix.loadbalancer.reactive.LoadBalancerCommand$3$1.call(LoadBalancerCommand.java:287) ~[ribbon-loadbalancer-2.2.2.jar!/:2.2.2]
at rx.internal.util.ScalarSynchronousObservable$3.call(ScalarSynchronousObservable.java:231) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.util.ScalarSynchronousObservable$3.call(ScalarSynchronousObservable.java:228) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.Observable.unsafeSubscribe(Observable.java:10211) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeConcatMap$ConcatMapSubscriber.drain(OnSubscribeConcatMap.java:286) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeConcatMap$ConcatMapSubscriber.onNext(OnSubscribeConcatMap.java:144) ~[rxjava-1.1.10.jar!/:1.1.10]
at com.netflix.loadbalancer.reactive.LoadBalancerCommand$1.call(LoadBalancerCommand.java:185) ~[ribbon-loadbalancer-2.2.2.jar!/:2.2.2]
at com.netflix.loadbalancer.reactive.LoadBalancerCommand$1.call(LoadBalancerCommand.java:180) ~[ribbon-loadbalancer-2.2.2.jar!/:2.2.2]
at rx.Observable.unsafeSubscribe(Observable.java:10211) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeConcatMap.call(OnSubscribeConcatMap.java:94) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeConcatMap.call(OnSubscribeConcatMap.java:42) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.Observable.unsafeSubscribe(Observable.java:10211) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OperatorRetryWithPredicate$SourceSubscriber$1.call(OperatorRetryWithPredicate.java:127) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.schedulers.TrampolineScheduler$InnerCurrentThreadScheduler.enqueue(TrampolineScheduler.java:73) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.schedulers.TrampolineScheduler$InnerCurrentThreadScheduler.schedule(TrampolineScheduler.java:52) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OperatorRetryWithPredicate$SourceSubscriber.onNext(OperatorRetryWithPredicate.java:79) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OperatorRetryWithPredicate$SourceSubscriber.onNext(OperatorRetryWithPredicate.java:45) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.util.ScalarSynchronousObservable$WeakSingleProducer.request(ScalarSynchronousObservable.java:276) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.Subscriber.setProducer(Subscriber.java:209) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.util.ScalarSynchronousObservable$JustOnSubscribe.call(ScalarSynchronousObservable.java:138) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.util.ScalarSynchronousObservable$JustOnSubscribe.call(ScalarSynchronousObservable.java:129) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:48) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:30) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:48) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:30) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:48) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:30) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.Observable.subscribe(Observable.java:10307) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.Observable.subscribe(Observable.java:10274) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.observables.BlockingObservable.blockForSingle(BlockingObservable.java:445) ~[rxjava-1.1.10.jar!/:1.1.10]
at rx.observables.BlockingObservable.single(BlockingObservable.java:342) ~[rxjava-1.1.10.jar!/:1.1.10]
at com.netflix.client.AbstractLoadBalancerAwareClient.executeWithLoadBalancer(AbstractLoadBalancerAwareClient.java:117) ~[ribbon-loadbalancer-2.2.2.jar!/:2.2.2]
at org.springframework.cloud.netflix.feign.ribbon.LoadBalancerFeignClient.execute(LoadBalancerFeignClient.java:63) ~[spring-cloud-netflix-core-1.3.5.RELEASE.jar!/:1.3.5.RELEASE]
at feign.SynchronousMethodHandler.executeAndDecode(SynchronousMethodHandler.java:97) ~[feign-core-9.5.0.jar!/:na]
… 95 common frames omitted原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，可以描述一下是在什么情况下出的错？比如:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-devops/执行 helm install c7n/gitlab-service…
安装等了很久就出现Error: Job failed: DeadlineExceeded
错误部署的时候 跟踪新启动容器的日志大哥，看清楚问题先。启动都没有启来。怎么跟踪？当你执行部署命令后 会启动一个新容器执行初始化任务，跟踪这个容器的日志。你们就不能弄个Q群或者微信群吗？安装搞这么复杂。。。我也是醉了！！！如果严格按照安装流程操作，安装并不困难，但如果你忽略了某些步骤会导致安装失败，失败后你可能需要对数据进行还原等操作。
如果想快速安装，建议使用一键安装脚本 。 为了让更多的人能够搜索到遇到问题，我们暂时不提供其他方式支持。一键安装，我按你们的方式部署K8S，网络环境，存储。跑了3次都没有成功。。。手动安装了2次了。你自己看论坛，这两天基本上都是我提的问题。。。在弄不好估计要放弃你们这个平台了。。。rceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-26 23:39:54.821  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaMonitorClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-26 23:39:54.825  INFO [agile-service,] 1 — [pool-2-thread-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@1c7efd6c: startup date [Sun Aug 26 23:39:54 CST 2018]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2893de87
2018-08-26 23:39:54.830  WARN [agile-service,] 1 — [pool-2-thread-1] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPing类似这种错，不是自己写的代码，很难去排查。。。Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
一直还停留在安装环境上，各种报错，各种问题无法解决，用的腾讯云的主机，是不是腾讯云主机或者是他们的镜像有问题啊 ，你们有推荐的服务商么？疑问:提出您对于遇到和解决该问题时的疑问你选择的什么操作系统呢？主机都是这种配置
安装环境是指安装Choerodon还是 kubernetes安装Choerodon过程中报错，一直过不去我们正在优化一键安装脚本以便更容易的找到问题Choerodon平台版本: 0.9.0遇到问题的执行步骤:在访问搭建好的Choerodon的api文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，请提出具体的问题。比如什么参数？哪个接口？步骤是什么？Choerodon平台版本: 0.9.0遇到问题的执行步骤:在访问搭建好的Choerodon的api文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/环境信息(如:节点信息):报错日志:Choerodon平台版本: 0.9.0遇到问题的执行步骤: 部署zookeeper文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/zookeeper/环境信息(如:节点信息):报错日志:
2018-08-25 17:33:30,464 [myid:3] - INFO  [WorkerSender[myid=3]:QuorumPeer$QuorumServer@167] - Resolved hostname: zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local to address: zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.65.9
2018-08-25 17:33:35,469 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumCnxManager@588] - Cannot open channel to 1 at election address zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.65.9:3888
java.net.SocketTimeoutException: connect timed out
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
at java.net.Socket.connect(Socket.java:589)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)原因分析:
ping zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.localPING zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local (10.233.65.9) 56(84) bytes of data.64 bytes from 10.233.65.9 (10.233.65.9): icmp_seq=1 ttl=63 time=0.172 ms64 bytes from 10.233.65.9 (10.233.65.9): icmp_seq=2 ttl=63 time=0.207 ms64 bytes from 10.233.65.9 (10.233.65.9): icmp_seq=3 ttl=63 time=0.201 ms64 bytes from 10.233.65.9 (10.233.65.9): icmp_seq=4 ttl=63 time=0.189 mssvc域名可ping通。。。。zookeeper 可以启动成功，但是不能连接。查看日志报错上面。。看看flannel中有没有报错4个flannel 节点都没有报错~~~2018-08-25 18:23:22,120 [myid:1] - WARN  [QuorumPeer[myid=1]/0.0.0.0:2181:QuorumCnxManager@588] - Cannot open channel to 3 at election address zookeeper-2.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.66.11:3888
java.net.SocketTimeoutException: connect timed out
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
at java.net.Socket.connect(Socket.java:589)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2018-08-25 18:23:22,121 [myid:1] - INFO  [QuorumPeer[myid=1]/0.0.0.0:2181:QuorumPeer$QuorumServer@167] - Resolved hostname: zookeeper-2.zookeeper-headless.jmsw-devops.svc.cluster.local to address: zookeeper-2.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.66.11
2018-08-25 18:23:22,121 [myid:1] - INFO  [QuorumPeer[myid=1]/0.0.0.0:2181:FastLeaderElection@852] - Notification time out: 800zookeeper 集群一直报这个错，疯了。。。你可以尝试把现有的flannel pod都删除一下看下我这个写法对不对：
net-conf.json: |
修改：
{
“Network”: “10.233.64.0/18”,
“Backend”: {
“Type”: “ali-vpc”
}
}原来的：
{
“Network”: “[PodsSubnet]”,
“Backend”: {
“Type”: “ali-vpc”
}
}提提示连不上zookeeper-2，你应该先检查zookeeper-2这个pod是否有报错2018-08-26 04:35:18,888 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer$QuorumServer@167] - Resolved hostname: zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local to address: zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.65.10
2018-08-26 04:35:18,888 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:FastLeaderElection@852] - Notification time out: 60000
2018-08-26 04:36:23,888 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumCnxManager@588] - Cannot open channel to 1 at election address zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.64.11:3888
java.net.SocketTimeoutException: connect timed out
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
at java.net.Socket.connect(Socket.java:589)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2018-08-26 04:36:23,889 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer$QuorumServer@167] - Resolved hostname: zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local to address: zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.64.11
2018-08-26 04:36:28,890 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumCnxManager@588] - Cannot open channel to 2 at election address zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.65.10:3888都说了，整个zk集群都在报错啊进入容器中查看这三个端口是否已启动刚刚看了，端口没有起来… k8S集群 NFS 都是按你们的文档部署的。。部署搞了几天。真的是无语。。。我刚刚把 zookeeper 参数设成 --set persistence.enabled=false 也是一样报错既然容器中的对应端口没有启动，自然无法连接，你是否部署过多次zookeeper?我都重装linux 系统试过 还是一样也不行。。。是否清空了NFS呢, zookeeper如果使用了其他节点的数据是会启动失败的。有清空的，我没用NFS安装 也是报一样的。尝试下面步骤：
1.先执行下面命令停止 zookeeper2.删除NFS中所有zookeeper的数据（非常重要,并非删除pv和pvc）
3.重启flannel4.重启zookeeper一样报错：
2018-08-26 09:21:00,052 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer$QuorumServer@167] - Resolved hostname: zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local to address: zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.67.6
2018-08-26 09:21:05,054 [myid:3] - WARN  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumCnxManager@588] - Cannot open channel to 2 at election address zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.65.8:3888
java.net.SocketTimeoutException: connect timed out
at java.net.PlainSocketImpl.socketConnect(Native Method)
at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
at java.net.Socket.connect(Socket.java:589)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:562)
at org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:614)
at org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:843)
at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:913)
2018-08-26 09:21:05,054 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:QuorumPeer$QuorumServer@167] - Resolved hostname: zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local to address: zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.65.8
2018-08-26 09:21:05,054 [myid:3] - INFO  [QuorumPeer[myid=3]/0.0.0.0:2181:FastLeaderElection@852] - Notification time out: 256003个容器都是一样。。ECS安全组是否设置了呢？安全组 要开2181，2888，3888 端口吗参考文档添加即可
Choerodon平台版本: 0.9.0遇到问题的执行步骤:kubectl apply -f kube-flannel-aliyun.yml文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):k8s报错日志:
error: error validating “kube-flannel-aliyun.yml”: error validating data: [ValidationError(DaemonSet.spec.template.spec.containers[0].env[2].value): invalid type for io.k8s.api.core.v1.EnvVar.value: got “array”, expected “string”, ValidationError(DaemonSet.spec.template.spec.containers[0].env[3].value): invalid type for io.k8s.api.core.v1.EnvVar.value: got “array”, expected “string”]; if you choose to ignore these errors, turn validation off with --validate=false原因分析:
yml 文件已修改需要修改的参数。Network ：为pod网段。ACCESS_KEY_ID ：必填ACCESS_KEY_SECRET ：必填疑问:
如何解决？如果这个是阿里云主机配置的项，如果你不是阿里云主机，你使用默认的网络即可,
ACCESS_KEY_ID , ACCESS_KEY_SECRET 你可以在阿里云控制台中获取是阿里云主机，就是按流程去配置的。这个错误怎么处理？这里的值是字符串 value:  xxx， 这里不要添加括号嗯，可以了。有外部域名，dns是不是直接解析到K8S管理服务器的公网IP就可以了吗？是的[2018-08-25 15:16:53,934] INFO Client environment:os.version=3.10.0-693.2.2.el7.x86_64 (org.apache.zookeeper.ZooKeeper)[2018-08-25 15:16:53,934] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)[2018-08-25 15:16:53,934] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)[2018-08-25 15:16:53,934] INFO Client environment:user.dir=/opt/kafka (org.apache.zookeeper.ZooKeeper)[2018-08-25 15:16:53,935] INFO Initiating client connection, connectString=zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.jmsw-devops.svc.cluster.local:2181 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@245b4bdc (org.apache.zookeeper.ZooKeeper)[2018-08-25 15:16:53,945] INFO Waiting for keeper state SyncConnected (org.I0Itec.zkclient.ZkClient)[2018-08-25 15:16:53,946] INFO Opening socket connection to server zookeeper-2.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.64.2:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:53,950] INFO Socket connection established to zookeeper-2.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.64.2:2181, initiating session (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:53,952] INFO Unable to read additional data from server sessionid 0x0, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:54,860] INFO Opening socket connection to server zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.66.6:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:56,861] WARN Client session timed out, have not heard from server in 2808ms for sessionid 0x0 (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:56,862] INFO Client session timed out, have not heard from server in 2808ms for sessionid 0x0, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:57,523] INFO Opening socket connection to server zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local/10.233.65.6:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:59,524] WARN Client session timed out, have not heard from server in 2562ms for sessionid 0x0 (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:59,524] INFO Client session timed out, have not heard from server in 2562ms for sessionid 0x0, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:16:59,946] INFO Terminate ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)[2018-08-25 15:17:00,338] INFO Session: 0x0 closed (org.apache.zookeeper.ZooKeeper)[2018-08-25 15:17:00,339] FATAL Fatal error during KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server ‘zookeeper-0.zookeeper-headless.jmsw-devops.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.jmsw-devops.svc.cluster.local:2181’ with timeout of 6000 msat org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:1233)at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:157)at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:131)at kafka.utils.ZkUtils$.createZkClientAndConnection(ZkUtils.scala:115)at kafka.utils.ZkUtils$.withMetrics(ZkUtils.scala:92)at kafka.server.KafkaServer.initZk(KafkaServer.scala:346)at kafka.server.KafkaServer.startup(KafkaServer.scala:194)at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:38)at kafka.Kafka$.main(Kafka.scala:92)at kafka.Kafka.main(Kafka.scala)[2018-08-25 15:17:00,340] INFO EventThread shut down for session: 0x0 (org.apache.zookeeper.ClientCnxn)[2018-08-25 15:17:00,341] INFO shutting down (kafka.server.KafkaServer)[2018-08-25 15:17:00,345] INFO shut down completed (kafka.server.KafkaServer)[2018-08-25 15:17:00,345] FATAL Exiting Kafka. (kafka.server.KafkaServerStartable)[2018-08-25 15:17:00,346] INFO shutting down (kafka.server.KafkaServer)安装kafka 连接不了zk
telnet zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local:2181telnet: zookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local:2181: Name or service not knownzookeeper-1.zookeeper-headless.jmsw-devops.svc.cluster.local:2181: Unknown hostChoerodon平台版本: 0.9.1遇到问题的执行步骤:
部署服务文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-devops/环境信息(如:节点信息):
Readiness probe failed: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed
0 0 0 0 0 0 0 0 --:–:-- --:–:-- --:–:-- 0curl: (7) Failed to connect to localhost port 8061: Connection refused报错日志:text_fields timer_off refresh exposure_zero file_download10:17:47.453 [pool-2-thread-1] WARN io.choerodon.asgard.saga.SagaMonitor - sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPing10:17:48.457 [pool-2-thread-1] WARN o.s.b.f.s.DefaultListableBeanFactory - Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.devops.infra.feign.IamServiceClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，可以将你运行的镜像名发一下吗，同时麻烦使用代码块贴一下完整的堆栈信息Choerodon平台版本：0.9.1运行环境(如localhost或k8s)：阿里云遇到问题时的前置条件：问题描述：
agile-service 8379 端口无法启来
Readiness probe failed: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed
0 0 0 0 0 0 0 0 --:–:-- --:–:-- --:–:-- 0curl: (7) Failed to connect to localhost port 8379: Connection refused2018-08-25 10:08:44.704  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaMonitorClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:44.707  INFO [agile-service,] 1 — [pool-2-thread-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@65f24686: startup date [Sat Aug 25 10:08:44 CST 2018]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2893de87
2018-08-25 10:08:44.711  WARN [agile-service,] 1 — [pool-2-thread-1] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPing
2018-08-25 10:08:45.714  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.agile.infra.feign.UserFeignClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:45.714  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.agile.infra.feign.FileFeignClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:45.715  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.producer.execute.EventStoreClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:45.715  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.consumer.FailedMsgEventStoreFeign’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:45.715  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:45.715  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaMonitorClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:45.718  INFO [agile-service,] 1 — [pool-2-thread-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@1ec18492: startup date [Sat Aug 25 10:08:45 CST 2018]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2893de87
2018-08-25 10:08:45.722  WARN [agile-service,] 1 — [pool-2-thread-1] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPing
2018-08-25 10:08:46.727  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.agile.infra.feign.UserFeignClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:46.727  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.agile.infra.feign.FileFeignClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:46.728  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.producer.execute.EventStoreClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:46.728  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.consumer.FailedMsgEventStoreFeign’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:46.728  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:46.729  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaMonitorClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:46.732  INFO [agile-service,] 1 — [pool-2-thread-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@4dde01f0: startup date [Sat Aug 25 10:08:46 CST 2018]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2893de87
2018-08-25 10:08:46.738  WARN [agile-service,] 1 — [pool-2-thread-1] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPing
2018-08-25 10:08:47.741  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.agile.infra.feign.UserFeignClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:47.741  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.agile.infra.feign.FileFeignClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:47.742  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.producer.execute.EventStoreClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:47.742  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.event.consumer.FailedMsgEventStoreFeign’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:47.742  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:47.742  WARN [agile-service,] 1 — [pool-2-thread-1] o.s.b.f.s.DefaultListableBeanFactory     : Bean creation exception on non-lazy FactoryBean type check: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘io.choerodon.asgard.saga.feign.SagaMonitorClient’: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘org.springframework.transaction.config.internalTransactionAdvisor’ defined in class path resource [org/springframework/transaction/annotation/ProxyTransactionManagementConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.transaction.interceptor.BeanFactoryTransactionAttributeSourceAdvisor]: Factory method ‘transactionAdvisor’ threw exception; nested exception is java.lang.NullPointerException
2018-08-25 10:08:47.745  INFO [agile-service,] 1 — [pool-2-thread-1] s.c.a.AnnotationConfigApplicationContext : Refreshing org.springframework.context.annotation.AnnotationConfigApplicationContext@181cbdcb: startup date [Sat Aug 25 10:08:47 CST 2018]; parent: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@2893de87
2018-08-25 10:08:47.749  WARN [agile-service,] 1 — [pool-2-thread-1] io.choerodon.asgard.saga.SagaMonitor     : sagaMonitor poll error Error processing condition on org.springframework.cloud.netflix.ribbon.eureka.EurekaRibbonClientConfiguration.ribbonPing原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问你好，可以将你运行的镜像名发一下吗，同时麻烦使用代码块贴一下完整的堆栈信息Choerodon平台版本: 0.9.0遇到问题的执行步骤: 添加Gitlab Client文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问参考文档中添加即可, organization_id与DTO中的一致Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：公司猪齿鱼平台遇到问题时的前置条件：问题描述：不能删除，没有容器，升级前好好的；升级后，部署实例全是bug；麻烦帮忙看看；很急!!!你好，你先在你们k8s集群 apply一下这个文件https://raw.githubusercontent.com/choerodon/choerodon-agent/master/scripts/crd.yaml，执行之后，你进你这个环境的配置库中的部署文件，怎么apply，有具体的操作步骤吗？怎么保证我原来k8s上的容器不受影响？能帮忙看看吗？现在实例都用不了apply就行了 没有具体步骤 就一步操作kubectl apply -fChoerodon平台版本: 0.9.0遇到问题的执行步骤: 部署choerodon test manager front文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-test-manager/环境信息(如:节点信息):报错日志:
rror: failed to download “c7n/choerodon-front-test-manager”原因分析:
这个文档写着 0.9.2 版本，你们服务器上又没有。–version=0.9.2，检查一下手动作安装文档啊，好几个–version=xxx 写着又报错，换低版本又没有报错。疑问:提出您对于遇到和解决该问题时的疑问这个版本是正确的，安装之前你是否执行过？肯定有的，用0.8.0 就可以了。0.9.0 以上就报错刚刚重新同步了chart,你再update一次。谢谢Failed to apply default image tag “//:0.9.2”: couldn’t parse image reference “//:0.9.2”: invalid reference formatError: InvalidImageName还是报错啊请问是哪一步报的错？执行helm install 等了一会就报这个了请问是 c7n/choerodon-front-test-manager 这个服务吗麻烦在命令最后添加  --debug --dry-run 粘贴完整的输出内容。测试管理 的两个helm都一样[debug] Created tunnel using local port: ‘45453’[debug] SERVER: “127.0.0.1:45453”[debug] Original chart version: “0.9.2”[debug] Fetched c7n/choerodon-front-test-manager to /root/.helm/cache/archive/choerodon-front-test-manager-0.9.2.tgz[debug] CHART PATH: /root/.helm/cache/archive/choerodon-front-test-manager-0.9.2.tgzError: a release named jmsw-front-test-manager already exists.Run: helm ls --all jmsw-front-test-manager; to check the status of the releaseOr run: helm del --purge jmsw-front-test-manager; to delete it安装提示先删除，然后再执行上述命令helm del --purge jmsw-front-test-manager有yaml 文件出来了，安装还是报错。。。能否粘贴下 yaml的具体内容呢[debug] Created tunnel using local port: ‘41026’[debug] SERVER: “127.0.0.1:41026”[debug] Original chart version: “0.9.2”
[debug] Fetched c7n/choerodon-front-test-manager to /root/.helm/cache/archive/choerodon-front-test-manager-0.9.2.tgz[debug] CHART PATH: /root/.helm/cache/archive/choerodon-front-test-manager-0.9.2.tgzNAME:   jmsw-front-test-manager
REVISION: 1
RELEASED: Sat Aug 25 11:58:51 2018
CHART: choerodon-front-test-manager-0.9.2
USER-SUPPLIED VALUES:
env:
open:
PRO_API_HOST: api.xxxx
PRO_CLIENT_ID: test-manager
PRO_HEADER_TITLE_NAME: Jmsw
PRO_HTTP: http
PRO_TITLE_NAME: Jmsw
ingress:
enable: true
host: test-manager.xxxxx
preJob:
preConfig:
mysql:
dbname: iam_service
host: jmsw-mysql
password: Jmsw@2018
port: 3306
username: jmsw
service:
enable: trueCOMPUTED VALUES:
env:
open:
PRO_API_HOST: api.xxxx
PRO_CLIENT_ID: test-manager
PRO_COOKIE_SERVER: devops.choerodon.example.com
PRO_HEADER_TITLE_NAME: Jmsw
PRO_HTTP: http
PRO_LOCAL: true
PRO_TITLE_NAME: Jmsw
image:
pullPolicy: Always
repository: registry.choerodon.com.cn/choerodon-c7ntest/choerodon-front-test-manager
ingress:
enable: true
host: test-manager.xxxx
logs:
parser: nginx
metrics:
group: nginx
path: /prometheus
preJob:
preConfig:
mysql:
dbname: iam_service
host: jmsw-mysql
password: Jmsw@2018
port: 3306
username: jmsw
replicaCount: 1
resources:
limits: null
requests: null
service:
enable: true
port: 80
type: ClusterIPapiVersion: batch/v1
kind: Job
metadata:
name: jmsw-front-test-manager-init-config
labels:
choerodon.io/release: “jmsw-front-test-manager”
annotations:
“helm.sh/hook”: pre-install,pre-upgrade
“helm.sh/hook-weight”: “2”
spec:
backoffLimit: 1
activeDeadlineSeconds: 120
template:
metadata:
name: jmsw-front-test-manager-init-config
spec:
containers:
- name: jmsw-front-test-manager-init-config
image: “registry.choerodon.com.cn/choerodon-c7ntest/choerodon-front-test-manager:0.9.2”
imagePullPolicy: Always
env:
- name: DB_HOST
value: jmsw-mysql
- name: DB_PORT
value: “3306”
- name: DB_USER
value: jmsw
- name: DB_PASS
value: Jmsw@2018
- name: DB_NAME
value: iam_service
command:
- /bin/sh
- -c
- ’
cd /usr/share/nginx/html;
python sql.py;
python dashboard.py -o sql
’
restartPolicy: Never
MANIFEST:apiVersion: extensions/v1beta1
kind: Ingress
metadata:
name: jmsw-front-test-manager
labels:
choerodon.io/release: “jmsw-front-test-manager”
spec:
rules:这个文件中image都是正确的 不应该出现 Failed to apply default image tag “//:0.9.2”,  你的helm和tailer是2.8.2版本吗？helm versionClient: &version.Version{SemVer:“v2.8.2”, GitCommit:“a80231648a1473929271764b920a8e346f6de844”, GitTreeState:“clean”}Server: &version.Version{SemVer:“v2.8.2”, GitCommit:“a80231648a1473929271764b920a8e346f6de844”, GitTreeState:“clean”}Choerodon平台版本: 0.6.0遇到问题的执行步骤:helm install manager service 服务文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/环境信息(如:节点信息):报错日志:
start: trying and failing to pull image原因分析:
0.9.0 版本服务器挂了？疑问:检查你服务器的网络，你可以手动在服务器上拉取镜像Choerodon平台版本: 0.8遇到问题的执行步骤: 验证部署文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/chartmuseum/环境信息(如:节点信息):k8s报错日志:
无法访问此网站原因分析:
域名已解析的k8s集群管理服务器IP地址疑问:
还是无法访问直接访问ip地址返回什么呢？无法访问 该网站 是还需要安装什么跳转到容器里面？请检查ip是否正确，云服务商是否有安全策略禁止了访问。Choerodon平台版本: 0.6.0遇到问题的执行步骤:
sh choerodon-install.sh values.sh文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
一直卡在choerodon-iam-service，一直显示这个not ready，持续几个小时，等半天就终止了！重新执行还是报这个错查看日志如下:
疑问:提出您对于遇到和解决该问题时的疑问这个要怎么排查是什么问题导致一直卡在choerodon-iam-service呢。kubectl get svc -n [NAMESPACE] 看看有没有 config-server同样的问题，config-server正常kubectl describe svc config-server -n [NAMESPACE] 看下详细信息。
没有发现问题再部署一遍 choerodon-iam-service 查看下 初始化的日志这个问题和前面问题没有关系，请你在相应的主题下提问，谢谢。好的，主要是这个问题又出现了，无法重现上一个问题执行后，截图如下
是否有重新下边的错误呢
重装了无数次了，还是报这个错呢Choerodon猪齿鱼是一个开源企业服务平台，是基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理的开源平台，同时提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，致力帮助企业聚焦于业务，加速数字化转型。2018年8月24日 ，Choerodon猪齿鱼发布 0.9 版本，本次更新对知识管理、敏捷管理、持续交付等各项服务增加了新的功能，并对一些功能细节做了进一步优化，欢迎各位更新体验，同时特别感谢社区中的朋友给Choerodon猪齿鱼提出的诸多中肯意见。发布版本：0.9发布时间：2018年8月24日功能范围：知识管理、敏捷管理、持续交付、测试管理以及微服务开发框架下面就为大家带来详细的版本更新介绍！01 知识管理知识管理界面添加了多语言支持和按钮权限用户登录知识管理系统时自动同步Choerodon平台上的用户基本信息知识管理系统编辑器添加了Markdown语法同时，知识管理页面添加删除空间的功能，方便对空间进行管理。02 敏捷管理敏捷管理服务新增了迭代速度图、史诗报告、统计图三个报告，并且可以对版本、史诗进行拖动排序，详情如下：除此之外，敏捷管理服务新增了agile-service基于Spock编写的单元测试，并在问题详情中添加创建分支功能，若用户修改问题状态为已完成时，会自动生成该状态下的问题解决日志。03 持续交付持续交付增加如下的功能：持续交付采用GitOps模型重构,持续交付环境流水线中，各个环境增加存放k8s部署文件的git库，部署相关操作时先通过操作部署文件git库，再触发环境客户端执行。Git库中文件的状态既是环境中实际运行应用的状态。通过GitOps，可以轻松使对象进行恢复和迁移。环境总览页面包含了某个环境内所有与应用部署相关的详情，其中主要包括了该环境中各应用实例的具体信息，如：实例状态、名称、应用版本、实例的各种容器信息、日志、网络以及域名的相关信息。所属项目成员均能通过环境总览页面直观的获取到上述信息，部署管理员能在此页面对所选的某个环境下与部署相关的实体进行管理和操作。总的来说，环境总览既是某个所选环境的快捷管理入口，又是此环境的状态显示器。安装实例插入相关平台标签新增前端API测试增加环境总览相关接口部署时自动给应用实例中k8s对象插入标签,应用chart中不需要在部署文件中额外添加微服务、日志等其他平台标签部署支持应用chart中存在依赖关系的复杂chart应用04 测试管理测试管理此次主要增加以下几个功能：增加循环导出功能，用户可将循环的内容导出为excel增加循环跨版本克隆功能，用户可将测试循环复制到其他版本中复用增加仪表盘展示界面增加部分单元测试和部分API测试增加了创建测试用例时的名称校验用例详情中的执行记录中增加循环转跳，用户可在用例详情中的执行表格中直接转跳增加用例管理的默认搜索，不需要先选择字段再进行选择了关联缺陷时支持转跳，方便新建缺陷另外，此次更新界面增加了多种言功能,可以配合平台进行多语言切换；问题编号增加了转跳，用户不必切换到敏捷界面查看缺陷；循环详情界面增加人员筛选功能，用户可筛选指派人或执行方。05 微服务开发框架微服务开发框架增加了如下的功能：新增事务定义，开发者能更好的实现分布式事务，避免了分布式场景下产生数据不一致的问题新增事务实例，开发者可以查看所有运行的事务，可以查看事务中任务的状态以及状态详情新增角色标签，平台管理员可以查看标签的说明与层级，角色标签只能在同层级的角色中添加新增仪表盘，用户能在不同的层级使用仪表盘实现信息概览新增仪表盘配置，平台管理员能设置用于展示的仪表盘卡片新增邮件模板，平台管理员和组织管理员可定义发送给用户的邮件内容新增邮箱配置，平台管理员可设置发件邮箱的信息新增用户批量导入，组织管理员可以下载导入模板，填写后上传文件批量导入用户数据新增UI组件，开发者可以引用UI组件，快速进行前端开发01 知识管理修改了知识管理界面的空间列表显示内容修改了知识管理系统站点favicon修改系统空间首页内容和布局修改知识管理系统侧边栏、人员信息页、创建页面入口页修改了知识管理系统通知弹出框的显示内容优化了知识管理系统页面的加载性能和使用oauth认证的性能知识管理页面的空间列表可显示树形空间结构优化了创建空间过程对用户的状态提示优化了所有更新页的显示删除了知识管理系统头部导航栏的人员列表按钮和系统中创建空间的功能02 敏捷管理agile-service消息机制由Kafka修改为Saga优化了版本报告图和燃尽图请求时间过长，待办事项界面中史诗和版本加载过慢的问题调整了待办事项界面样式和版本状态样式重构了日志处理逻辑03 持续交付重写部署实例values，支持标准yml格式网络多端口支持，label selector 支持，NodePort类型支持修改网络界面通过填写标签创建网络增加网络的 NodePort 类型配置移除网络关联的应用版本未修改配置信息不可重新部署优化各模块数据加载效果修改容器日志选择背景色，和非编辑状态不可复制04 测试管理优化了报表、测试循环、测试步骤、缺陷等查询接口事件消息改为saga模式执行详情和用例管理中测试步骤可表格内编辑，降低操作成本测试状态图标样式变更测试摘要页面接口整合优化用例管理页面增加展示内容，排序去掉多余字段优化报表页面布局，列宽不会因为展开变动05 微服务开发框架API测试优化为在界面上即可进行API测试操作，输入测试数据并查看结果，且支持其他账号的授权。修改asgard服务ci，deploy依赖，修改chart部署服务。asgard服务优化，taskInstance的返回值由map json修改为json。API测试中，后端解析dto中的注释，将注释显示在界面。页面优化为第一个input框自动获取光标。01 知识管理修复系统使用https时，回调地址错误的问题修复用户在知识管理系统中无法退出登录的问题修复了空间名字中带有"."的时候，空间显示错误的问题修复了系统设置按钮的权限判断错误的问题02 敏捷管理修复待办事项界面内存溢出问题修复燃尽图数、累积流图数据不一致问题修复模块管理创建模块后数据展示不一致问题03 测试管理修复测试循环和步骤分页显示问题修复删除测试用例后的计数不会级联删除的问题修复删除执行后的页面不会全局自动刷新的问题修复执行详情界面宽度兼容错误导致看不到编辑按钮的问题修复了报表的分页数据错误的问题04 微服务开发框架修复重新部署oauth后要清缓存才能登录的问题。修复切换组织/项目后，菜单面板没有收起的问题。修复手机登录页重定向问题。修复choerodonui国际化显示不正确的问题。修复LDAP同步用户，无法全部同步的问题。修复实例管理部分服务没有配置信息的问题。修复解析权限的时候，可能报重复字段，导致插入失败的问题。修复角色分配界面，移除用户角色可能不发送data的问题。更加详细的内容，请参阅Release Notes和官网。欢迎通过Choerodon的GitHub和猪齿鱼社区进行反馈与贡献，感谢各位朋友陪伴Choerodon猪齿鱼不断成长，Choerodon会持续优化，敬请期待。Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：有时候会pending很久，然后报错403多点击几次其他菜单，然后再点击这里，就又显示正常。这个问题不仅在截图那个地方出现，好几处其他地方也会出现类似问题。Choerodon平台版本：0.9.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问没有注册中心，你可以本地把@EnableEurekaClient 注释掉好的 我试试@EnableEurekaClient注视掉了本地的注册中心服务和猪齿鱼服务资源后，仍然报如下错误：
注视的服务：//@EnableEurekaClient
//@EnableChoerodonResourceServer
你好，方便贴一下pom依赖吗<?xml version="1.0" encoding="UTF-8"?>

框架和依赖的版本升一下，这个是之前0.5.1版本依赖的一个bug，然后重新import下pom依 赖好的，我试试Choerodon平台版本: 0.8遇到问题的执行步骤:手动部署文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/dns/环境信息(如:节点信息):k8s报错日志:无原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:
我有阿里云的域名，如何解析到指定服务器？我内网不用在部署DNS解析服务了吧？你可以在阿里云的域名解析控制台添加a记录到 任意master节点上master节点master节点不需要在安装什么了吗不用的, 你可以选择解析到任意master节点的内网地址进行内网访问，或者解析到对应的公网地址以便外部访问，如果你选择解析到公网地址 请确定 能够通过此地址访问到主机的 80 和443端口。如果你使用国内的主机，解析到公网的域名需要备案。好的，我试一下。TKSChoerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：在Choerodon组织的租户设置中的客户端修改会报错，导致修改不成功。修改的数据：报错信息(请尽量使用代码块的形式展现)：
Network:原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问像这样一直处在处理中，导致实例不能替换你好，请问你是在本地搭建的0.6.0的版本吗？
用的是你们的猪齿鱼平台不是本地搭建的不是本地搭建的devops，用的是公司的devops，部署网络报错，猪齿鱼更新之前好好的，更新之后就用不了了，帮忙看看，两天了ing-hpay-front.yaml这个文件中有错误，这个文件中的ingress  host path组合与另一个文件重复了，你删除掉一个，或改一下，保证host path的唯一性你们你们同步好了，只是需要重新建一个域名，先不用之前那个域名地址，那一条数据有点问题。Choerodon平台版本: 0.8遇到问题的执行步骤: 部署nfs报错文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/chartmuseum/环境信息(如:节点信息):k8s 集群报错日志:
MountVolume.SetUp failed for volume “chartmuseum-pv” : mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/1ca889ac-a6a8-11e8-89db-000c29038331/volumes/kubernetes.io~nfs/chartmuseum-pv --scope – mount -t nfs 192.168.11.233:/data/chartmuseum /var/lib/kubelet/pods/1ca889ac-a6a8-11e8-89db-000c29038331/volumes/kubernetes.io~nfs/chartmuseum-pv Output: Running scope as unit run-24007.scope. mount.nfs: mounting 192.168.11.233:/data/chartmuseum failed, reason given by server: No such file or directory原因分析:
nfs 已挂载成功，测试可用疑问:nfs上没有这个目录哦
/data/chartmuseum这个目录要自己创建？文档有提示的
刚刚试了下，nfs下创建了一个目录，已经好了，我以为是执行自己创建的，不需要手动创建呢。谢谢运行环境()：localhost遇到问题时的前置条件：
本地开发demo,新建了应用，代码clone到本地后，数据库创建成功，数据也初始化成功，在启动项目时报如下错误。问题描述：
你好，有创建对应的数据库用户吗创建了一个todo_sever的数据库，按照文档上创建的。   yml 文件里的数据库用户是choerodon，首先得确认数据库中确实有该用户，而且该用户有对应数据库的访问权限好的遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问已经解决，路径问题。.sh文件应该放在电脑根路径下。用例执行时，希望左侧能看到左右分配给自己的测试用例，每个用例执行的状态后台应该可以自定义（成功，失败，作废，锁定等），其次不同状态应该以不同颜色在左侧案例中标记出来，且左侧一级目录下能看到所有该目录下的所有案例总数，执行成功的用例数，失败的用例数等。可以图片上的作为参考。
您好，感谢您的反馈。首先，我们测试管理模块支持执行状态的自定义，请您参考
http://choerodon.io/zh/docs/user-guide/test-management/setting/status/
这篇文档其次，在0.9.0版本中，测试循环加入了按照执行方、被指定人筛选的功能，如下图所示：
同时，我们也优化了测试循环的进度显示，如下图所示：
请您期待这周发布的0.9版本的choerodon平台，您提出来的页面需求我们也会多加讨论、参考，感谢您对我们平台的支持。好的，谢谢了~Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：在Choerodon组织的租户设置中的客户端修改会报错，导致修改不成功。修改的数据：报错信息(请尽量使用代码块的形式展现)：
Network:原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问和上一个帖子同一个。IT大咖说上的第二讲 猪齿鱼持续交付视频什么时候能上传已经重新录制好了，今天就可以上传了Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：liquibase生成数据时乱码，报sql语法错误报错信息(请尽量使用代码块的形式展现)：2018-08-21 18:18:46.661  INFO 16200 — [           main] i.c.liquibase.excel.ExcelSeedDataReader  : found table:sys_user ,sheet:sys_user, begin row:7
2018-08-21 18:18:46.692  INFO 16200 — [           main] i.c.liquibase.excel.ExcelDataLoader      : ---- begin round 1 ----
2018-08-21 18:18:46.729  INFO 16200 — [           main] io.choerodon.liquibase.excel.DbAdaptor   : [sys_user] check exists: <> row:[ADMIN, e10adc3949ba59abbe56e057f20f883e, 18065588777, admin@qq.com,
鍗庢鼎缃湴鏈夐檺鍏徃, 涓彴, 3bed9a6581e7f1a6e5a3ce7d09ceab38, Y, Y, 鍗庢鼎缃湴鏈夐檺鍏徃, APPROVED, 1, 1, 1, 绯荤粺绠＄悊鍛榏 ,result :not exists
2018-08-21 18:18:46.764 ERROR 16200 — [           main] io.choerodon.liquibase.excel.DbAdaptor   : [sys_user]error insert row:[ADMIN, e10adc3949ba59abbe56e057f20f883e, 18065588777, admin@qq.com, 鍗庢
鼎缃湴鏈夐檺鍏徃, 涓彴, 3bed9a6581e7f1a6e5a3ce7d09ceab38, Y, Y, 鍗庢鼎缃湴鏈夐檺鍏徃, APPROVED, 1, 1, 1, 绯荤粺绠＄悊鍛榏  sql:INSERT INTO sys_user(user_name,password,tel_num,email,organizati
on,application,key,is_publish,is_admin,description,state,OBJECT_VERSION_NUMBER,CREATED_BY,LAST_UPDATED_BY,real_name) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
2018-08-21 18:18:46.780 ERROR 16200 — [           main] i.c.liquibase.excel.ExcelDataLoader      : You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version
for the right syntax to use near ‘key,is_publish,is_admin,description,state,OBJECT_VERSION_NUMBER,CREATED_BY,LAST_’ at line 1com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘key,is_
publish,is_admin,description,state,OBJECT_VERSION_NUMBER,CREATED_BY,LAST_’ at line 1请问是哪个服务？是在执行init-local-database.sh，用工具包生成数据的时候，服务是项目上的一个服务自己的服务请您检查下你的文件格式 。
这乱码不是重点，重点是你数据库字段有个Key,数据库保留字段，插入的时候报语法错误，换个名字。还有建表最好不要用保留字段好的，多谢，确实是这个问题，之前以为是乱码，因为打印出来的日志两列合并成一列了。乱码是excel里面的中文，liquibase插入的时候log的乱码，但不影响排错，乱码问题我们记录下，后面解决掉问题描述：
1、mysql 的docker容器情况说明：
1.1mysql的docker容器修改手册中的3306端口为3307，启动成功，状态up
$ docker ps
CONTAINER ID        IMAGE                                                                COMMAND                  CREATED             STATUS              PORTS                                                                    NAMES
81b7dc4c7776        registry.cn-shanghai.aliyuncs.com/choerodon/iam-service:0.5.0        “/bin/sh -c 'exec ja…”   4 days ago          Up 4 days           0.0.0.0:8030->8030/tcp                                                   iam-service
f4e194ef151b        registry.cn-hangzhou.aliyuncs.com/choerodon-tools/mysql:5.7.17       “docker-entrypoint.s…”   4 days ago          Up 4 days           3306/tcp, 0.0.0.0:3307->3307/tcp                                         mysql
48f99aaf4238        registry.cn-shanghai.aliyuncs.com/choerodon/api-gateway:0.5.0        “/bin/sh -c 'exec ja…”   4 days ago          Up 4 days           0.0.0.0:8080->8080/tcp                                                   api-gateway
157d2a1f6d2c        registry.cn-shanghai.aliyuncs.com/choerodon/eureka-server:0.5.0      “/bin/sh -c 'exec ja…”   4 days ago          Up 4 days           0.0.0.0:8000->8000/tcp                                                   eureka-server
8dc53b6c0803        registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kafka:1.0.0        “sh -c '/opt/kafka/b…”   4 days ago          Up 4 days           0.0.0.0:9092->9092/tcp                                                   kafka-0
f20c29c116f8        registry.cn-hangzhou.aliyuncs.com/choerodon-tools/zookeeper:3.4.10   “/opt/zookeeper/entr…”   4 days ago          Up 4 days           0.0.0.0:2181->2181/tcp, 0.0.0.0:2888->2888/tcp, 0.0.0.0:3888->3888/tcp   zookeeper-0
80e6f7be3b7f        registry.saas.hand-china.com/tools/phpmyadmin                        “/run.sh phpmyadmin”     4 days ago          Up 4 days           0.0.0.0:8888->80/tcp                                                     phpadminw
1.2已启动正常的docker容器ip查看
docker inspect -f ‘{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}’   f4e194ef151b
172.18.0.7
1.3容器ip及端口查看
“NetworkMode”: “choerodon_default”,
“PortBindings”: {
“3307/tcp”: [
{
“HostIp”: “”,
“HostPort”: “3307”
}
]
},“Labels”: {
“com.docker.compose.config-hash”: “8741ebc838e58a678fd6b6bcd19ebd7b2aa6a2f50f12cd56d1f14de39d792fc8”,
“com.docker.compose.container-number”: “1”,
“com.docker.compose.oneoff”: “False”,
“com.docker.compose.project”: “choerodon”,
“com.docker.compose.service”: “mysql”,
“com.docker.compose.version”: “1.20.1”
}
},
“Ports”: {
“3306/tcp”: null,
“3307/tcp”: [
{
“HostIp”: “0.0.0.0”,
“HostPort”: “3307”
}
]
},
“SandboxKey”: “/var/run/docker/netns/50a4bc34ab4e”,
“SecondaryIPAddresses”: null,
“SecondaryIPv6Addresses”: null,
“EndpointID”: “”,
“Gateway”: “”,
“GlobalIPv6Address”: “”,
“GlobalIPv6PrefixLen”: 0,
“IPAddress”: “”,
“IPPrefixLen”: 0,
“IPv6Gateway”: “”,
“MacAddress”: “”,
“Networks”: {
“choerodon_default”: {
“IPAMConfig”: null,
“Links”: null,
“Aliases”: [
“mysql”,
“f4e194ef151b”
],
“NetworkID”: “e3801204140b8a9e431892f6e1faa9dec2cc8d4be0705ebd58e2091997455aa8”,
“EndpointID”: “425011bb5f15be9f4ffc7b7a41c547b940623c28fa6d30641a58bdee99ad2b0f”,
“Gateway”: “172.18.0.1”,
“IPAddress”: “172.18.0.7”,
“IPPrefixLen”: 16,
“IPv6Gateway”: “”,
“GlobalIPv6Address”: “”,
“GlobalIPv6PrefixLen”: 0,
“MacAddress”: “02:42:ac:12:00:07”,
“DriverOpts”: null
}
}
}
}
2、连接报错信息：
2.1通过127.0.01端口3307连接
报错：unknown mysql serverhost 127.0.0.1(11001)
2.2通过localhost端口3307连接
报错：lost conectioconnection to mysql server at，system error 34
2.3通过localhost端口3306连接
报错：access denied for user choerodon @ localhost （using password ：YES）
2.4通过localhost端口3306连接
报错：access denied for user choerodon @ localhost （using password ：YES）
2.5通过172.18.0.7端口3306连接
报错：cannot connect to mysql server on 172.18.0.7
2.5通过172.18.0.7端口3307连接
报错：cannot connect to mysql server on 172.18.0.7
预期结果：
mysql成功连接麻烦把docker-compose的文件内容贴一下# docker-compose.yaml
version: “3”
services:
  zookeeper-0:
    container_name: zookeeper-0
    image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/zookeeper:3.4.10
    hostname: zookeeper-0
    environment:
    - ZK_REPLICAS=1
    - ZK_HEAP_SIZE=2G
    - ZK_TICK_TIME=2000
    - ZK_INIT_LIMIT=10
    - ZK_SYNC_LIMIT=5
    - ZK_MAX_CLIENT_CNXNS=60
    - ZK_SNAP_RETAIN_COUNT=3
    - ZK_PURGE_INTERVAL=1
    - ZK_LOG_LEVEL=INFO
    - ZK_CLIENT_PORT=2181
    - ZK_SERVER_PORT=2888
    - ZK_ELECTION_PORT=3888
    ports:
    - “2181:2181”
    - “2888:2888”
    - “3888:3888”
    command:
    - sh
    - -c
    - zkGenConfig.sh && exec zkServer.sh start-foreground
    volumes:
    - “./kafka/zk:/var/lib/zookeeper”
  kafka-0:
    container_name: kafka-0
    image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kafka:1.0.0
    hostname: 127.0.0.1
    depends_on:
    - zookeeper-0
    links:
    - zookeeper-0
    ports:
      - “9092:9092”
    command:
    - sh
    - -c
    - "/opt/kafka/bin/kafka-server-start.sh config/server.properties 
           --override zookeeper.connect=zookeeper-0:2181 
           --override log.dirs=/opt/kafka/data/logs 
           --override broker.id=0 "
    volumes:
    - “./kafka/kafka:/opt/kafka/data”
  mysql:
    container_name: mysql
    hostname: mysql
    image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/mysql:5.7.17
    ports:
    - “3307:3307”
    environment:
      MYSQL_ROOT_PASSWORD: root
    volumes:
    - C:\Users\wenjiandong\Desktop\choerodon\mysql\mysql_data:/var/lib/mysql
    - C:\Users\wenjiandong\Desktop\choerodon\mysql\mysql_db.cnf:/etc/mysql/conf.d/mysql_db.cnf
    expose:
    - “3307”
  phpadmin:
    container_name: phpadmin
    image: registry.saas.hand-china.com/tools/phpmyadmin
    ports:
      - “8888:80” # 80 port for explore view
    environment:
      PMA_ARBITRARY: 1 # for phpadmin settings
  eureka-server:
    container_name: eureka-server
    hostname: eureka-server
    image: registry.cn-shanghai.aliyuncs.com/choerodon/eureka-server:0.5.0
    ports:
    - “8000:8000”
    links:
    - kafka-0
    environment:
    - spring.kafka.bootstrap-servers=kafka-0:9092
    - eureka.client.serviceUrl.defaultZone=http://127.0.0.1:8000/eureka/
    - eureka.client.register-with-eureka=false
    - eureka.client.fetch-registry=false
    - hystrix.stream.queue.enabled=false
    - spring.cloud.bus.enabled=false
    - spring.sleuth.stream.enabled=false
    expose:
    - “8000”
  api-gateway:
    container_name: api-gateway
    image: registry.cn-shanghai.aliyuncs.com/choerodon/api-gateway:0.5.0
    links:
    - eureka-server
    depends_on:
    - eureka-server
    ports:
    - “8080:8080”
    environment:
    - zuul.routes.test.path=/test/**
    - zuul.routes.test.serviceId=test-service
    - eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
    - hystrix.stream.queue.enabled=false
    - spring.cloud.bus.enabled=false
    - spring.sleuth.stream.enabled=false
    expose:
    - “8080”
  gateway-helper:
    container_name: gateway-helper
    image: registry.cn-shanghai.aliyuncs.com/choerodon/gateway-helper:0.5.0
    depends_on:
    - eureka-server
    - mysql
    links:
    - eureka-server
    - mysql
    ports:
    - “9180:9180”
    environment:
    - zuul.routes.test.path=/test/**
    - zuul.routes.test.serviceId=test-service
    - eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
    - spring.datasource.url=jdbc:mysql://mysql/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
    - spring.datasource.username=root
    - spring.datasource.password=root
    - hystrix.stream.queue.enabled=false
    - spring.cloud.bus.enabled=false
    - spring.sleuth.stream.enabled=false
  iam-service:
    container_name: iam-service
    image: registry.cn-shanghai.aliyuncs.com/choerodon/iam-service:0.5.0
    depends_on:
    - eureka-server
    - mysql
    - kafka-0
    links:
    - eureka-server
    - mysql
    - kafka-0
    ports:
    - “8030:8030”
    environment:
    - eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
    - spring.kafka.bootstrap-servers=kafka-0:9092
    - spring.datasource.url=jdbc:mysql://mysql/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
    - spring.datasource.username=root
    - spring.datasource.password=root
    - hystrix.stream.queue.enabled=false
    - spring.cloud.bus.enabled=false
    - spring.sleuth.stream.enabled=false
  manager-service:
    container_name: manager-service
    image: registry.cn-shanghai.aliyuncs.com/choerodon/manager-service:0.5.0
    depends_on:
    - eureka-server
    - mysql
    - kafka-0
    links:
    - eureka-server
    - mysql
    - kafka-0
    ports:
    - “8963:8963”
    environment:
    - spring.kafka.bootstrap-servers=kafka-0:9092
    - eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
    - spring.datasource.url=jdbc:mysql://mysql/manager_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
    - spring.datasource.username=root
    - spring.datasource.password=root
    - hystrix.stream.queue.enabled=false
    - spring.cloud.bus.enabled=false
    - spring.sleuth.stream.enabled=false
  oauth-server:
    container_name: oauth-server
    image: registry.cn-shanghai.aliyuncs.com/choerodon/oauth-server:0.5.0
    depends_on:
    - eureka-server
    - mysql
    links:
    - eureka-server
    - mysql
    ports:
    - “8020:8020”
    environment:
    - eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
    - spring.datasource.username=root
    - spring.datasource.url=jdbc:mysql://mysql/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
    - spring.datasource.password=root
    - hystrix.stream.queue.enabled=false
    - spring.cloud.bus.enabled=false
    - spring.sleuth.stream.enabled=false首先不能通过127.0.0.1 连接，因为不同容器内的127.0.0.1都是本地，可以通过host连接。
access denied for user choerodon 请确保下数据库有对应的choerodon 用户，而且该用户有数据库的访问权限docker run -p ip:hostPort:containerPort，你要把mysql的3306端口映射出来到3307啊，把mysql ports改成-“3307:3306”你好，你可以参考下docker mysql 官方的说明，了解下如何使用docker启动mysql
https://hub.docker.com/_/mysql/Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：应用部署，点击最后一步 “部署” 时报错：
{“failed”:true,“code”:“error.gitlab.groupId.select”,“message”:“error.gitlab.groupId.select”}执行的操作：
周末貌似猪齿鱼升级了？
原有环境全部断开连接，于是在平台上获取了新的连接命令，在k8s机器上执行后，环境连接成功。
之后按照正常流程部署已经通过gitlab ci打包好的应用版本。
最后一步选择部署时出现报错，而且不管选择新建实例还是替换实例都一样。。你好，你可以私发提供一下你们的项目，给你们看一下项目的k8s地址还是项目的git地址还是项目的猪齿鱼projectId猪齿鱼projectIdChoerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：k8s集群中如果要使用猪齿鱼微服务开发框架，而不搭建整个猪齿鱼环境，需要在k8s里面部署哪些？
官网文档里面好像不是很清楚？
服务器配置的最低要求是啥？
比如这些，是不是都要
你要部署一个运行环境吗？就是想实践一下前两天培训的前端开发和后端微服务开发如果仅做开发用途 可以参考开发手册 本机部署对应应用就可以了哦比如这些， 都不需要吗？这些都是用公司平台上面的吗？参考开发环境搭建
http://choerodon.io/zh/docs/development-guide/backend/develop-env/install_windows/嗯嗯， 感谢！
我看文档里面是通过docker-compose启动kafka，zookeeper，mysql这些在本地跑起来那如果是要部署到k8s集群里， 我上面说的这些，应该还是需要的吧?registry.cn-shanghai.aliyuncs.com/choerodon/eureka-server:0.6.0
pull 失败， 是不是文档中版本不对？
用registry.choerodon.com.cn/choerodon-framework/eureka-server:0.6.0这个替换是否可以？可以的Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：http://choerodon.io/zh/docs/quick-start/microservice-front/
当 ci 执行通过以后，可以关闭分支，将 feature 分支合并到 develop 分支上。
在 开发流水线  ->  应用  页面，找到 choerodon-front-demo 。选择分支管理，找到 feature-1  分支，点击 结束分支 。结束分支后，会触发 develop  分支的 ci流水线 。问题：请问如何结束分组，根据文档中的菜单根本就找不到。从开发流水线中的分支管理也没有结束分支的按钮?你好，0.6.0版本是有结束分支的按钮的，请确认下自己的版本我使用的是https://choerodon.com.cn/#/iam/member-role?type=project&id=83&name=成都技术中心&organizationId=20， 这个应该是最新版本的吧。你可以在 开发流水线 -> 分支 菜单中，找到对应应用的分支，然后选择删除分支就可以删除分支了。如果想要合并到某一分支。可以在同一页面中，选择对应分支后面的创建合并请求功能。 创建自己分支到需要合并的分支的merge request我合并之后，分支并没有删除，这里关闭分支的意思，是要人工把分支删除吗？是的，现在合并之后不会自动删除分支了，可以再merge的时候选择删除源分支，或者在分支管理中删除分支。点击升级实例，报截图错误牛哥，问题的类型需要选择正确。你好，你提供一下你们项目，或者私发一下给我.Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：自主搭建问题描述：
搭建猪齿鱼平台的 K8S集群，大量pod出现Error ，查看pod详细，发现大量重启，有报错信息：
Normal   SandboxChanged  12m (x4803 over 2d)  kubelet, node4  Pod sandbox changed, it will be killed and re-created.报错信息(请尽量使用代码块的形式展现)：
建议你重启 node4 的docker重启了 docker 无效，后来重启了全部节点 好了架构图中的“Event服务 ”是什么意思？是数据一致性的支持，需配合choerodon-starter-event-consumer和choerodon-starter-event-producer使用，我们实现了新的数据一致性的实现，基于saga，有兴趣可以看下https://github.com/flyleft/spring-cloud-base/tree/master/asgard-saga-demoChoerodon平台版本: 0.6.0遇到问题的执行步骤:
在执行ansible-playbook -i inventory/hosts -e @inventory/vars scale.yml命令时，偶尔成功，但是经常报错，报错信息如下：
TASK [node : Set Master Node schedule] *************************************************************************
Sunday 19 August 2018  17:01:42 +0800 (0:00:00.341)       0:01:13.754 *********
fatal: [node1 -> 192.168.56.11]: FAILED! => {“changed”: true, “cmd”: [“kubectl”, “taint”, “nodes”, “node1”, “node-role.kubernetes.io/master-”], “delta”: “0:00:00.270828”, “end”: “2018-08-19 17:01:42.580652”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-08-19 17:01:42.309824”, “stderr”: “The connection to the server localhost:8080 was refused - did you specify the right host or port?”, “stderr_lines”: [“The connection to the server localhost:8080 was refused - did you specify the right host or port?”], “stdout”: “”, “stdout_lines”: []}
…ignoring疑问：
1.单独重启这个服务的命令是啥？
2.要查看错误日志怎样查看？
3.出现了这样错误，都只能每次运行 ansible-playbook -i inventory/hosts reset.yml -K之后再执行 ansible-playbook -i inventory/hosts -e @inventory/vars scale.yml 来启动k8s服务器吗？文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问已经ignore的错误无需考虑Choerodon平台版本: 0.6.0遇到问题的执行步骤:
#IAM_SERVICE_DB=(“mysql” “3306” “iam_service” “username” “password”)
#MANAGER_SERVICE_DB=(“mysql” “3306” “manager_service” “username” “password”)
#EVENT_STORE_SERVICE_DB=(“mysql” “3306” “event_store_service” “username” “password”)
#DEVOPS_SERVICE_DB=(“mysql” “3306” “devops_service” “username” “password”)
#GITLAB_SERVICE_DB=(“mysql” “3306” “gitlab_service” “username” “password”)
#AGILE_SERVICE_DB=(“mysql” “3306” “agile_service” “username” “password”)
#TEST_MANAGER_SERVICE_DB=(“mysql” “3306” “test_manager_service” “username” “password”)
#GITLAB_DB=(“mysql” “3306” “gitlabhq_production” “username” “password”)定义外部mysql数据库格式填写是这样写么,比如mysql的数据库是IP是192.168.1.2，mysql账号密码都是root root
那是否都是这样的格式填呢：
IAM_SERVICE_DB=(“mysql” “192.168.1.2:3306” “iam_service” “root” “root”)现在我老是这一步过不去，不知道是不是配置外部mysql数据库的原因。
Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found
?choerodon-manager-service not ready,sleep 5s,check it.文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问#IAM_SERVICE_DB=(“mysql” “3306” “iam_service” “username” “password”)
#MANAGER_SERVICE_DB=(“mysql” “3306” “manager_service” “username” “password”)
#EVENT_STORE_SERVICE_DB=(“mysql” “3306” “event_store_service” “username” “password”)
#DEVOPS_SERVICE_DB=(“mysql” “3306” “devops_service” “username” “password”)
#GITLAB_SERVICE_DB=(“mysql” “3306” “gitlab_service” “username” “password”)
#AGILE_SERVICE_DB=(“mysql” “3306” “agile_service” “username” “password”)
#TEST_MANAGER_SERVICE_DB=(“mysql” “3306” “test_manager_service” “username” “password”)
#GITLAB_DB=(“mysql” “3306” “gitlabhq_production” “username” “password”)定义外部mysql数据库格式填写是这样写么,比如mysql的数据库是IP是192.168.1.2，mysql账号密码都是root root
那是否都是这样的格式填呢：
IAM_SERVICE_DB=(“mysql” “192.168.1.2:3306” “iam_service” “root” “root”)现在我老是这一步过不去，不知道是不是配置外部mysql数据库的原因。
Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found
?choerodon-manager-service not ready,sleep 5s,check it.你打算使用外部的mysql吗你好，是的，我打算用外部的Mysql,一键安装会自动导入数据么？IAM_SERVICE_DB=(“mysql” “192.168.1.2:3306” “iam_service” “root” “root”)我那个配置是正确的么？Choerodon平台版本: 0.6.0遇到问题的执行步骤:
sh choerodon-install.sh values.sh文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
配置文件：
疑问:提出您对于遇到和解决该问题时的疑问
配置文件中,我把DEBUG="–debug --dry-run"加上，最后部署提示成功，但是我去掉之后部署报各种问题。加上这个命令到底有部署成功么？
我用域名去访问http://api.tuiyun.com  报错
你好，在文档的参数配置文件第一句话就有说明哈，添加DEBUG="--debug --dry-run"仅debug不真实部署，删除此变量则进行正式部署。http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/我搭建测试环境，是要添加还是不添加呢？请求一个问题，前端拉取代码，在开发新模块的步骤中，启动的时候报了一个错
拉取的代码package.json是在iam目录下的， 应该在iam目录下npm i然后npm start。 图中package.json是你自己创建的吗嗯嗯，这样的吧，但是启动的时候页面还是没有反应你怎么又在iam/src/app/iam下创建package.json了?  第一层iam下有package.json啊。  choerodon-front-iam模块不需要你创建任何package.json，config.js和webpack.config.js，只需要进入第一层iam的目录后，执行npm i ， 再执行npm start就可以运行了。
制作的这个chart有7个微服务和4个中间件（redis，activemq，zookeeper, activemq）
其中的微服务会依赖中间件等
具体目录结构如下
这个chart直接用helm发布没有问题。
通过猪齿鱼发布后，去k8s上看也是成功的。效果如下
然后，问题来了
猪齿鱼平台上显示的状态是红色状态
我自己的推测？
是不是猪齿鱼不支持这种复杂的chart。遇到一个chart里面有多个deployment，就默认取了排在第一位的activemq作为这个chart的信息？是否有啥办法解决？这边我们平台识别是需要标签的，如果chart包总渲染出的deployment文件，spec.template中带了标签，那pod也会带上我们平台标签，就能识别。在本周将发布的新版本中，不需要用户手动chart模板加入平台的label了，所以这个复杂chart在新版本能够得到支持。好的，谢谢不过为什么k8s里面pod的状态已经是running，但是猪齿鱼里面确实pending你说的是这种标签吗？ 并没有用
我对比了下猪齿鱼上显示出来的那个deploymmen，和其他没显示出来的deployment 的标签，结构是一样的。1.Choerodon 猪齿鱼敏捷管理
内容：直播地址：http://www.itdks.com/liveevent/detail/15184
时间：8月14日 10:00-11:00
IT大咖讲师：常壮    Choerodon猪齿鱼架构师2.Choerodon 猪齿鱼持续交付
内容：直播地址：http://www.itdks.com/liveevent/detail/15190
时间：8月14日 14:00-16:00
IT大咖讲师：李佳桐 Choerodon猪齿鱼架构师3.Choerodon 猪齿鱼后端开发（上）
内容：直播地址：http://www.itdks.com/liveevent/detail/15193
时间：8月15日 10:00-11:00
IT大咖讲师：董凡    Choerodon猪齿鱼架构师4.Choerodon 猪齿鱼后端开发（下）
内容：直播地址：http://www.itdks.com/liveevent/detail/15195
时间：8月15日 14:00-16:00
IT大咖讲师：董凡 Choerodon猪齿鱼架构师5.Choerodon 猪齿鱼测试管理和知识管理
内容：测试管理知识管理直播地址：http://www.itdks.com/liveevent/detail/15196
时间：8月16日 10:00-11:30
IT大咖讲师：吴国凯（Choerodon猪齿鱼架构师） 、王喆（Choerodon猪齿鱼架构师）6.Choerodon 猪齿鱼前端开发
内容：直播地址：http://www.itdks.com/liveevent/detail/15197
时间：8月16日 14:00-16:00
IT大咖讲师：吴华真 Choerodon猪齿鱼架构师培训资料从哪里下载？1.视频回放，现在除了持续交付，其他的都可以观看回放
2.培训使用PPT等。这个资料我们不对外提供下载的。你好，工号 21252 用户是公司员工，但是无法登录HAND公司的猪齿鱼系统。敏捷管理添加项目成员时输入工号提示不存在此用户
我们同步了LDAP您再试一下。已经可以了，谢谢Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：自己创建了分支，而且提交修改后的代码并且合并到了master分支，到持续集成看状态时失败的，为什么呢请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：使用了一下迭代管理，到冲刺发布以后就结束了。 之前还想着是需求和feature相关之类的。迭代发布会和release分支关联在一起执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块或系统截图的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作麻烦提供一下失败的日志输出，请确保不是因为gitlab-ci文件编写问题或代码问题造成的ci失败Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost问题描述：
1，创建网络成功
2，修改端口
3，端口被占用，此时网络会一直显示处理中，无法修改或者删除
4，kill掉占用端口的进程，网络状态依然是处理中，无法修改或删除
疑问：怎么停止处理中，多久会变成失败状态，可以对此条记录进行处理（修改或者删除）现在还是处理中的状态，已经十几个小时了，以前的端口也被占用着，释放不掉你的外部ip只能是集群中的ip,如果网卡中没有这个ip是不能指定的。端口被其他应用占用着建议你看下是哪个线程占用的。你的外部ip只能是集群中的ip,如果网卡中没有这个ip是不能指定的。端口被其他应用占用着建议你看下是哪个线程占用的。我现在想终止网络更新，现在一直处于处理中，导致我没法操作。
创建网络是正常的，只是更新端口的时候一直处于处理中。然后更新前的端口一直占用着集群服务器的进程，
kill掉会立即启动。
你不能删除kube-proxy，这是kubernetes集群的必要组件。
@crockitwood 这个问题有什么好的解决办法？这个问题有解决方案吗？已经卡了两天还处于加载处理中，暂时没有的话我就重新去部署一次环境了。
这个应该是平台的BUG吧，按理说长时间未响应或者说处理不了，不应该报错吗？什么端口被占用，你填了externalIp吗，你可以在k8s直接把这个service删掉，还有这个版本处理中只能清数据库，当然这个service出现处理中我们bug。修改网络 一直处于处理中是可能我们这边同步状态的时候报错了，下次遇到这种情况可以把报错日志贴出来，然后关于端口占用的 你可以直接在集群中把你建的service删除掉就行了。之后我们会仔细测试排除这种bugexternalIp最初是根据文档使用的80端口，然后我换成8060端口，这个时候就一直处理中，这个时候我去查找8060端口，发现被其他进程占用，kill掉8060端口，仍旧是处理中状态。我用的公司的Choerodon平台，这个处理中怎么清除数据？界面一直是处理中，并且一直是执行的状态，一直在加载状态。从哪个地方可以查看报错信息？
报错信息可以从devops-service日志中查看Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：自主搭建问题描述：
choerodon-front升级到了 0.8.4 wiki-server 升级到 0.8.1 .进入 wiki后 创建页面后，跳到Editing 页面，发现文本编辑不出来，页面有报错
感谢您的反馈。这是因为其它页面的js效果影响了该页面的显示，下一个版本我们将修复这个问题。Choerodon平台版本：0.6.0运行环境：http://choerodon.io/zh/docs/development-guide/backend/develop-env/install_windows/问题描述：
Windows10后端开发环境mysql容器无法开启成功mysql:
container_name: mysql
hostname: mysql
image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/mysql:5.7.17
ports:执行的操作：
Windows10后端开发环境mysql容器无法开启成功报错信息(请尽量使用代码块的形式展现)：
Starting 4f87aa34e98a_mysql … errorERROR: for 4f87aa34e98a_mysql  Cannot start service mysql: driver failed programming external connectivity on endpoint 4f87aa34e98a_mysql (5cb01510a3460547198c9621552c0316f0f7dd4cf1a6407829484f674adf5cbd): Error starting userland proxy: Bind for 0.0.0.0:3306: unexpected error Permission deniedERROR: for mysql  Cannot start service mysql: driver failed programming external connectivity on endpoint 4f87aa34e98a_mysql (5cb01510a3460547198c9621552c0316f0f7dd4cf1a6407829484f674adf5cbd): Error starting userland proxy: Bind for 0.0.0.0:3306: unexpected error Permission denied
Encountered errors while bringing up the project.解决了，以前已经用过3306的docker端口了，虽然没启容器，换新端口ok了Choerodon平台版本: 0.6.0遇到问题的执行步骤:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/文档地址:环境信息(如:节点信息):报错日志:
RUNNING HANDLER [master : Master | reload kubelet] *****************************
Thursday 16 August 2018  12:49:28 +0800 (0:00:00.220)       0:01:33.166 *******
changed: [node1]RUNNING HANDLER [master : Master | wait for kube-scheduler] ********************
Thursday 16 August 2018  12:49:28 +0800 (0:00:00.229)       0:01:33.396 *******
ok: [node1]RUNNING HANDLER [master : Master | wait for kube-controller-manager] ***********
Thursday 16 August 2018  12:49:29 +0800 (0:00:00.281)       0:01:33.677 *******
FAILED - RETRYING: Master | wait for kube-controller-manager (15 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (14 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (13 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (12 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (11 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (10 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (9 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (8 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (7 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (6 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (5 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (4 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (3 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (2 retries left).
FAILED - RETRYING: Master | wait for kube-controller-manager (1 retries left).
fatal: [node1]: FAILED! => {“attempts”: 15, “changed”: false, “content”: “”, “msg”: “Status code was -1 and not [200]: Request failed: <urlopen error [Errno 113] No route to host>”, “redirected”: false, “status”: -1, “url”: “http://localhost:10252/healthz”}RUNNING HANDLER [master : Master | wait for the apiserver to be running] *******
Thursday 16 August 2018  12:50:47 +0800 (0:01:18.093)       0:02:51.771 *******NO MORE HOSTS LEFT *************************************************************
to retry, use: --limit @/data/kubeadm-ansible/cluster.retry原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在在腾讯云上单机部署，执行命令ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml就报这个错疑问:提出您对于遇到和解决该问题时的疑问请问你是选择的那种模式进行的安装？测试模式亲，建议你进行现在重置集群后再进行部署使用时候有哪些配置吗？

依赖于mino，需要部署mino，并在file-service的环境变量配置正确的mino地址即可，变量如下：Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：h请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：
同一个k8s集群的条件下，目前有一套dev环境，想再部署test环境，哪些猪齿鱼微服务可以共用，哪些猪齿鱼微服务需要新部署？这个取决于您的应用。比如您的应用需要使用kafak，但固定了topic则不能共用kafka，如果你的应用可以配置不同环境用不同的topic则可以共用kafka。可以说下你们是怎么做的吗？参考下我们需要开发Choerodon,所以每个环境都会部署一套Choerodon，你们用Choerodon管理开发，如果你的应用基于Choerodon-Framework则需要每个环境部署一套Choerodon-Framework，如果不基于Choerodon-Framework，则你仅部署您的应用及agent即可。Choerodon平台版本：0.6.0运行环境：猪齿鱼问题描述：$ docker login ${DOCKER_REGISTRY} -u admin -p Harbor12345
Error response from daemon: Get https://registry.choerodon.com.cn/v2/: unauthorized: authentication required
ERROR: Job failed: error executing remote command: command terminated with non-zero exit code: Error executing in Docker Container: 1麻烦看报错信息。unauthorized: authentication required请问git的账号和密码是多少？
[vagrant@node1 project]$ git clone https://code.choerodon.com.cn/ora-cddc/nginx.git
Cloning into ‘nginx’…
Username for ‘https://code.choerodon.com.cn’: jinjin.peng@hand-china.com
git push -u origin masterjinjin.peng@hand-china.com@code.choerodon.com.cn’:
fatal: Authentication failed for ‘https://code.choerodon.com.cn/ora-cddc/nginx.git/’
[vagrant@node1 project]$ ll
total 0
[vagrant@node1 project]$您好 请规范提问。Choerodon平台版本：？运行环境：？问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：创建了一个已启用状态的项目，并分配了项目管理员的权限，但是菜单中并没有项目部署，环境流水线等菜单选项执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作Choerodon平台版本: 0.8.0遇到问题的执行步骤:➜ choerodon-iam-service not ready,sleep 5s,check it.文档地址:环境信息(如:节点信息):CentOS7，kernel4.17.14， 32G，150GHDD报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
这个服务之前我没清理NFS挂载文件夹，直接启动不了。现在是启动了，连接不到8031端口。看配置文件好像是个监控配置？
一键部署总是有各种问题，建议出现问题时，服务能自己写日志，找问题很麻烦。疑问:提出您对于遇到和解决该问题时的疑问执行 kubectl logs [POD名称] -n [NAMESPACE] 查看一下日志隔了两个小时再看，现在莫名奇妙好了。直接到最后一步结束了。。。然后有下面几个容器和job不正常，然后front的网址没有建ingress。
看下 choerodon-xxx-init-xx 日志 有没有什么异常jobs有日志吗？你在安装的时候能够看到job对应的pod，查看pod的日志即为job日志。Choerodon平台版本: 0.6.0遇到问题的执行步骤:
文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):
deepin桌面版报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请问你的机器是什么操作系统？执行一键部署需要在k8s集群的master节点上执行。Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：
创建一个nginx示例,部署一个空的 nginx 的 kubernetes yaml文件.
执行 kubectl apply -f nginx.yaml问题描述：
Pods的状态为ImagePullBackOff.
报错信息(请尽量使用代码块的形式展现)：
describe 这个失败的 Pod：
Failed to pull image “registry.choerodon.com.cn/operation-choerodon-dev/nginx-demo:1.13.5-alpine”: rpc error: code = Unknown desc = Error: image operation-choerodon-dev/nginx-demo:1.13.5-alpine not found原因分析：
本地环境 docker pull registry.choerodon.com.cn/operation-choerodon-dev/nginx-demo:1.13.5-alpine 依旧报错。也就是你的镜像没找到哦， 查看下面这个贴子choerodon-front-iam,choerodon-front-devops,还有我们开发的模块。怎么用choerodon-front打包到一起你可以参考choerodon-front - Choerodon Front is a total front-end of Choerodon that combines Choerodon IAM and Choerodon DevOps.将所有的项目都作为一个项目的子项目然后通过命令进行打包npm run build choerodon-front-iam/iam choerodon-front-agile/agile 我执行这个操作报错0 info it worked if it ends with ok
1 verbose cli [ ‘E:\nodejs\node.exe’,
1 verbose cli   ‘E:\nodejs\node_modules\npm\bin\npm-cli.js’,
1 verbose cli   ‘run’,
1 verbose cli   ‘build’,
1 verbose cli   ‘choerodon-front-iam/iam’,
1 verbose cli   ‘choerodon-front-agile/agile’ ]
2 info using npm@5.6.0
3 info using node@v8.9.4
4 verbose run-script [ ‘prebuild’, ‘build’, ‘postbuild’ ]
5 info lifecycle choerodon-front@0.8.1~prebuild: choerodon-front@0.8.1
6 info lifecycle choerodon-front@0.8.1~build: choerodon-front@0.8.1
7 verbose lifecycle choerodon-front@0.8.1~build: unsafe-perm in lifecycle true
8 verbose lifecycle choerodon-front@0.8.1~build: PATH: E:\nodejs\node_modules\npm\node_modules\npm-lifecycle\node-gyp-bin;E:\product\open\git\choerodon\truck\choerodon_front\choerodon-front\node_modules.bin;E:\Oracle\xe\app\oracle\product\11.2.0\server\bin;C:\Program Files (x86)\Intel\iCLS Client;C:\Program Files\Intel\iCLS Client;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files (x86)\Intel\Intel® Management Engine Components\DAL;C:\Program Files\Intel\Intel® Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel® Management Engine Components\IPT;C:\Program Files\Intel\Intel® Management Engine Components\IPT;C:\Program Files (x86)\cvsnt;C:\Program Files\TortoiseSVN\bin;C:\Program Files\TortoiseGit\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0;C:\WINDOWS\system32\config\systemprofile.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm;C:\Program Files (x86)\Windows Kits\8.1\Windows Performance Toolkit;C:\Program Files\Microsoft SQL Server\120\Tools\Binn;C:\Program Files\Microsoft SQL Server\130\Tools\Binn;E:\nodejs;C:\Program Files\Microsoft VS Code\bin;C:\Program Files (x86)\Common Files\Thunder Network\KanKan\Codecs;E:\mac_share\maven\bin;D:\develop\Apache2.4\bin;E:\Oracle\xe\app\oracle\product\11.2.0\server\bin;E:\Android\tools;E:\Android\android-sdk-windows\tools;E:\Android\android-sdk-windows\platform-tools;E:\Android\android-ndk-r6b;D:\develop\MySQL\mysql-5.5.40-winx64\bin;D:\Program Files (x86)\Git\bin;C:\Program Files (x86)\CVSNT;D:\Program Files\TortoiseSVN\bin;E:\nodejs;C:\Users\fly\AppData\Local\Programs\Python\Python35-32;C:\Program Files\MongoDB\Server\3.2\bin;E:\Git\bin;C:\develop\Java\jdk1.8.0_60\bin;C:\Users\fly\AppData\Local\Microsoft\WindowsApps;D:\MinGW\bin;C:\Python27;C:\Users\fly\AppData\Roaming\npm;C:\Program Files\Docker Toolbox;C:\Program Files\Oracle\VirtualBox;C:\Program Files\Microsoft VS Code\bin
9 verbose lifecycle choerodon-front@0.8.1~build: CWD: E:\product\open\git\choerodon\truck\choerodon_front\choerodon-front
10 silly lifecycle choerodon-front@0.8.1~build: Args: [ ‘/d /s /c’,
10 silly lifecycle   ‘choerodon-front-boot build --config ./config.js “choerodon-front-iam/iam” “choerodon-front-agile/agile”’ ]
11 silly lifecycle choerodon-front@0.8.1~build: Returned: code: 1  signal: null
12 info lifecycle choerodon-front@0.8.1~build: Failed to exec build script
13 verbose stack Error: choerodon-front@0.8.1 build: choerodon-front-boot build --config ./config.js "choerodon-front-iam/iam" "choerodon-front-agile/agile"
13 verbose stack Exit status 1
13 verbose stack     at EventEmitter. (E:\nodejs\node_modules\npm\node_modules\npm-lifecycle\index.js:285:16)
13 verbose stack     at emitTwo (events.js:126:13)
13 verbose stack     at EventEmitter.emit (events.js:214:7)
13 verbose stack     at ChildProcess. (E:\nodejs\node_modules\npm\node_modules\npm-lifecycle\lib\spawn.js:55:14)
13 verbose stack     at emitTwo (events.js:126:13)
13 verbose stack     at ChildProcess.emit (events.js:214:7)
13 verbose stack     at maybeClose (internal/child_process.js:925:16)
13 verbose stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5)
14 verbose pkgid choerodon-front@0.8.1
15 verbose cwd E:\product\open\git\choerodon\truck\choerodon_front\choerodon-front
16 verbose Windows_NT 10.0.14393
17 verbose argv “E:\nodejs\node.exe” “E:\nodejs\node_modules\npm\bin\npm-cli.js” “run” “build” “choerodon-front-iam/iam” “choerodon-front-agile/agile”
18 verbose node v8.9.4
19 verbose npm  v5.6.0
20 error code ELIFECYCLE
21 error errno 1
22 error choerodon-front@0.8.1 build: choerodon-front-boot build --config ./config.js "choerodon-front-iam/iam" "choerodon-front-agile/agile"
22 error Exit status 1
23 error Failed at the choerodon-front@0.8.1 build script.
23 error This is probably not a problem with npm. There is likely additional logging output above.
24 verbose exit [ 1, true ]Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：重建一个项目吧，当然数据库中应用表中存着有token。iam  如何和新开发的模块整合在一起，如何修改首页请问下是前端还是后端？麻烦补充一下描述，问题不是很清楚前端開發,想要調开发前端项目，然后把iam模块包含进去。你可以参考choerodon 的方式通过添加git子模块。https://github.com/choerodon/choerodon-front
通过命令将两个模块打包到一起。按照这个执行不行了。就只加iam都直接报错，为什么？
npm run build choerodon-front-iam/iamchoerodon-front@0.8.1 build \choerodon\truck\choerodon_front\choerodon_front
choerodon-front-boot build --config ./config.js “choerodon-front-iam/iam”events.js:183
throw er; // Unhandled ‘error’ event
^Error: spawn npm.cmd ENOENT
at _errnoException (util.js:992:11)
at Process.ChildProcess._handle.onexit (internal/child_process.js:190:19)
at onErrorNT (internal/child_process.js:372:16)
at _combinedTickCallback (internal/process/next_tick.js:138:11)
at process._tickCallback (internal/process/next_tick.js:180:9)
at Function.Module.runMain (module.js:695:11)
at startup (bootstrap_node.js:191:16)
at bootstrap_node.js:612:3
npm ERR! code ELIFECYCLE
npm ERR! errno 1
npm ERR! choerodon-front@0.8.1 build: choerodon-front-boot build --config ./config.js "choerodon-front-iam/iam"
npm ERR! Exit status 1
npm ERR!
npm ERR! Failed at the choerodon-front@0.8.1 build script.
npm ERR! This is probably not a problem with npm. There is likely additional logging output above.node_modules目录的权限问题， chmod -R 777 node_modules现在项目刚开始，只使用Choerodon-UI的情况下，引用choerodon-ui.less报错，改成choerodon-ui.css就可以了，但是现在想要对主题颜色进行覆盖，所以还是需要引用choerodon-ui.less，下面是报错信息和报错的文件，麻烦帮忙看一下，谢谢。
@viviprprpr 尝试下：我修改了图二文件中的路径然后就可以引用了，因为没有配置按需加载，less-loader需要添加配置

那么图二的路径属于bug吧，可以修改么，因为项目中会用到
谢谢请使用babel-plugin-import插件，不要直接引入choerodon-ui/dist/choerodon-ui.less。
babel配置的plugins：主题替换的话用modifyVars来注入，webpack中的配置如下：那页面中的字体如果想使用less里面定义的某种颜色怎么办呢，比如choerodon-ui/dist/choerodon-ui.less里面定义的@blue-10可以在你的模块中创建less文件，然后在less文件中@import “choerodon-ui/dist/choerodon-ui.less”, 然后就能使用变量@blue-10了Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：
完成[创建项目]
完成[创建环境]，环境流水线中有状态为运行中的环境。
|应用编码 |choerodon-front-demo|
|应用名称 |猪齿鱼前端Demo应用|
|选择应用模板 | MicroServiceFront|
但是按照以上信息创建一个前端应用报错。
应用模板不给值依然报错。报错信息(请尽量使用代码块的形式展现)：
Request processing failed; nested exception is java.lang.IllegalArgumentException: Illegal character in path at index 32: http://gitlab-service/v1/groups/{groupId}/members/1910重新创建一个项目看看，应该是这个项目没有在gitlab中同步成功。问题确实是项目没有在gitlab同步成功，因为项目名称中有&符号。
& 符号影响会Gitlab的集成？
Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
创建一个新用户，登录报错
使用新的用户登录swagger，全部403，使用以前的用户登录正常
创建子用户之后，平台登陆也403:disappointed_relieved:/iam/v1/users/self这个接口不是iam-service的吗？为啥显示的服务为hpay-iam？你们权限表有这个权限吗？我继承了iam-service服务，客户化了一个新的服务hpay-iam。
这个问题是这样的，我的登录用户（非admin）分配了一个组织层的角色，角色的level和权限的level不等，角色的login_access不为0，不能通过下面sql的校验，所以报错403
还是没完全明白你的意思，/iam/v1/users/self这个接口是login-access权限的话，权限校验sql是这个数据库有这个权限且已经登录就可以访问了你贴的是iam的菜单权限，跟接口权限校验403没有任何关系，接口校验403是在因为gateway-helper没校验通过。你新创建的用户要给他分配角色，但如果是loginAccess接口数据库有匹配的uri且登陆就会放行报错是 ： error.permissionVerifier.permission when passSourcePermission
说明 判断是不是public接口获取loginAccess接口没有通过
如果通过，就直接return true了
需求：创建一个用户（非admin），分配一个组织层的角色，能够登录服务器上，还是报403，loginAccess为 1的接口，日志报错是 ： error.permissionVerifier.permission when passSourcePermission，根据日志无法查找错误已解决，gateway-helper数据库配错了Choerodon平台版本: 0.8.0遇到问题的执行步骤:
一键部署 0.8版本，多次到部署manager-service 就报错， 每次遇见报错就重新执行两次脚本删除重新部署，就是没有一次成功，烦请帮忙解决下文档地址:环境信息(如:节点信息):
报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问这是正常提示，表示服务还未安装上，你可以另起一个窗口 执行查看容器状态kubectl get po -n [NAMESPACE]执行 查看一下：
helm list | grep managerhelm list | grep manager能否粘贴一下后边的日志Choerodon manager service …能否粘贴一下…中间的日志？也就是图片中报错之前的日志这是全部了啊你的命令行界面不能向上滚动吗？麻烦查看一下在此之前的日志。的日志？也就是图片中报错之前的日志数据库初始化, 检查下您的nfs是否正确配置及查看下mysql日志。Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
执行helm激活环境的脚本报错
helm install --repo=http://chart.choerodon.com.cn/choerodon/c7ncd/ 
–namespace=dev 
–name=dev 
–version=0.8.0 
–set config.connect=ws://devops.service.choerodon.com.cn/agent/ 
–set config.token=b80ff2b0-6689-46ea-8a06-8cbfc354094d 
–set config.envId=112 
–set rbac.create=true 
choerodon-agent
报错信息(请尽量使用代码块的形式展现)：
Error: release dev failed: namespaces “dev” is forbidden: User “system:serviceaccount:kube-system:default” cannot get namespaces in the namespace “dev”原因分析：kubectl create namespace xxx可以成功疑问：安装helm是否指定了serviceaccount服务端执行如下命令安装的应该是没有指定serviceaccount吧？请你参考我们的文档安装哦，使用其他版本的helm及tailler可能导致其他问题产生。文档中貌似只有版本要求2.8.2， 安装步骤好像没有， 能提供个吗？安装2.8.2版本也是一样的报错？
麻烦认真读文档哦
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/helm/嗯嗯， 成功了
这部分在choerodon部署文档里面，没有搭建choerodon， 就没有看了。现在我没有自己部署choerodon，只是部署了k8s集群，choerodon用的是公有云的；这种情况下图的这些组件都要安装吗？
根据你应用的具体需求安装，比如你的应用不需要kafka则不需要安装kafka。嗯嗯， 明白， 谢谢啦在集群中执行helm init的时候报错，报错信息如下：
[vagrant@node1 vagrant]$ helm init --tiller-image=registry.cn-shanghai.aliyuncs.com/choerodon/tiller:v2.8.2 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account=helm-tiller
$HELM_HOME has been configured at /home/vagrant/.helm.
Error: error installing: Post http://localhost:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments: dial tcp 127.0.0.1:8080: getsockopt: connection refused
[vagrant@node1 vagrant]$执行查看输出什么[vagrant@node1 vagrant]$ kubectl get po -n kube-system
NAME                                        READY     STATUS    RESTARTS   AGE
default-http-backend-6dd4d5b7c9-gbwg4       0/1       Pending   0          38s
heapster-746d67c7b9-v4xxd                   0/1       Pending   0          24s
kube-apiserver-node1                        1/1       Running   0          50s
kube-controller-manager-node1               1/1       Running   0          50s
kube-dns-79d99555df-rzlpw                   0/3       Pending   0          56s
kube-flannel-qq6lm                          2/2       Running   0          50s
kube-lego-6f45757db7-j5rh7                  0/1       Pending   0          13s
kube-proxy-smtt7                            1/1       Running   0          56s
kube-scheduler-node1                        1/1       Running   0          50s
kubernetes-dashboard-dc8fcdbc5-7hbl8        0/1       Pending   0          31s
nginx-ingress-controller-5d77d4945d-s6r99   0/1       Pending   0          37s
[vagrant@node1 vagrant]$我重新安装了一遍就成功了，但是又报其他的错误了。
[vagrant@node1 vagrant]$ helm version
Client: &version.Version{SemVer:“v2.8.2”, GitCommit:“a80231648a1473929271764b920a8e346f6de844”, GitTreeState:“clean”}Error: cannot connect to Tiller查看下 kube-system 中 pod pendding 的原因。Choerodon平台版本: 0.6.0遇到问题的执行步骤:安装NFS报错文档地址:环境信息(如:节点信息):
[root@k8s-node1 /]# cat /etc/exports
/u01 172.18.194.0/24(rw,sync,no_root_squash,no_all_squash)报错日志:
[root@k8s-node1 /]# systemctl start nfs-server
Failed to start nfs-server.service: Transaction order is cyclic. See system logs for details.
See system logs and ‘systemctl status nfs-server.service’ for details.
[root@k8s-node1 /]# systemctl status nfs-server.service
● nfs-server.service - NFS server and services
Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)
Drop-In: /run/systemd/generator/nfs-server.service.d
└─order-with-mounts.conf
Active: inactive (dead)Aug 02 18:34:27 k8s-node1 systemd[1]: Breaking ordering cycle by deleting job u01.mount/start
Aug 02 19:38:17 k8s-node1 systemd[1]: Found ordering cycle on nfs-server.service/start
Aug 02 19:38:17 k8s-node1 systemd[1]: Found dependency on u01.mount/start
Aug 02 19:38:17 k8s-node1 systemd[1]: Found dependency on nfs-server.service/start
Aug 02 19:48:10 k8s-node1 systemd[1]: Found ordering cycle on nfs-server.service/start
Aug 02 19:48:10 k8s-node1 systemd[1]: Found dependency on u01.mount/start
Aug 02 19:48:10 k8s-node1 systemd[1]: Found dependency on nfs-server.service/start
Aug 02 19:51:48 k8s-node1 systemd[1]: Found ordering cycle on nfs-server.service/start
Aug 02 19:51:48 k8s-node1 systemd[1]: Found dependency on u01.mount/start
Aug 02 19:51:48 k8s-node1 systemd[1]: Found dependency on nfs-server.service/start提出您分析问题的过程,以便我们能更准确的找到问题所在
安装之后启动一直报错。。。。提出您对于遇到和解决该问题时的疑问是不是起了多个nfs server呢总共四台阿里云服务器，只有一台安装了nfs server 其他全部客户端，今天突然起不了，然后我又在另外一台装了nfs.想用另外一台做nfs server 可是也起不来，报同样的错，卸载又卸载不了:joy:看报错是Transaction order is cyclic， 建议先停止docker 然后unmount 随后重新mount 再启动docker，如果不行请重启机器你是不是在/etc/fstab里面先加上了挂载的任务，然后NFS盘还没建。把挂载NFS的任务先删掉再说。在Node1节点中启动集群的时候，报错。具体报错信息如下：
TASK [base/install : Ensure Base Kubernetes] ***********************************************************************************************
Friday 10 August 2018  22:49:50 +0800 (0:00:00.249)       0:03:05.778 *********
failed: [node1] (item=[u’kubeadm-1.8.5-0.x86_64’, u’kubectl-1.8.5-0.x86_64’, u’kubelet-1.8.5-0.x86_64’, u’kubernetes-cni-0.5.1-1.x86_64’]) => {“changed”: true, “item”: [“kubeadm-1.8.5-0.x86_64”, “kubectl-1.8.5-0.x86_64”, “kubelet-1.8.5-0.x86_64”, “kubernetes-cni-0.5.1-1.x86_64”], “msg”: “https://file.choerodon.com.cn/kubernetes/kubernetes-v1.8.5-el7-x86_64/f9a3e9f13f7dacb8a39b90a2331007bebbed4f84643448e83e5c18b3efe3d45b-kubeadm-1.8.5-0.x86_64.rpm: [Errno 12] Timeout on https://file.choerodon.com.cn/kubernetes/kubernetes-v1.8.5-el7-x86_64/f9a3e9f13f7dacb8a39b90a2331007bebbed4f84643448e83e5c18b3efe3d45b-kubeadm-1.8.5-0.x86_64.rpm: (28, ‘Operation too slow. Less than 1000 bytes/sec transferred the last 30 seconds’)\nTrying other mirror.\nhttps://file.choerodon.com.cn/kubernetes/kubernetes-v1.8.5-el7-x86_64/7330c09dd7dccc300c9c5f696fc4a8a76327e5068e51a60e4517b59a1c4b3dbc-kubelet-1.8.5-0.x86_64.rpm: [Errno 14] curl#6 - “Could not resolve host: file.choerodon.com.cn; Name or service not known”\nTrying other mirror.\n\n\nError downloading packages:\n  kubelet-1.8.5-0.x86_64: [Errno 256] No more mirrors to try.\n\n”, “rc”: 1, “results”: [“Resolving Dependencies\n–> Running transaction check\n—> Package kubeadm.x86_64 0:1.8.5-0 will be installed\n—> Package kubectl.x86_64 0:1.8.5-0 will be installed\n—> Package kubelet.x86_64 0:1.8.5-0 will be installed\n–> Processing Dependency: socat for package: kubelet-1.8.5-0.x86_64\n—> Package kubernetes-cni.x86_64 0:0.5.1-1 will be installed\n–> Running transaction check\n—> Package socat.x86_64 0:1.7.3.2-2.el7 will be installed\n–> Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                Arch           Version               Repository    Size\n================================================================================\nInstalling:\n kubeadm                x86_64         1.8.5-0               k8s           15 M\n kubectl                x86_64         1.8.5-0               k8s          7.3 M\n kubelet                x86_64         1.8.5-0               k8s           16 M\n kubernetes-cni         x86_64         0.5.1-1               k8s          7.4 M\nInstalling for dependencies:\n socat                  x86_64         1.7.3.2-2.el7         base         290 k\n\nTransaction Summary\n================================================================================\nInstall  4 Packages (+1 Dependent package)\n\nTotal download size: 46 M\nInstalled size: 244 M\nDownloading packages:\n”]}NO MORE HOSTS LEFT *************************************************************************************************************************
to retry, use: --limit @/vagrant/cluster.retryPLAY RECAP *********************************************************************************************************************************
node1                      : ok=51   changed=17   unreachable=0    failed=1请检查下您的网络网络是OK的，具体要检查那个IP呢？我使用的是猪齿鱼的默认配置看提示是下载rpm包过慢 你的服务器是哪里的？怎么看服务器，我就是按照猪齿鱼上面一步一步的来操作的，使用的是猪齿鱼里面提供的虚拟机你在node1中下载 这个文件 查看网速是否正常：https://file.choerodon.com.cn/kubernetes/kubernetes-v1.8.5-el7-x86_64/f9a3e9f13f7dacb8a39b90a2331007bebbed4f84643448e83e5c18b3efe3d45b-kubeadm-1.8.5-0.x86_64.rpm请问是修改了什么吗？现在报错信息改变了。报错信息如下：
TASK [master : kubeadm | delete old kube-dns service] **************************************************************************************
Saturday 11 August 2018  09:39:55 +0800 (0:00:00.726)       0:01:29.221 *******
fatal: [node1]: FAILED! => {“changed”: true, “cmd”: [“kubectl”, “delete”, “svc”, “kube-dns”, “-n”, “kube-system”], “delta”: “0:00:00.608865”, “end”: “2018-08-11 09:39:56.171238”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-08-11 09:39:55.562373”, “stderr”: “Error from server (NotFound): services “kube-dns” not found”, “stderr_lines”: [“Error from server (NotFound): services “kube-dns” not found”], “stdout”: “”, “stdout_lines”: []}
…ignoringTASK [master : kubeadm | create kube-dns service] ******************************************************************************************
Saturday 11 August 2018  09:39:56 +0800 (0:00:00.974)       0:01:30.196 *******
changed: [node1]TASK [master : Update kube-proxy command args] *********************************************************************************************
Saturday 11 August 2018  09:39:58 +0800 (0:00:01.722)       0:01:31.918 *******
fatal: [node1]: FAILED! => {“changed”: true, “cmd”: “kubectl -n kube-system get ds -l ‘k8s-app=kube-proxy’ -o json | jq '.items[0].spec.template.spec.containers[0].command += [”–masquerade-all"]’ | kubectl apply -f - && kubectl delete pods -n kube-system -l ‘k8s-app=kube-proxy’", “delta”: “0:00:00.570311”, “end”: “2018-08-11 09:39:58.991136”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-08-11 09:39:58.420825”, “stderr”: “error: error validating “STDIN”: error validating data: apiVersion not set; if you choose to ignore these errors, turn validation off with --validate=false”, “stderr_lines”: [“error: error validating “STDIN”: error validating data: apiVersion not set; if you choose to ignore these errors, turn validation off with --validate=false”], “stdout”: “”, “stdout_lines”: []}可能的原因是您的网络不稳定。 安装失败请参考安装失败步骤重新安装。安装失败后步骤在哪里？http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/建议你先详读文档之后再进行安装，以避免一些未知的问题。现在还是有一个报错
fatal: [node1]: FAILED! => {“changed”: true, “cmd”: “kubectl -n kube-system get ds -l ‘k8s-app=kube-proxy’ -o json | jq '.items[0].spec.template.spec.containers[0].command += [”–masquerade-all"]’ | kubectl apply -f - && kubectl delete pods -n kube-system -l ‘k8s-app=kube-proxy’", “delta”: “0:00:00.426252”, “end”: “2018-08-11 10:02:46.837490”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-08-11 10:02:46.411238”, “stderr”: “error: error validating “STDIN”: error validating data: apiVersion not set; if you choose to ignore these errors, turn validation off with --validate=false”, “stderr_lines”: [“error: error validating “STDIN”: error validating data: apiVersion not set; if you choose to ignore these errors, turn validation off with --validate=false”], “stdout”: “”, “stdout_lines”: []}失败之后请清理集群重新操作尝试了，还是同样的错误重新安装虚拟机就好了使用【敏捷管理】=>【模块管理】=>【创建模块】功能创建新模块后无法查询到数据。但数据实际上已经保存，在【问题管理】中可以选到新建的模块。这个bug已经修复，会在下一个迭代发布。我按照猪齿鱼上给的集群安装步骤来安装，结果出现如下错误。
[root@localhost kubeadm-ansible]# vagrant up
Bringing machine ‘node1’ up with ‘virtualbox’ provider…
==> node1: Importing base box ‘bento/centos-7.3’…
==> node1: Matching MAC address for NAT networking…
==> node1: Setting the name of the VM: kubeadm-ansible_node1_1533886347553_39217
==> node1: Clearing any previously set network interfaces…
==> node1: Preparing network interfaces based on configuration…
node1: Adapter 1: nat
node1: Adapter 2: hostonly
==> node1: Forwarding ports…
node1: 22 (guest) => 2222 (host) (adapter 1)
==> node1: Running ‘pre-boot’ VM customizations…
==> node1: Booting VM…
==> node1: Waiting for machine to boot. This may take a few minutes…
node1: SSH address: 127.0.0.1:2222
node1: SSH username: vagrant
node1: SSH auth method: private key
Timed out while waiting for the machine to boot. This means that
Vagrant was unable to communicate with the guest machine within
the configured (“config.vm.boot_timeout” value) time period.If you look above, you should be able to see the error(s) that
Vagrant had when attempting to connect to the machine. These errors
are usually good hints as to what may be wrong.If you’re using a custom box, make sure that networking is properly
working and you’re able to connect to the machine. It is a common
problem that networking isn’t setup properly in these boxes.
Verify that authentication configurations are also setup properly,
as well.If the box appears to be booting properly, you may want to increase
the timeout (“config.vm.boot_timeout”) value.
[root@localhost kubeadm-ansible]#请执行以下命令试一试是否正常vagrant destroy -f还是同样的错误，是需要修改IP吗？你在安装这两个软件时是否出现过异常情况？Virtualbox 5.1.34Vagrant 2.0.1没有，都是很正常的我只保留了node1节点，其他的信息，按照文档说明已经删除。
具体信息如下：
Vagrant.configure(2) do |config|(1…1).each do |i|
config.vm.define “node#{i}” do |s|
s.vm.box = “bento/centos-7.3”
s.vm.box_url = “http://file.choerodon.com.cn/vagrant/box/bento_centos-7.3.bo
x”
s.vm.hostname = “node#{i}”
n = 10 + i
s.vm.network “private_network”, ip: “192.168.56.#{n}”
s.vm.provider “virtualbox” do |v|
v.memory = 2048
end
end
endif Vagrant.has_plugin?(“vagrant-cachier”)
config.cache.scope = :box
end[root@localhost inventory]# more hosts
[all]
node1 ansible_host=192.168.56.11 ansible_user=root ansible_ssh_pass=vagrant ansi
ble_become=true[kube-master]
node1[etcd]
node1[kube-node]
node1[root@localhost inventory]#问题已经解决，是由于虚拟机没有启用虚拟化导致的。还是非常感谢Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
通过vagrant搭建的本地集群，如何通过本机访问虚拟机集群里面的dashboard？需要特殊设置吗？搭建集群后默认在kube-system namespace下创建有dashboard对应的service，请参考下面资料添加externalIPs即可。可以访问了， dashboard的登陆token如何获取？感谢，整理下脚本
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep heapster-token | awk ‘{print $1}’) | grep -E ‘^token’ | awk ‘{print $2}’Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
创建猪齿鱼前端Demo应用一直在创建中，gitlab里面已经有项目，但是项目无内容
原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问应该是偶然bug，刚刚重新创建一个又成功了。。。。。嗯，这边一致性我们0.9会采用sega的方式。Choerodon平台版本: 0.8.0遇到问题的执行步骤:
windows下用vagrant安装kubernetes集群报错：
TASK [base/prepare : Download cfssl-certinfo] *******************************************************************
changed: [node3]
fatal: [node2]: FAILED! => {“changed”: false, “dest”: “/usr/local/bin/cfssl-certinfo”, “msg”: "Request failed: ", “state”: “absent”, “url”: “http://file.choerodon.com.cn/kubernetes//cfssl/cfssl-certinf                                                      o_linux-amd64”}
fatal: [node1]: FAILED! => {“changed”: false, “dest”: “/usr/local/bin/cfssl-certinfo”, “msg”: "Request failed: ", “state”: “absent”, “url”: “http://file.choerodon.com.cn/kubernetes//cfssl/cfssl-certinf                                                      o_linux-amd64”}http://file.choerodon.com.cn/kubernetes//cfssl/cfssl-certinfo_linux-amd64，这个url 404，麻烦处理下。文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，谢谢你的反馈，给你带来不便敬请谅解，请做以下更改重新执行安装即可：出现这个错误是什么原因？
ignoring 的错误不用考虑哦虚拟机建议给多大的内存？
我vagrant up， 安装k8s集群成功之后
过一会集群就down了， 我看有些pod没有起来
猜测是内存不足， 现在给的是2G
内存加到4G之后没问题了， 但是有个dns的pod没起来， 是什么原因？
是CPU不够了哦那个dns的pod没起来是cpu不够了吗？[root@node1 .kube]# kubectl describe pod kube-dns-79d99555df-stzvj -n kube-system
Name:           kube-dns-79d99555df-stzvj
Namespace:      kube-system
Node:           
Labels:         k8s-app=kube-dns
pod-template-hash=3585511189
Annotations:    kubernetes.io/created-by={“kind”:“SerializedReference”,“apiVersion”:“v1”,“reference”:{“kind”:“ReplicaSet”,“namespace”:“kube-system”,“name”:“kube-dns-79d99555df”,“uid”:"584d3a31-9c65-11e8-88f3-08002737…
Status:         Pending
IP:
Created By:     ReplicaSet/kube-dns-79d99555df
Controlled By:  ReplicaSet/kube-dns-79d99555df
Containers:
kubedns:
Image:  registry.cn-hangzhou.aliyuncs.com/choerodon-tools/k8s-dns-kube-dns-amd64:1.14.5
Ports:  10053/UDP, 10053/TCP, 10055/TCP
Args:
–domain=cluster.local.
–dns-port=10053
–config-dir=/kube-dns-config
–v=2
Limits:
memory:  170Mi
Requests:
cpu:      100m
memory:   70Mi
Liveness:   http-get http://:10054/healthcheck/kubedns delay=60s timeout=5s period=10s #success=1 #failure=5
Readiness:  http-get http://:8081/readiness delay=3s timeout=5s period=10s #success=1 #failure=3
Environment:
PROMETHEUS_PORT:  10055
Mounts:
/kube-dns-config from kube-dns-config (rw)
/var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-94f5r (ro)
dnsmasq:
Image:  registry.cn-hangzhou.aliyuncs.com/choerodon-tools/k8s-dns-dnsmasq-nanny-amd64:1.14.5
Ports:  53/UDP, 53/TCP
Args:
-v=2
-logtostderr
-configDir=/etc/k8s/dns/dnsmasq-nanny
-restartDnsmasq=true
–
-k
–cache-size=1000
–log-facility=-
–server=/cluster.local/127.0.0.1#10053
–server=/in-addr.arpa/127.0.0.1#10053
–server=/ip6.arpa/127.0.0.1#10053
Requests:
cpu:        150m
memory:     20Mi
Liveness:     http-get http://:10054/healthcheck/dnsmasq delay=60s timeout=5s period=10s #success=1 #failure=5
Environment:  
Mounts:
/etc/k8s/dns/dnsmasq-nanny from kube-dns-config (rw)
/var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-94f5r (ro)
sidecar:
Image:  registry.cn-hangzhou.aliyuncs.com/choerodon-tools/k8s-dns-sidecar-amd64:1.14.5
Port:   10054/TCP
Args:
–v=2
–logtostderr
–probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
–probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
Requests:
cpu:        10m
memory:     20Mi
Liveness:     http-get http://:10054/metrics delay=60s timeout=5s period=10s #success=1 #failure=5
Environment:  
Mounts:
/var/run/secrets/kubernetes.io/serviceaccount from kube-dns-token-94f5r (ro)
Conditions:
Type           Status
PodScheduled   False
Volumes:
kube-dns-config:
Type:      ConfigMap (a volume populated by a ConfigMap)
Name:      kube-dns
Optional:  true
kube-dns-token-94f5r:
Type:        Secret (a volume populated by a Secret)
SecretName:  kube-dns-token-94f5r
Optional:    false
QoS Class:       Burstable
Node-Selectors:  
Tolerations:     CriticalAddonsOnly
node-role.kubernetes.io/master:NoSchedule
Events:
Type     Reason            Age                From               MessageWarning  FailedScheduling  27m (x4 over 27m)  default-scheduler  No nodes are available that match all of the predicates: NodeNotReady (1).
Warning  FailedScheduling  26m (x3 over 26m)  default-scheduler  No nodes are available that match all of the predicates: Insufficient cpu (1), NodeNotReady (1).
Warning  FailedScheduling  24m (x8 over 26m)  default-scheduler  No nodes are available that match all of the predicates: Insufficient cpu (1).
Warning  FailedScheduling  1m (x56 over 16m)  default-scheduler  No nodes are available that match all of the predicates: Insufficient cpu (1).No nodes are available that match all of the predicates: Insufficient cpu (1). 就是CPU不足的意思，你可以通过 kubectl describe no [NODE-NAME] 查看具体CPU占用情况。嗯嗯， 感谢， vagrant调整
v.memory = 4096
v.cpus = 2
之后，可以了不过kubectl我本机版本比虚拟机服务器的版本高，是不是不可连接了
现在连接报下面的错
我昨天尝试的虚拟机配置是2G内存，2核cpu，我改成8G内存，8核CPU，choerodon能跑的起来么？我这边2核4G， 能跑起来， 不是choerodon， 是k8s集群在集群安装时，每个master节点都将自动安装好kubectl命令行，不需要手动安装，请卸载你现在版本的kubectl,安装1.8.5版本建议你直接到虚拟机中执行。好的， 感谢Choerodon平台版本: 0.8.0遇到问题的执行步骤:
在Windows下，通过vagrant安装的kubernetes集群，一键部署的时候values.sh文件中的CHOERODON_API_EXTERNAL_URL，CHOERODON_DEVOPS_EXTERNAL_URL，CHOERODON_FRONT_EXTERNAL_URL，GITLAB_EXTERNAL_URL，CHARTMUSEUM_EXTERNAL_URL，MINIO_EXTERNAL_URL，HARBOR_EXTERNAL_URL，这些需要配置域名的参数，如何配置？文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问每个参数下边有具体说明的  （PS:安装Choerodon最低要求~48G内存）我知道有说明，我想问下这些参数的域名在哪配置的？比如写死host或者使用外部域名系统，我就是想简单的在本地部署好，先体验下，那我怎么配置这些域名？建议你把域名的DNS A记录解析到虚拟机的内网ip上,如果你没有域名，请参考http://choerodon.io/zh/docs/installation-configuration/steps/dns/ 中关于自主搭建DNS的教程。 直接改个人电脑的host安装可能会导致安装失败。如果按照文档中自主搭建DNS教程，那我集群中所有的虚机和我的Windows宿主机都要设置这个dns来解析？虚拟机你无需配置，本地电脑直接修改HOST即可。强烈建议使用一个真实的域名以简化安装。本地电脑直接修改HOST，是改choerodon前端的地址？values里面配置的域名都需要配的那我Windows的dns要配置成自主安装的dns么？如果你修改了host文件就无需配置谢谢，我试试，如果成功了，我就贡献一个文档。TASK [etcd : Generate Ca certs] *************************************************************************************************************************************************************
fatal: [node1 -> 192.168.56.11]: FAILED! => {“changed”: true, “cmd”: “/etc/ssl/etcd/gen_cert.sh”, “delta”: “0:00:00.003366”, “end”: “2018-08-09 21:41:06.998525”, “msg”: “non-zero return code”, “rc”: 126, “start”: “2018-08-09 21:41:06.995159”, “stderr”: “/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory”, “stderr_lines”: ["/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory"], “stdout”: “”, “stdout_lines”: []}
这个错误怎么解决？我已经把/etc/ssl/etcd/gen_cert.sh文件的格式由dos改成unix了，重新安装又变回了dos了。检查下后边的那个^Mkubernetes集群跟Choerodon已经安装成功，那我hosts文件配置values中域名和对应的ip，这个对应的ip从哪里查看？安装choerodon，得到输出：
Welcome to use ChoerodonNote: The following information is displayed only once, please make a backup.按照文档说的，这个提示就表示安装成功了，我为什么在kuberntes集群中没有查到跟choerodon有关的任何东西呢？TASK [etcd : Generate Ca certs] *************************************************************************************************************************************************************
fatal: [node1 -> 192.168.56.11]: FAILED! => {“changed”: true, “cmd”: “/etc/ssl/etcd/gen_cert.sh”, “delta”: “0:00:00.003366”, “end”: “2018-08-09 21:41:06.998525”, “msg”: “non-zero return code”, “rc”: 126, “start”: “2018-08-09 21:41:06.995159”, “stderr”: “/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory”, “stderr_lines”: ["/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory"], “stdout”: “”, “stdout_lines”: []}
这个问题很累。我只能先安装一次，出错之后就去删除kubeadm-ansible\roles\etcd\tasks\gen_etcd_certs.yml文件中的：大佬们，能提供一个通过vagrant快速体验choerodon的方式么？都是测试了的，不同机器由于磁盘网络CPU不同可能会遇到不同的问题，一般情况下 The connection to the server localhost:8080 出现这个提示是由于你不在master节点执行命令或者内存不足导致部分组件意外退出。/bin/bash^M: bad interpreter: No such file or directory，这个问题能修一下么？Choerodon平台版本：0.8.0运行环境：官方平台问题描述：昨天下午我们的k8s有一个节点出现故障无法连上网络，今天恢复了。
因为猪齿鱼更新了之后构建镜像时tag格式发生变化，k8s尝试重新创建容器时出现找不到镜像的错误
然后我们在gitlab上重新构建了镜像，然后用“升级实例”来更新镜像的地址。结果失败了，信息如下：

Kubernetes上的资源没有任何变化，部署、副本集和容器组都没有被更新。（容器状态依然可以监控）我们尝试删除实例再重新创建，操作完成后，猪齿鱼上的实例状态立即变成“创建失败”：
执行的操作：
如上文所述报错信息(请尽量使用代码块或系统截图的形式展现)：
没有找到详细的报错信息建议：
我们还有一个需要持续更新版本的服务，使用的时候是由我们的系统随时创建Docker镜像的。它的Docker镜像托管在猪齿鱼平台上，我们必须要天天构建一次来维持镜像不被删除，以保持我们的功能能够正常使用。希望能有一个永久保存镜像的功能。问题解决了Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：部署流水线，创建网络无法选择实例
重现步骤：
先选择应用，选择版本，这时是可以选择实例的；切换应用，再选择版本，这时则无法选择实例，提示“请先选择版本”应用是否在运行中呢在的，场景就是我想创建基于应用A实例的网络，但在选择实例时才发现前面选择的应用及版本都是B的，这时候修改应用名称，再选择版本后，在选择实例时无可选实例bug我们已记录，感谢反馈。Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost疑问：外部IP是只发布POD的节点的IP吗？如果我这个pod需要k8s集群外部访问这里应该如何设置？外部ip可以是你的集群中任意节点的IP， 如果你需要外部访问，建议创建网络和域名。集群是我本机搭建的， 好像不能设置localhost?一般你无需填写外部ip当我运行下面的命令的时候，报错Error: Get https://192.168.99.100:8443/api/v1/namespaces/kube-system/pods?labelSelector=app%3Dhelm%2Cname%3Dtiller: dial tcp 192.168.99.100:8443: connect: connection refused，请问怎样解决？
helm install --repo=http://chart.choerodon.com.cn/choerodon/c7ncd/ 
–namespace=dev 
–name=dev 
–version=0.8.0 
–set config.connect=ws://devops.service.choerodon.com.cn/agent/ 
–set config.token=0e48005d-193a-49f3-8956-0def687a44ee 
–set config.envId=106 
–set rbac.create=true 
choerodon-agent请参考文档中关于kubernetes集群安装文档，校验kubernetes集群正确安装。后续命令操作请在Master节点执行。如果你刚接触Kubernetes集群，强烈建议使用我们提供的安装方式安装kubernetes集群。在 前端开发手册 -> 基础环境准备 -> 代码运行 章节，进入到项目根目录(choerodon-front-iam/iam)，打开终端，在终端中执行命令 npm install是报错。



你好，请确保本地的nodejs环境安装正常nodejs安装正常
https://cnodejs.org/topic/55704ef1693bb2265dfba10e
这里有一个年代久远的解决方法
执行npm config set msvs_version 2015 --global 解决PS：如果使用windows开发强烈建议终端环境使用 git bash通过 pip  安装 PyYAML ， PyMySQL 。打开  git bash  执行  pip install PyYAML PyMySQL ，然后安装完成之后执行 pip list  查看安装的版本。上面的pip安装以及pip install PyYAML PyMySQL 应该在cmd里面执行吧。git bash就不认命令
在cmd里面就可以对pip进行安装 并且pip install PyYAML PyMySQL也能执行。你们的手册是不是需要完善并验证一下呢我也这么觉得，前端的开发手册就是意思一下，很多问题需要自己去解决，包括bash环境，环境变量，很魔法的boot等，很多历史遗留问题都是有历史原因的。。。git bash找不到pip可以尝试一下这个连接里的解决方案。


stackoverflow.com






pip install gives me this error "can't open file 'pip': [Errno 2] No such file or directory"


python, python-2.7, python-3.x, remote-access


  asked by
  
  
    Bipin Shetty
  
  on 10:20PM - 04 Mar 17






执行bash环境是什么，另外 这个在cmd里面可以执行这个的  python -m pip install pypiwin32
git bash里面执行不通的Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：
部署nginx-demo到本地环境失败问题描述：k8s dashboard显示无法pull镜像
原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问你好，可以查看下ci流水线的日志，或者直接查看镜像库中是否为有该镜像CI流水线正常， 看日志也是正常push了
能否粘贴一下您部署时候的值在哪里看部署时候的值？没有明白您的意思@coder-zhw 应用部署界面，或者实例详情中能够找到：另外我helm初始化的时候执行的命令是这样的
‘’’
helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
‘’’是否有影响？建议你先把部署文件中的 tag 替换为版本号。修改哪里？抱歉新手根据文档http://choerodon.io/zh/docs/quick-start/nginx-demo/
只修改了下面的两处地方， 不修改ci报错
1.修改gitlab-ci.yml文件注释掉2.修改Dockerfile, 猪齿鱼镜像库里面的这个镜像拉不下来
感谢提醒 我们会尽快修正。部署文件中的 tag 替换为版本号这个是修改部署实例里面的那个吗？另外修改成哪个版本号呢？我修改的这两个地方有问题吗？那个docker login是需要登录的吗？也就是你 CI中的镜像 tag问题及解决， 原因是部署文件的tag不对Choerodon平台版本: 0.8.0遇到问题的执行步骤:创建环境之后通过指令进行激活环境
helm install --repo=http://chart.choerodon.com.cn/choerodon/c7ncd/ 
–namespace=hwms-dev 
–name=hwms-dev 
–version=0.8.0 
–set config.connect=ws://devops.service.choerodon.com.cn/agent/ 
–set config.token=81423ece-c08d-4d38-a873-7f1047c8ea92 
–set config.envId=108 
–set rbac.create=true 
choerodon-agent文档地址:http://choerodon.io/zh/docs/user-guide/deployment-pipeline/environment-pipeline/环境信息(如:节点信息):localhost minikube集群报错日志:
Error: release hwms-dev failed: roles.rbac.authorization.k8s.io “hwms-dev” is fo
rbidden: attempt to grant extra privileges: [PolicyRule{Resources:[""], APIGrou
ps:[""], Verbs:["*"]}] user=&{system:serviceaccount:kube-system:helm-tiller 344
fdb49-9b97-11e8-80dd-080027fa4a88 [system:serviceaccounts system:serviceaccounts
:kube-system system:authenticated] map[]} ownerrules=[] ruleResolutionErrors=[cl
usterroles.rbac.authorization.k8s.io “cluster-admin” not found]原因分析:已加 ServiceAccount
kubectl create serviceaccount --namespace kube-system helm-tiller
kubectl create clusterrolebinding helm-tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:helm-tiller
helm init --tiller-image=registry.cn-shanghai.aliyuncs.com/choerodon/tiller:v2.8.2 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts --service-account=helm-tiller疑问:提出您对于遇到和解决该问题时的疑问查看下 kubectl get clusterrole 输出什么 ?$ kubectl get clusterrole
No resources found.执行 kubectl version 查看下您的k8s版本，建议使用1.8.5版本$ kubectl version
Client Version: version.Info{Major:“1”, Minor:“8”, GitVersion:“v1.8.5”, GitCommi
t:“cce11c6a185279d037023e02ac5249e14daa22bf”, GitTreeState:“clean”, BuildDate:“2
017-12-07T16:16:03Z”, GoVersion:“go1.8.3”, Compiler:“gc”, Platform:“windows/amd6
4”}
Server Version: version.Info{Major:"", Minor:"", GitVersion:“v1.9.4”, GitCommit:
“bee2d1505c4fe820744d26d41ecd3fdd4a3d6546”, GitTreeState:“dirty”, BuildDate:“201
8-03-25T05:35:21Z”, GoVersion:“go1.9.1”, Compiler:“gc”, Platform:“linux/amd64”}一定要1.8.5版本吗？我重新执行了一次指令，报了另外一个错误。
$ helm install --repo=http://chart.choerodon.com.cn/choerodon/c7ncd/ \Error: Looks like “http://chart.choerodon.com.cn/choerodon/c7ncd/” is not a vali
d chart repository or cannot be reached: Get http://chart.choerodon.com.cn/choer
odon/c7ncd/index.yaml: read tcp 10.2.64.82:9698->47.100.198.117:80: wsarecv: An
existing connection was forcibly closed by the remote host.执行 curl http://chart.choerodon.com.cn/choerodon/c7ncd/index.yaml 看看截取了一部分如下：看上去没有问题，再次执行helm install 报错吗？再次执行报错hwms-dev already exists。
$ helm install --repo=http://chart.choerodon.com.cn/choerodon/c7ncd/ \Error: a release named hwms-dev already exists.
Run: helm ls --all hwms-dev; to check the status of the release
Or run: helm del --purge hwms-dev; to delete it执行helm ls --all hwms-dev
$ helm ls --all hwms-dev
NAME            REVISION        UPDATED                         STATUS  CHART
NAMESPACE
hwms-dev        1               Thu Aug  9 15:28:05 2018        FAILED  choerodo
n-agent-0.8.0   hwms-dev这因为以及有一个同名的hwms-dev, 按照提示删除 重新安装。这个我也试过，去掉之后又报最初 release hwms-dev failed: roles.rbac.authorization.k8s.io “hwms-dev” is fo
rbidden: attempt to grant extra privileges:的错误如果您的集群没有开启rbac
请将此处
--set rbac.create=true \
设为false。 建议您使用我们的教程安装kubernetes集群以避免类似问题。Choerodon平台版本: 0.8.0遇到问题的执行步骤:部署猪齿鱼的服务器，缓存内存过多
我是否可以使用  echo 1 > /proc/sys/vm/drop_caches  来清除缓存不建议清理，可能会导致数据丢失。请问有什么方案可以合理的使用服务器的内存资源，目前我们一台服务器32G，只能用到10G左右，就不能再装入pod了。k8s会根据您申请的CPU和内存进行调度，如果您的服务申请了较高的CPU,即使内存很大也不能够运行新的PODChoerodon平台版本：0.6.4运行环境：自主搭建问题描述：
在前端开发新页面
成功之后在项目根目录会生成config.yml文件。然后再执行命令
$ python ./demo/node_modules/choerodon-front-boot/structure/sql.py -i ip地址 -p 端口号 -u 用户名 -s 密码
这里的-i ip地址 -p 端口号 -u 用户名 -s 密码  我这分别给了 localhost 3306 root 123456
发现需要数据库的信息，这些数据里表结构从哪获取
你的choerodon-front-boot版本是0.6.4吧 这是用来配置目录的脚本，数据库是choerodon后端的数据库，因为menu菜单是写在数据库里的，路由是写在js代码里的，没有这个数据库的话可以先不配菜单，写好路由就可以通过输入url来访问写的页面。Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：wiki管理的入口太深了，建议提到外面去嗯嗯，多谢提醒，我们会再慎重考虑下wiki管理的菜单定位的:hugs:Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问
我的域名配置如下：
CHOERODON_API_EXTERNAL_URL=“api.api.tuiyun.com”
CHOERODON_DEVOPS_EXTERNAL_URL=“devops.api.tuiyun.com”
CHOERODON_FRONT_EXTERNAL_URL=“api.tuiyun.com”
HARBOR_EXTERNAL_URL=“registry.api.tuiyun.com”一键部署的value.sh里面有很多域名配置，我把choerodon都通过一键部署都安装在一台服务器上了，IP为123.207.187.83，
我有自己的域名api.tuiyun.com,已经将api.tuiyun.com解析到123.207.187.83了，其他的那些比如api.api.tuiyun.com三级域名也要解析到123.207.187.83么？都是同一台机子，我就不太明白需要怎么解析。是的 你可以把 api.tuiyun.com , *.api.tuiyun.com A记录解析到任意Master节点的ip地址上Choerodon平台版本：0.6.0运行环境：HAND公司搭建的猪齿鱼问题描述：在敏捷管理中，创建问题之后在问题管理菜单点击 导出，没有反应，无法正常导出问题。可能是网络问题，刷新重试下刷新了很多次，没有效果。F12打开浏览器Console出现 502的http状态码。Console报跨域。Failed to load https://api.choerodon.com.cn/zuul/agile/v1/projects/85/issues/export: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. Origin ‘https://choerodon.com.cn’ is therefore not allowed access. The response had HTTP status code 502.请问浏览器是？Chrome谢谢反馈，我们会尽快修复现在可以在别的浏览器（如Edge）下进行导出。我们会尽快修复。Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost问题描述：
新建  choerodon-cloud-front 完后
git clone https://github.com/choerodon/choerodon-front-iam.git
再运行代码npm install后报错
$ npm install
npm ERR! code ETARGET
npm ERR! notarget No matching version found for choerodon-front-boot@0.7.0
npm ERR! notarget In most cases you or one of your dependencies are requesting
npm ERR! notarget a package version that doesn’t exist.
npm ERR! notarget
npm ERR! notarget It was specified as a dependency of ‘choerodon-front-iam’
npm ERR! notarget要修改choerodon-front-boot的版本还是有其他解决办法？你好，这是因为npm公库缺少front-boot的0.7.0的依赖，我们马上发布一下Choerodon平台版本: 0.8.0遇到问题的执行步骤:
sh choerodon-install.sh values.sh文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):报错日志:
都提示安装成功了，还是报这个not ready域名报404
http://api.tuiyun.com/
提出您分析问题的过程,以便我们能更准确的找到问题所在
就一台测试机子提出您对于遇到和解决该问题时的疑问
一直提示choerodon-manager-service not ready ，下一步要如何操作呢安装时候需要耐心等待，切勿取消。一般情况下微服务启动容器需要下载镜像和启动服务。下载镜像速度取决于您的网速。Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：问题管理 不能按时间、人员筛选已经增加了按照人员（经办人，报告人）进行模糊搜索筛选。将在下个版本发布，还有冲刺，版本，史诗等筛选项。另外这个时间是指什么时间？按问题的完成时间筛选，比如查看当月的我的已完成的问题，统计当月工作量根据你的描述猜想，这一个月是属于某个活跃冲刺的，我们提供了过滤器的功能。可以在设置——快速搜索中创建快速搜索，然后在待办事项中，用这个过滤器过滤问题进行查看。如果你想要的不是当前冲刺的情况，这个功能我们会记录并进行评估。Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：对于在改环境中的所有操作都会发生这个错误吗。Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。菜单的图标是怎么做的，图标网址是多少？

图标网址 http://ui.choerodon.io/components/icon-cn/好的，谢谢Choerodon平台版本: 0.6.0遇到问题的执行步骤:
sh choerodon-install.sh values.sh文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):
单机单节点
kafka默认安装也是这个错，后面我干脆直接单独安装，还是报这个错！到底要怎么做才不报错啊
报错日志:
Error from server (NotFound): statefulsets.apps “choerodon-kafka” not found原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问能否粘贴这个报错前后的日志你好，由于脚本问题给你带来不便敬请谅解，请再次按照安装文档命令执行，脚本已修复这个bug。Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：我的当前冲刺时间是从7月30日到8月5日（其中4日、5日是周末，按照猪齿鱼的设定，周末可能不会算作冲刺时间中），如上图，我有两个疑问：疑问1：最后结束的那个时间为什么显示的是7月19日，是一个跟当前冲刺毫无关系的时间
疑问2：燃尽图的基本单位是什么，从图上来看，似乎不是以天为单位，也不是以时为单位，看不出是以什么为单位计量出来的燃尽图目前版本的燃尽图是根据当前冲刺操作的相关记录进行显示计算的，如果冲刺正在进行则时间为当前时间。
该设计目前经过讨论和实际使用发现产生的疑问较多，因此燃尽图会在下个版本进行功能修改，会修改为：
1、结束时间为冲刺的预计结束时间，若冲刺结束后则为实际结束时间
2、x轴的时间刻改为天
3、周末不会从中摘除，由于目前还未嵌入工作日历相关功能，所以休息日相关的计算逻辑会在较晚的版本进行修改支持您的提问目前收到的类似反馈较多，会尽快进行优化修改针对第3点，目前冲刺管理那里，也就是看板那里，右上角显示的冲刺剩余时间是自动的排除了周六周日，建议统一逻辑，燃尽图、看板、冲刺等均保持一致的逻辑。好的，我们会对相关逻辑进行统一Choerodon平台版本：0.8.0运行环境：hand 猪齿鱼平台问题描述：
如下图：从昨晚到现在，CI一直等待
请问你是哪个项目呢？Choerodon平台版本: 0.6.0遇到问题的执行步骤:按照官方步骤，执行到一键安装choerodon的时候报错
sh choerodon-install.sh values.sh文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):报错日志:choerodon-install.sh: line 214: helm: command not found[Step 3]: checking job …[Step 4]: checking helm package and check its version …
?Not install helm,install its.
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
Dload  Upload   Total   Spent    Left  Speed
100 14.2M  100 14.2M    0     0  2963k      0  0:00:04  0:00:04 --:–:-- 2930k
linux-amd64/
linux-amd64/helm
linux-amd64/README.md
linux-amd64/LICENSE
Creating /root/.helm
Creating /root/.helm/repository
Creating /root/.helm/repository/cache
Creating /root/.helm/repository/local
Creating /root/.helm/plugins
Creating /root/.helm/starters
Creating /root/.helm/cache/archive
Creating /root/.helm/repository/repositories.yaml
Adding stable repo with URL: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
Adding local repo with URL: http://127.0.0.1:8879/charts
$HELM_HOME has been configured at /root/.helm.Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.Please note: by default, Tiller is deployed with an insecure ‘allow unauthenticated users’ policy.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation
Happy Helming!
?Success installed helm-v2.8.2.[Step 5]: add helm repositories …
“c7n” has been added to your repositories
Hang tight while we grab the latest from your chart repositories…
…Skip local chart repository
…Successfully got an update from the “c7n” chart repository
…Successfully got an update from the “stable” chart repository
Update Complete. ?Happy Helming!?
?Success add helm repositories.[Step 6]: checking tiller ready …
?Tiller is ready.[Step 7]: install Mysql for Choerodon …
Error: unknown flag: --dry-run
Error: unknown flag: --dry-run
?Success of install Mysql for Choerodon.[Step 8]: checking Mysql ready …
?Mysql is ready.[Step 9]: create database for Choerodon …
Error: unknown flag: --dry-run
?create database failed提出您分析问题的过程,以便我们能更准确的找到问题所在valuse.sh我复制官网的，就改了下nfs地址和域名地址,其他没动提出您对于遇到和解决该问题时的疑问
按照官方文档部署，老是不成功，到一键安装又报错，麻烦指导一下，谢谢啊建议您先将values中的DEBUG="–debug --dry-run"去掉。（PS:你是否自己装了其他版本的HELM）报这个错 是什么问题啊这是你的编辑器保存时候自动在每行末尾加了/r，通常使用Windows自带编辑器会导致这个问题 建议你使用VS code或其他代码编辑器编辑。编辑器保存时候自动在每行末尾加了/r我用vscode 用VI编辑后保存，还是报这个错！还有别的原因导致整个错误么
因为已经有这个符号了，即使你换编辑器重新保存也存在。你需要删除/r， 或者创建一个新的文件现在OK了 但是创建数据库会报timeout 这个如何解决啊请参考下失败重试的步骤。Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：目前做一个项目需要和SalesForce对接，请问Choerodon在接口通信方面有哪些服务器安全性的配置.你是指Choerodon的api接口吗一方面是Choerodon微服务开发框架的api接口，另一方面是 基于 Choerodon Framework 实现的业务应用的api接口。关于接口方面的安全你可以参考下这篇http://choerodon.io/zh/docs/concept/security/Choerodon平台版本：0.6.0.RELEASE运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：Controller或者Service中，DetailsHelper.getUserDetails获取不到认证信息,请问yml是还要做什么特别的设置吗？还是说要通过Feign呢?本地运行，启动了api-gateway,gateway-helper,ouath-server了么？登录之后才可以通过DetailsHelper.getUserDetails获取起了耶，本地起了api-gateway,gateway-helper,ouath-server,manager-service，奇了怪了。知道啦。没事啦，哈哈执行时报错
[root@centos-linux bin]# kubectl cluster-info
Kubernetes master is running at http://localhost:8080To further debug and diagnose cluster problems, use ‘kubectl cluster-info dump’.
The connection to the server localhost:8080 was refused - did you specify the right host or port?请问你是按照我们的文档安装吗？是的，我在创建环境的时候，需要先安装kubectl，在安装的过程中，报错了麻烦贴一下需要安装kubectl的文档地址 谢谢服务集成websocket之后，如何通过认证拿到httpsession呢？你好你的问题不是很明确而且问题提的不规范。哦，这样的。
原来是想问下怎么在微服务环境下搭websocket，然后websocket建立连接的时候怎么拿到当前的Http Session，现在不管这个了。
现在想问下大佬，choerodon-socket-helper，这个是怎么用的哇，我现在是集成到自己的服务里了，应该用哪个方法去推送消息给订阅者呢？前端直接用websocket(ws://)连接就行了对吧是这样的，这里我写了一个例子，直接可以跑，需要依赖redis。https://github.com/crockitwood/websocket-sample/blob/master/choerodon-websocket-helper.md哇哦。多谢好的，多谢。我试试废了九牛二虎，终于把项目跑起来了。
然后大佬，我想问两个问题，
1.websocket放在子服务了，怎么通过网关路由过去呢
2.现在集成猪齿鱼，websocket的怎么通过权限认证呢明白。谢谢Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：本地consumer 引入的event-consumer版本 0.5.0（0.5.1）
本地使用的event服务是能下载的最新的版本



GitHub



choerodon/event-store-service
event-store-service - Event Store Service implements data consistency,and the message queue kafka is supported.





event-store-service - Event Store Service implements data consistency,and the message queue kafka is supported.问题描述：在consumer 设置消息失败策略为 send_back_event_store 的时候，失败消息一直不能回传，开启io.choerodon.event 和 org.springframework.web 的logger 发现报的是401，之后强行改了（POST）v1/messages/failed 接口为public，数据能发送过来，但是event服务里会报数据错误。发现consumer 发送的消息结构和event服务接口接受的消息不匹配，而且event服务的数据库payload字段是必输的，导致问题。
您好，目前失败策略目前只支持nothing。我们实现了新的实现数据一致性的方法，正在内部测试，即将发布,您可以看下使用demo。https://github.com/flyleft/spring-cloud-base/tree/master/asgard-saga-demo好的  了解了Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：菜单分配权限与角色分配权限有什么关系，为什么不可以角色分配菜单？菜单分配权限用于进入该菜单时需要检查的权限，如果没有这些权限就不会显示该菜单。
角色可以理解为一组权限的集合，拥有某角色的用户能够进行相应权限的操作。菜单和角色有关系吗？因为之前hap是按角色单去分配菜单的没有那菜单只能是按照全局、组织、项目、用户来分配是吗，只能接口导入是吗？菜单分配可不可以加一个角色都是根据业务功能来设计的。
角色分配权限可以控制到每个api，也就是在同一个页面里，按钮的权限也可以进行控制，例如我们平台项目所有者可以进行创建应用，而项目成员只能查看应用信息。菜单是不可以在页面上分配权限的，只能通过前端初始化进数据库。菜单上的权限是指用户在进入系统后，根据用户的角色分配来判断是否显示该菜单。更多的是用于前台用户体验。角色权限是将一个角色和多条权限进行关联，然后再将角色在不同层级下分配给用户，则用户拥有该层级对应角色的所有权限。所以菜单和角色其实没有强关联。只是都和权限有所关联。请问根据用户的角色分配来判断是否显示该菜单，具体操作是不是：根据角色id，先查iam_role_permission表的permission_id，根据permission_id，查iam_menu_permission表的menu_id?如果在角色分配权限的时候，去掉一个页面按钮的权限，是不是这个按钮就失效了呢？去掉一个页面按钮的权限，不是这个按钮失效；
而是用户通过分配这个角色仍没有权限进行这个按钮的操作。关于角色分配的所有判断都是基于iam_member_role 这张表的，通过用户、层级、组织ID/项目ID 查到角色，然后才关联到permission上。具体的代码逻辑你可以参考下。



GitHub



choerodon/iam-service
iam-service - IAM Service is used for the management of user, role, permission, organization, project, password policy, fast code, client, menu, icon, multi-language , and supports for importing th...





iam-service - IAM Service is used for the management of user, role, permission, organization, project, password policy, fast code, client, menu, icon, multi-language , and supports for importing th...页面上的控件权限只做显示控制，接口的实际权限是在发起请求是判断的。好的，谢谢Choerodon平台版本: 0.8.0遇到问题的执行步骤:
发现在node1上执行不了 kubectl命令，去其他master节点上 使用 kubectl get node ，发现 node1 的状态不正常报错日志:原因分析:
一开始 先发现 choerodon-gateway-helper 这个服务坏了，想要通过pod的详情及日志，来追踪这个服务报错的信息。当使用了 kubectl describe pod choerodon-gateway-helper-88fbd9574-pn8jk -n choerodon-devops-prod
提出您分析问题的过程,以便我们能更准确的找到问题所在一般情况下，资源不足会造成NotReady，如磁盘不足，cpu或内存耗尽等。建议检查该节点资源情况，重启kubelet和docker。我重启过node1 几次，但过一段已经 就会 NotReady。查看了 内存 cpu 和 磁盘 都正常。可以参考这个issue


github.com/kubernetes/kubernetes





Issue: Node stay in NotReady state (Ubuntu Single Node)


	opened by artto
	on 2015-04-04


	closed by roberthbailey
	on 2017-06-13


kubectl get nodes
NAME        LABELS       STATUS
127.0.0.1    <none>       NotReady

Therefore all pods stay in Pending state. What might be the reason?

kubectl get...

kind/documentation
priority/backlog
sig/cluster-lifecycle
triage/support






Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost问题描述：目前用的注册中心是eureka,注册中心不能发消息，如果换成go-register-server的话不能注册到swagger上去，所以相对go-register-server进行扩展，开发go语言用什么开发工具好呢？eureka注册中心也能发消息到kafka。开发go一般用gogland你指的是不能混合开发吗是的，我们这边统一用的混合开发清一下缓存已经ok在看板的快速搜索中，只能搜到经办人的问题，不能搜索到报告者的问题，能否增加该部分的快捷搜索？
目前可以用项目所有者权限在快速搜索的功能中添加报告人的筛选器完成该需求。如下图，设置之后相关的筛选器可以在快速搜索的工具栏中生效显示：
这里的全文搜索过滤功能已经在后续的迭代中进行了规划，但是根据功能开发计划可能会在稍晚的版本进行开发OK，尝试过了，设置选项还是比较丰富的，感谢！Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：https://choerodon.com.cn/问题描述：
在敏捷管理-活跃冲刺中，对子任务创建分支后，在开发流水线-分支中的分支列表中，显示的对应的创建者不是我本人。
报错信息：
创建者有误，并且没有用显示的创建者的账号登陆过猪齿鱼平台。感谢反馈，我们排查一下出错原因。是通过token获取当前用户信息的，可以通过如下方式测试是否为登录用户错误
1.创建一个任务
2.在详情中点击指派给我（经办人和报告人都行），如果指派成功且为你想要的用户，那么确认为平台bug，请告知我们。
反之可能是因为某种原因导致token被缓存，我们会进行排查Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：如上图，修改了域名为一个现有的域名，会正常提示域名路径已经存在。但是在返回到列表页面的时候，会一直转圈表示更新中，其实应该已经不在更新了吧好的，我们已记录下，排查原因，会在这个版本里解决。平台相关服务部署运行，部分服务访问异常，问题点如下：首页登录一直等待响应，后续跳转到如下页面：
sonarqube访问异常，卡在首页，如下图：
检查域名是否能访问， sonarqube请查看网络请求。注意区分80和443sonarqube访问：
选择这个ALL再看一下
@fuchen 检查下您的数据库sonar用户密码是否配置正确。Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：k8s问题描述：k8s运行的是 go-register-server,自己的微服务运行没有报错，并且有打印日志：18090 - registration status: 200。/eureka/apps没有看到注册信息你好，请确保go-register-server 有配置变量REGISTER_SERVICE_NAMESPACE: 服务namespace名 是不是这么配置的？已经解决，谢谢！go-register-server - The microservice registration is implemented by monitoring the state changes of the k8s pod, and pull the interface in the spring cloud eureka client service list.api swagger访问，无法显示相关模块服务页面js提示报错你好，请确认下manager-service中mgmt_route 表中有没有数据，也可以通过管理员登录到系统，在
管理->微服务管理->路由管理 中查看是否有数据是有数据的，如下图：
Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：
新建一个用户，挂在一个新的组织下面报错信息(请尽量使用代码块的形式展现)：
原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问已解决，权限没有刷进去你好，感谢你的反馈。我们已经注意到这个问题，关于权限自动刷入的功能会在后续版本中得到改进Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：k8s问题描述：如：微服务注册到k8s，swagger-ui 页面报错：500org.springframework.remoting.RemoteAccessException: fetch failed, instance:HITOM-BACKEND你好，请确保你的服务已经注册到注册中心，且在 管理-> 微服务管理 -> 路由管理中 有对应服务的路由信息Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：我们这边环境很多，一个项目组下面有些应用会部署在阿里云，有些是IDC，有些在腾讯云，并且可能都会存在dev、sit、uat、prod等多套环境，目前环境这里是全部显示在一排，可否考虑加个环境分组或者标签，比如腾讯云分组可以显示在一排、阿里云分组可以显示在一排，按分组来增加排数。。。。。以上，看看有什么好的解决方案ok,我们记下了，感谢反馈~~~部署时一直处于处理中，无法执行其他操作，也不可以删除，这个怎么解决请问是自己搭建的Choerodon平台吗？不是，choerodon是公司提供的，kubernetes环境是自己的现在好了吗刚刚服务器出现了 问题我们修复了一下
重新部署是可以的，但是以前的处理中的一直是处理中，无法删除。这个处理多久就会失败啊？失败了才可以删除这些正在处理中的任务有没有办法删除呢？我们修复了一下，还有处理中的吗没有了Choerodon平台版本: 0.8.0遇到问题的执行步骤:通过分步安装的，安装后各个组件都是正常的
文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，麻烦检查下，iam_service数据库中，iam_permission表 和
manager_service 数据库中，mgmt_route 表中是否有数据iam_permission表里是没有iam-service的权限吗？没有的话可以调用这个手动刷新接口，手动刷下权限。iam-service{
“timestamp”: “2018-07-26 17:30:07”,
“status”: 500,
“error”: “Internal Server Error”,
“exception”: “org.springframework.dao.DataIntegrityViolationException”,
“message”: “Request processing failed; nested exception is org.springframework.dao.DataIntegrityViolationException: \n### Error updating database.  Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Column ‘value’ cannot be null\n### The error may involve io.choerodon.manager.infra.mapper.SwaggerMapper.insert-Inline\n### The error occurred while setting parameters\n### SQL: INSERT INTO mgmt_swagger  ( creation_date,created_by,last_update_date,last_updated_by,object_version_number,id,service_name,service_version,is_default,value ) VALUES( ?,?,?,?,1,?,?,?,?,? )\n### Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Column ‘value’ cannot be null\n; SQL []; Column ‘value’ cannot be null; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Column ‘value’ cannot be null”,
“path”: “/docs/permission/refresh/iam-service”
}知道了，还要加上版本号，可以了。更新后，应用市场还是报403.我试着更新devops-service，0.8.0,看起来是找不到跟这个对应的权限，报错了。手动刷新下devops-service权限。你可以追踪下权限刷新不进去的原因，可以看下go-register-server -> manager-server -> iam-service。目前我们自动刷新权限依赖于kafka和注册中心。服务启动register-server向kafka发送，manager接收之后iam接收刷新权限。有一步出问题，就可能刷新不出权限，这种方法容易出问题，我们正在考虑其他方式实现。Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：
提交代码后，自动mvn package，通过
第二步点击结束分支时，stage docker_build 失败
这个时候，如果想再次进行docker build 的话，只能重新新建分支，git clone 代码
提交,然后点击结束分支才能再次触发stage docker build吗？有没有更快捷的方式点击retry按钮即可重新运行~点击retry 重新运行他会运行我最新提交的代码吗？还是只运行我结束分支时提交的代码？运行的还是触发pipeline时的那个commit id对应的代码那这样的话如果我是代码有问题，但是已经结束分支了，docker build 失败。如果需要更新代码重新docker build 的话 。只能新建一个分支再次提交，结束分支吗 来触发docker build吗？你看一下你的提交是否合并了，如果合并进去了就不影响，只要重新建一个分支修改代码再合并即可。没合并进去要试试能不能取得到刚刚的那个提交再做修改。另外，点击失败的docker build可以看具体的报错，采取对应的解决方案。对，我的意思就是我已经采取解决方案了，但是就必须的新建feature分支，再拉下来提交才能再次docker build 是吧？ 嗯，明白是的问题描述：首页重复发无效请求问题截图：感谢你的反馈，这个问题我们会尽快修复。Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：我是按照文档中后端开发手册，开发一个demo程序执行
在应用部署时，实例无法启动，输出下面日志信息下面是我的gitlab-ci文件pom.xml根据网上的解释，说是少了spring-boot-starter-web 依赖
我加上之后，还是报同样的错误，另外一些说法是，少了spring-boot-starter-tomcat依赖，我也加上了
好像还是不行，也还是同样的输出。
这个请问大家知道怎么解决吗？你好，你发的日志是初始化数据库的日志，不是实例实际的运行日志那请问一下，这个错误应该怎么解决！日志上没有报错啊。。初始化结束后，正常的输出就是下面这样我还以为这个日志输出是启动失败的原因，因为我的应用没有启动起来下面这个截图是我的实例列表上提示的信息，就是报了一个这个错误
就是Demo的部署阶段job报错了,你看看是不是初始化配置出错了，第二个阶段的日志好，我看看！Choerodon平台版本: 0.8.0遇到问题的执行步骤:test manager service 和test manager font 都没有创建service然后验证前端是否已经安装成功的时候，因为没有svc，也验证不通过。
curl $(kubectl get svc choerodon-front-test-manager -o jsonpath="{.spec.clusterIP}" -n choerodon-devops-prod)文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问验证测试管理后端部署是否成功的命令为：如果你部署了整合前端0.8.*版本，那么其他分前端可以不用部署验证整合前端部署是否成功的命令为：验证测试管理前端部署是否成功的命令为：查看你所提供的截图，貌似你并未部署测试管理后端和测试管理前端，请部署后执行以上验证命令。由于测试管理后端不需要外部直接访问，所以没有创建测试管理后端的svc。测试管理后端和测试管理前端都部署了。测试管理前端也没有创建svc。其他那几个微服务后端需要外部直接访问吗？*如果你部署了整合前端0.8.版本，那么其他分前端可以不用部署意思是说如果 choerodon front这个部署了， choerodon agile/test/iam front这些前端就不用部署了？是的Choerodon平台版本: 0.8.0遇到问题的执行步骤:
helm install c7n/postgresql 
–set persistence.enabled=true 
–set persistence.existingClaim=sonarqube-postgresql-pvc 
–set env.open.POSTGRES_USER=admin 
–set env.open.POSTGRES_PASSWORD=password 
–set env.open.POSTGRES_DB=sonar 
–name sonarqube-postgresql --namespace=choerodon-devops-prod文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问解决了。需要POSTGRES_USER需要跟宿主机的账号相同
改成root就好了。你们主要使用Psql？按着教程装，出的问题。psql就是给sonarqube用的吧。有很多项目都是用excel表管理用例。
希望Choerodon Team开发一个可以从excel表导入用例的功能。
Zephyr的用例功能是很好用的，希望Choerodon有一个类似Zephyr的功能。
谢谢。请Choerodon Team 丰富用例统计功能。
用系统来管理用例，除了用系统来清晰的管理用例，快速分配到测试人员，还希望通过系统的统计功能，以从用例执行状态，分配到人执行状态，整体的执行进度。等多个维度反映用例的执行情况。谢谢。您好，谢谢您的耐心反馈，我们会参考您的意见和建议对未来版本迭代进行规划。Choeeodon的测试管理也是参考Zephyr设计的参考好的设计，让Choerodon变得更强大。麻烦帮忙看一下这个ci问题是什么原因，谢谢
现在我们正式环境的持续交付服务出现了故障，正在修复中！好的，谢谢Choerodon平台版本：0.8.0运行环境：https://choerodon.com.cn/问题描述：开发流水线->分支管理,界面非常慢好的 我们排查一下原因~Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：我这里已经将readinessProbe下的配置都删掉了，
但是点下一步到部署预览时，神奇的又给加上去了我将httpGet修改为exex command:
部署预览的时候，两者合并到一块了。
这个问题我们尽快修复。现在部署的平台中devops-service的版本是什么？你好 暂不支持无root权限的用户安装。我这拿到的用户是无root权限的，那这个k8s集群的安装还有其他方案吗？安装k8s集群需要 root权限 如果没有root权限是暂时无法安装的。建议你申请root权限了解，那我这向上反馈下结果吧，谢谢了哈ansible_user这个必须得是root用户吗？我用其他用户root权限的用户，sudo 执行ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml这个可以不？ansible_user需要有root权限，可以是有root权限的其他用户。使用ansible本质上就是使用特定用户登录到远程机器上执行操作，所以sudo执行 ansible-playbook 和 直接执行并无任何区别麻烦给瞅瞅这个咋解决？
如果你是一个需要密码的sudo用户，在执行 ansible-playbook 后边 加上 一个参数 -K ，关于ansible的具体操作请参考 http://www.ansible.com.cn/index.htmlChoerodon平台版本：0.7.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：
结束分支时，执行docker_build失败
问题描述：求教应该怎么解决，谢谢！@notsaltfish  这是你的证书无效。这种自定义的域名还是在docker的配置文件中添加insecure-registry 比较好
@404 如果配置了insecure-registry， 那docker是需要重启的吧，如果重启的话，猪齿鱼所有的服务会自动恢复吗？会自动恢复的你好，我添加了insecure-registry， 重启docker之后，gitlab再次docker_build 报另外一个错误，请问这个是怎么解决？谢谢！
检查下你的 ingress是否配置了 http://registry.example.choerodon.io 这个域名哦， 注意是 http不是https的。我直接从从本地浏览器输入url http://registry.example.choerodon.io  是可以访问的
后边 加 /v2/ 呢这是加上v2的信息
runner是否部署在单独的环境中呢？不是，runner 运行在docker里面请粘贴一下你的 .gitlab-ci.yml 文件内容   谢谢这是我的.gitlab.ci文件
image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/cibase:0.5.0改成这样试一试好的，我试试还是报同样的错
请在服务器上执行以下命令尝试登陆,看看提示信息docker login https://registry.example.choerodon.io是同样的结果，下面是输出
[root@node1 ~]# docker login https://registry.example.choerodon.io
Username: admin
Password:
Error response from daemon: Get http://registry.example.choerodon.io/v2/: error parsing HTTP 404 response body: invalid character ‘d’ looking for beginning of value: “default backend - 404”
[root@node1 ~]#Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：查看容器日志的时候，loding几秒后出现No logs，但是又过了几秒就正常出现了日志。有时候直接访问，会立刻出现日志。请问这里的加载日志是什么问题这是使用长连接获取数据有一点延时，我们已经在想优化方案了~感谢提醒一开始的loading 是等待WebSocket状态， 在连上后WebSocket发送日志，若内容为空，变成No Logs。有日志过来，No Logs 消失，日志实时刷新。问题里提的一些直接访问 是loading后直接获取链接发过来的日志了 就没有No logs显示了Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：所有项目的测试管理报表这里，显示的都是projectzzy，这个是什么意思这是个前端BUG，晚上会发布新版本进行修复。谢谢反馈。Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：打开项目下的测试管理，个别项目 会出现空白页面，以下是前端和后端服务报错截图:数据库编码是UTF-8的吗是的噢，很认真的utf8,只有个别项目会出现这种情况您好，请问您是升级到0.8.0的版本的吗？
如果是已经有旧的数据请问您是否按照文档:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-test-manager/
中的
我这是第一次安装，不过可以试试这个方法那报错的这几个项目是否已经新建过版本了呢？是的，原先有创建版本，执行该方法后已经好了好的，谢谢反馈，我们会加上校验避免未执行数据修复会导致报错。Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：自主搭建问题描述：
使用admin登陆平台，发现在项目层级只有4个目录（敏捷管理，开发流水线，部署流水线，项目设置），其中开发流水线中只有持续集成一个菜单。报错信息(请尽量使用代码块的形式展现)：
这个问题已经在0.8.1修复了，请尽快将choerodon-front 更新至0.8.1升级之后可以了 谢谢Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：创建wiki空间失败，请问是什么原因，wiki-service日志如下:wiki也可以正常登录访问了而且发现只有其中两个项目有默认创建的空间，其他项目都没有，而且创建失败
请问你点击这个空间地址链接能够跳转正确地址吗？可以跳转过来，现在这个项目对应的空间是创建成功的，你再创建一次空间可以成功吗？就是在这个页面可以的，
然后只有两个项目有默认空间是因为我们需要对原始数据进行同步，已经同步成功了，感谢。这部分操作貌似没有在文档中看到呀，还是说正常情况下不会出现刚才创建失败是因为那个项目对应的空间还没有创建成功，所以报错”项目同步错误“。在接下来的版本中，我们将会优化这个错误提示。只有两个项目有对应的空间是因为kafka遗留下来的消息被消费的结果，旧的kafka消息现在应该已经被清理了，所以首次部署需要同步旧的组织和项目对应的空间。同步完之后应该在wiki这边的空间弹出框可以看到对应的组织项目结构client这边已经设置重定向地址为https,并却wiki的ingress也配置了，但是oauth跳转后还是到http，是不是wiki那边还要改什么配置
是的，这个问题我们也注意到了，现在wiki使用oauth登录的重定向地址存在bug。我们再下个版本中将会修复这个问题，现在的解决方案是把client的重定向地址设置为http。Choerodon平台版本：0.8.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
在使用 gitlab 合并代码时，有报错。目前是有一个前端项目 在merge时会报500，查看gitlab的日志，有sql报错。报错信息(请尽量使用代码块的形式展现)：请参考这里进行数据库优化http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/gitlab/#gitlab数据库优化问题描述：公司已存在些项目其实并不是一个代码库对应一个服务，而是多个。举个例子，项目结构截图如下:
现打算将这个项目迁至猪齿鱼来管理，但是在猪齿鱼中一个实例应该是只对应一个服务。有想过将服务都抽取出来分别作为单独一个git仓库管理，而共用的作为依赖库。但是这样无疑增加了本地开发调试的难度，可能需要新增一个实体类，得先发布依赖 ，api和web才能进行开发，每次小改动的话就比较麻烦。项目比较老了，不会对项目结构做太大调整，目前只维护某些逻辑。那么如果我在helm chart的部署文件里定义多个deployment的话，到猪齿鱼上一个实例跟多个deployment该怎么对应起来，以及创建网络选择实例等。亦或者是deployment、service、ingress全写到helm chart里面的话，是否还能通过猪齿鱼来维护这些对象呢。对于类似这种项目结构的，helm chart 有没有更好的推荐的写法。多个服务不建议放在一个库中，如果放在一个库中，目前Choerodon对多个不同部署是支持不是很友好，你可以把多个部署写在一个chart中的并内置service，域名则可以使用choerodon管理。Choerodon平台版本: 0.6.0遇到问题的执行步骤:1.注销登录，跳转页面有问题
2.域名页面，应用市场页面找不到，用管理员账号和其他账号都一样3.gitlab settings里面找不到 outbound request ，用的root账号
gitlab是根据0.6版本的脚本安装的。文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问您好 gitlab如果没有该选项 直接忽略即可使用现在文档中已经注明 感谢您的使用您好，由于devops0.8版本对菜单做了较大改动（升级更改说明中有标注），所以升级后需要对历史数据进行处理。您可以：感谢您的使用！Choerodon平台版本: 0.6.0遇到问题的执行步骤:看起来是执行任务超时。执行了多次，都是这种情况。helm命令里配置超时时间，也是这样文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问找到了
你好，你这种情况应该是改了组织的名字。目前的话解决方案是先在界面上把组织名改回运营组织，然后升级，成功后可以再把组织名改回来。嗯，我差不多这么做了，也通过了。不过现在helm拉取包好慢，导致更新的时候总是超时。这种能解决问题，但是不是很友好，我们在0.9.0会做一些改动，解决这个问题。helm拉包慢
@vinkdonghelm包很小的，一般都是毫秒级，请问你的超时信息具体是什么。我觉得是拉取实际的镜像的时候很慢。之前感觉比现在要好一些。是不是你们用户量提高，导致拉取速度变慢了:disappointed_relieved:Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
在使用feign进行调用的时候，在调用方看用户信息是正确的，但被调用方获取到的用户信息就不对。。。
接口在swagger调用的时候是正常的，哪位大佬能说下会是什么原因导致的，是不是哪里配置的问题。。查看下pom 里面有没有引入choerodon-starter-feign-replay的依赖有的我发现走到choerodon的RequestInterceptor实现类的时候，获取这个没有获取到，这个是在哪有什么配置吗？choerodon-starter-feign-replay 是在调用方这边引入的。如果引用了，debug 一下 DetailsHelper.getUserDetails，看看和这里有没有区别。那个就是调用方那块的:joy:
调用前传递的
我们feign是无法传递用户信息的，传的是一个匿名用户信息。因为我们的API分为两种，一种是外部接口，经过网关访问可以获取正确用户信息，另一种是服务间内部调用接口，无法获取用户信息，只验证请求是否合法。我们暂时是这样的，如果现在必须要传递用户信息，可以修改被调用接口，把用户id传给它我看iam服务feign调用file-service这里好像是把用户传递过去了，这里是有什么配置么？我们框架并没有提供任何feign传递用户信息的方法。可以试下参照FeignRequestInterceptor，将调用端DetailsHelper.getUserDetails()获取到的userDetail加密生成JWT，feign请求时加上@RequestHeader(“Jwt_Token”)传递过去测试发现，服务调用的时候，调用方开启熔断器就会导致获取不到用户信息，有什么比较好的解决方法吗？Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：持续集成通过报错信息(请尽量使用代码块的形式展现)：
下面是gitlab.ci文件请检查下您的dockerhub用户名密码是否正确？这个看起来像是你配置了错误的docker registry地址或用户名密码。你是指在安装gitlab runner 时配置的docker registry 地址和用户名密码吗？
下面是我安装的时候的参数
helm install c7n/gitlab-runner 
–set rbac.create=true 
–set env.concurrent=3 
–set env.url=http://gitlab.example.choerodon.io 
–set env.token=bac305267b85596dec24b1ee75512d 
–set env.environment.DOCKER_REGISTRY=registry.example.choerodon.io 
–set env.environment.DOCKER_USER=admin 
–set env.environment.DOCKER_PWD=Password123 
–set env.environment.CHOERODON_URL=http://api.example.choerodon.io 
–set env.persistence.runner-maven-pvc="/root/.m2" 
–set env.persistence.runner-cache-pvc="/cache" 
–name=runner --namespace=choerodon-devops-dev这是我安装gitlab runner 输出的信息这是ci中变量名错了，你这个应用是什么时候创建的，你重新基于预定义模板创建一个应用，或者将ci中docker login -u ${DOCKER_USER} -p ${DOKCER_PWD} ${DOCKER_REGISTRY}的DOKCER_PWD纠正为DOCKER_PWD我这个应用是根据 文档中创建一个后端应用示例创建的， `${DOKCER_PWD} 这个变量我也没有改，是直接生成的文件就是这样，那是不是你们这个gitlab.ci模板文件里面有问题？另外我修改一下我的文件！谢谢现在创建应用选择后端模板生成出来的ci应该是正确的，你确认一下。Choerodon平台版本: 0.8.0遇到问题的执行步骤:
升级了0.8.0后，发现之前的环境流水线里的环境不可使用，发现有说明 版本过低，请更新。使用指令进行升级，过程中没有报错。升级后 环境仍然处于未激活状态。查看日志 该环境的pod ，发现有报错信息websocket: bad handshake你确认一下 devops服务的values中这个AGENT_SERVICEURL这个配置的是正确的，没有变动过这是pod日志
你把agent日志贴一段出来，如果devops-service有异常日志也贴一下出来devops-service这个服务 我也已经升级到 0.8.0了你手动改过agent的namespace？应该没有这么操作过你确认一下之前的环境agent是否真的停掉了，被替换了。如果的确一个agent只存在一个实例pod了那重启一下devops服务。我昨天也升级了0.8.0版本，然后在环境那会提示版本落后，然后需要执行命令升级一下就好了我们自己的正式环境升级也是OK的。我的环境 是和 我的猪齿鱼平台 放在一个 K8S集群里的 是否有影响我重启了 devops-service 还是不行
我这个 namespace下 只有这一个 agent我新建环境 OK的 就是 升级环境 会有问题。我尝试过 通过helm命令删除 该agent，之后在手动安装 也不行。
我觉得应该是 devops这个程序里有问题吧那现在这样的话，我暂时不知道是由什么原因造成的，有一个解决办法，
进入devops-service所连的redis所在的pod，执行redis-cli，再执行DEL "\xac\xed\x00\x05t\x00\x0eagent-sessions",再重启devops 服务查看了devops的源码 发现这个报错信息是因为 你们将 EnvSession 存到了 redis里面。可能是我老版本的环境信息还遗留在 redis中，我尝试将 devops-redis 重启，之后发现环境就OK了 。你们可能需要在升级的时候把相应的redis的值删除就行正常情况下，是会被清掉的，我们这边升级都是很正常的。Choerodon平台版本: 0.8.0遇到问题的执行步骤:
部署知识管理后端文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-wiki/原因分析:
上面几个 preJob.preInitDB 对应manager_service的 应该是 preJob.preConfig吧是的，我们正在检查和更新文档。env.open.WIKI_URL这一项 需要 加上 http://需要的，这里是填写完整的wiki域名Choerodon平台版本：0.8.0运行环境：自主搭建问题描述：猪齿鱼平台关于权限这块提示不是十分的友好，经常会出现请求403的问题，但是界面上不会有所提示，能看到的是一直在加载中。建议社区可否考虑参照各大云厂商平台的权限提示方式，如果请求资源无权访问，那么可以提示出该资源需要授权什么权限，做个友好的提示，体验会更加友好，也以便授权。好的，建议我们记下了~~Choerodon平台版本: 0.8.0遇到问题的执行步骤:
部署完wiki后 ，添加了wiki的客户端 ，之后页面后报500.文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-wiki/报错日志:原因分析:
可能是我参数配置错了，–set env.OIDC_CLIENTID=wiki 
–set env.OIDC_SECRET=secret 
–set env.OIDC_WIKI_TOKEN=Choerodon \这几项 我直接使用的默认值，是否要更改看上去像是 scope配置有问题，检查下 client的 scope你好！
这个是因为oauth的客户端配置的scope不正确，现在wiki默认的scope是”openid“，还有重定向地址的path配置成”/oidc/authenticator/callback“。
可以了 谢谢Choerodon猪齿鱼是一个开源企业服务平台，是基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理的开源平台，同时提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，致力帮助企业聚焦于业务，加速数字化转型。2018年7月20日，Choerodon猪齿鱼发布0.8.0版本，为了使您的应用交付更加敏捷化，运营管理更加自动化，本次更新加入了 知识管理 、 测试管理 等新服务，并且大量的功能优化也在新版本中得以实现，特别感谢社区中的朋友给Choerodon猪齿鱼提出的诸多中肯意见，让我们一起做的更好！发布版本：0.8.0发布时间：2018年7月20日功能范围：知识管理、测试管理、敏捷管理、持续交付、运营管理，以及微服务开发框架等下面就为大家带来详细的版本更新介绍！新发布的服务1.知识管理知识管理服务是一个轻量级的强大Wiki平台，允许用户根据自己的特定需求自定义Wiki，为企业、IT团队提供方便的项目协作平台和强大的项目内容管理平台，集中式管理产品相关内容、管理相关内容等，例如需求收集、架构设计、功能设计、开发规范、命名规范、会议记录、计划安排等。主要特点：知识沉淀——沉淀软件开发过程中的需求、设计、规范等知识文档。项目协同——有效管理项目中的计划安排，会议记录等，加强项目成员之间的合作。产品文档——便捷地编写软件产品的概念说明、用户手册、快速入门等产品文档。培训教材——方便地编写软件功能使用等培训材料，甚至视频教程等。2.测试管理测试管理主要为用户提供敏捷化的持续测试工具，功能包括测试用例管理、测试循环、测试分析等，可以有效地提高软件测试的效率和质量，提高测试的灵活性和可视化水平，最终减少测试时间，让用户将主要精力放到软件功能构建上。主要特点：敏捷化 ——测试管理与敏捷管理集成，为用户提供无缝的敏捷体验。自动化——与主流的自动化测试框架集成，显著提高测试的自动化覆盖率。DevOps——提高DevOps全流程端到端的测试可视化程度，提高软件交付的质量和资源利用率。测试分析——最大限度地利用自动化，优化测试用例实现，以及缺陷趋势预测，提高软件交付质量。新增功能1.敏捷管理敏捷管理服务新推出了新功能方便对版本和问题的管理，主要新增功能如下：版本报告功能：通过版本报告来详细展示团队在完成版本方面的进展，同时报告会根据剩余预估时间、故事点、问题计数进行筛选，还会根据您的团队自版本开始以来的平均进度（速度）以及估计的剩余工作量向您显示预测的发布日期。
累积流程图功能：累积流程图是一个区域图，显示应用程序、版本、sprint的各种工作项状态。水平x轴表示时间，垂直y轴表示问题计数，图表的每个彩色区域等同于面板上列的问题变化，累积流程图可用于识别瓶颈，如果您的图表包含随时间垂直加宽的区域，则等于加宽区域的列通常会成为瓶颈。除此之外，敏捷管理服务还增加了问题导出Excel功能，问题转换为子任务，问题复制，以及版本界面新增查看发布日志等功能。2.持续交付增强分支管理功能，支持更多的分支管理模型，0.8版本的分支管理功能比原来更加灵活，例如，支持gitlab-flow和github-flow模型，实现分支与敏捷管理的问题关联，实现敏捷问题管理及持续交付代码管理一致性，以及分支管理集成push、merge request webhook。
在实例部署阶段日志中增加阶段执行相关事件日志。在输出阶段Job pod中日志之前，增加了Job启动详细过程的日志记录，例如该阶段Job开始，分配节点，拉取镜像，执行。以便于在部署实例时，排查各个阶段的执行日志，方便部署人员快速的定位问题。
应用管理增加sonarqube代码质量检查链接跳转，方便用户查看代码质量检查的结果。另外，持续交付服务还增加了版本升级的时候通过请求API实现版本间的平滑升级，用导出时默认获取所有应用的最新版本，以及置文件信息支持保存新增的参数等功能。3.微服务开发框架微服务开发框架增加了如下的功能：新增微服务功能，可以查看平台中的所有微服务。
新增API测试，可以查看微服务下的controller以及controlller下面的API接口。新增个人中心的组织和项目信息，可以查看在不同组织或者项目中被分配的角色以及这些角色的权限。客户端新增了作用域和自动授权域字段。功能优化1.敏捷管理在敏捷管理中，0.8版本还修改优化了如下部分功能：更新问题的版本关联，不能删除已经归档的版本关联。优化搜索接口，修改触发逻辑。报告界面可以关联查看问题列表和每个问题详情。发布版本问题可以通过点击链接到问题管理中。还有其他诸多细节的优化。2.持续交付在持续交付中，0.8版本还修改优化了如下部分功能：修改CI生成版本号的命名规则。配置文件信息存储方式修改为只保存修改内容。优化部分页面字段长度及显示方式。修改Agent默认返回消息行数。完善网络唯一性校验及域名地址校验规则。还有其他诸多细节的优化。3.微服务开发框架在微服务开发框架中，0.8版本增强了部分功能：创建组织优化为组织列表跳转到第一页。删除自设目录时提示优化。创建用户、修改用户页字段优化与密码取值修改。LDAP组件合并优化。缺陷修复1.敏捷管理0.8版本修复了如下的缺陷：简易创建问题卡顿。问题详情锚点定位不准确。问题标题为编辑状态时切换时，编辑框内容会被清除。富文本编辑器在多英文的情况下断词失败。还有其他已知bug。2.持续交付0.8版本修复了以下缺陷：修复Select框的全选取数据问题。Table组件的筛选条件，从父组件刷新无法清空。修复网络管理修改网络切换版本未清空实例值的问题。修复实例详情日志阶段切换内容未改变的问题。修改Agent多余时间戳的问题。还有其他已知bug。3.微服务开发框架0.8版本修复了以下缺陷：修复添加权限时，如果进行了权限过滤，再次进入没有清空搜索结果的问题。修复项目无法停用成功的问题。修复后端配置https不跳转的问题。修复用户全局过滤时后端没有返回数据的问题。修复密码策略无法保存的问题。修复实例管理在选择微服务之后，不能查询对应的实例的问题。修复个人中心页修改头像之后，再次保存用户时失败的问题。修复无法更新用户的问题。修复移动端无法登录跳错误页的问题。修复实例详情元数据标无过滤表文字的问题。修复liquibase工具包如果excel的某一行有空值的问题。还有其他已知bug。更加详细的内容，请参阅Release Notes和官网。欢迎通过我们的GitHub和猪齿鱼社区进行反馈与贡献，感谢各位朋友陪伴Choerodon猪齿鱼不断成长，我们会持续迭代优化，敬请期待。Choerodon猪齿鱼社区Choerodon平台版本: 0.7.0遇到问题的执行步骤:在 升级devops service时，根据提示 需要开启Outbound requests 选项，但在admin area里面的setting中没找到如图所示的 开启 Outbound requests 选项的按钮请问你的gitlab版本是多少0.1.0如果没有这个选项，忽略即可，这是新版本的gitlab添加的功能。Choerodon平台版本：0.8.0运行环境：https://choerodon.com.cn/问题
敏捷管理的经办人列表使用不方便，例如我以为就这么几个人，找不到我需要的人。没有明显的提示告诉我，其实这仅仅是显示了一部分。截图：目前是同时显示20个，然后可以通过筛选器进行筛选
我们建议如果成员过多尽量使用筛选搜索进行范围缩小
下次迭代我们会对该功能进行优化Choerodon平台版本: 0.7.0文档地址:
http://choerodon.io/zh/docs/installation-configuration/update/0.7-to-0.8/报错日志:
Error: failed to download “c7n/manager-service”.
应该是https://openchart.choerodon.com.cn/choerodon/c7n/这个chart仓库里面没有 manager-service 0.8.0版本。我查看了api ，发现又0.8.1 就升级了0.8.1版本谢谢你的反馈，我们会马上进行修正的，在此给你带来不便敬请谅解。agile-service 也存在同样问题好的，谢谢你的反馈，我们会马上进行修正的，在此给你带来不便敬请谅解。请问猪齿鱼0.7版本如何升级到0.8版本？请参考文档：
http://choerodon.io/zh/docs/installation-configuration/update/0.7-to-0.8/Tks，Vink总是那么的贴心和及时。Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：centos7 k8s遇到问题时的前置条件：问题描述：
can’t stat ‘target/app.jar’: No such file or directory在结束feature分支触发ci流水线时报错
下面是我的.gitlab-ci.yml文件
image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/cibase:0.5.0找到原因了，是因为maven pom 文件的打包名称和.gitlab-ci.yml配置文件的值不一致导致的，现在也修改成了 app就好了
好的我想在HAPCloud中同时使用AntD跟choerodon-ui这样会有问题吗？同一个js中一部分使用And的组件一部分使用choerodon-ui封装好的组件，这样会有问题吗？如果这样有问题，分开在不同的js中分开引用这样会有问题吗？比如说一个js中引用Antd的组件，另一个js中引用choerodon-ui组件这样有问题吗？还是项目中引用的只能是其中的一个，另外一个不能使用当前版本不建议一起使用。 choerodon-ui样式的prefix依然是antd，会和antd的样式冲突Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：k8s问题描述：
可以看下gateway-helper的日志，403的请求会打印出来。第一：路由hpay-back-10097这个路由配了吗？第二：可以看下permission表里这条权限有没有刷进去？Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：多看板问题，设置多看板后每次进入查看看板后总得切换看板您好，此次发布的版本将包含这个功能，用户切换多看板将记录切换的看板，下次进入为之前切换的看板。刚接触Choerodon很多地方都不太懂,抱歉，我又来问问题了！我按照文档中的创一个nginx示例操作时，在提交gitlab后CI一直报错，请问我是什么地方操作有问题Choerodon平台版本: 0.7.0遇到问题的执行步骤: 创建一个nginx-demo文档地址:http://choerodon.io/zh/docs/quick-start/nginx-demo/环境信息(如:节点信息):报错日志:
下面是我的.gitlab-ci.yml`image: registry.choerodon.com.cn/tools/cibase:0.5.0stages:chart_build:
stage: chart_build
script:
- docker login ${DOCKER_REGISTRY} -u admin -p Harbor12345
- docker build --pull -t ${DOCKER_REGISTRY}/${GROUP_NAME}/${PROJECT_NAME}:${CI_COMMIT_TAG} .
- docker push ${DOCKER_REGISTRY}/${GROUP_NAME}/${PROJECT_NAME}:${CI_COMMIT_TAG}
- chart_build.auto_devops: &auto_devops |
curl -o .auto_devops.sh 
“${CHOERODON_URL}/devops/ci?token=${Token}&type=front”
source .auto_devops.shbefore_script:这是调用devops服务的API失败了 检查下这个服务是否正常不好意思，另外有个疑问，我的Harbor设置的登录密码 是

但是我看这个.gitlab-ci.yml写的是

这两者是指同一个密码吗？是同一个密码好的，谢谢你好，这是我打开Swagger页面查看，这个devops的服务应该是正常的吧！但是runner还是测试走不通！
token在项目设置中能够找到，你可以参考ci中的命令用 curl 看下返回结果。你好，我打开我的设置 没有发现有token
您是用gitlab直接建的项目吗？不是，就是按照文档中创建一个nginx示例执行的，
但是前面的 kubernetes 部署这一步跳过了没有执行按照这个文档里面的执行，好像没有创建应用，只是有一个项目。是不是因为这个原因哦哦，也就是说文档里面的操作步骤不对是吧？是的，我们会进行改正。现在需要上传文件到服务器，并且保存到宿主机中
看了下chats文件，感觉和之前的k8s部署的yml文件有很大差别，
想知道valumes要怎么配置。你好，可以通过文件服务来上传文件到 minio 上file-service - The file service is built on minio server, we can use minio client to upload and delete files.可以不用文件服务吗。。想单独写一个上传下载的功能。。客户没装minio可以不用文件服务吗。。想单独写一个上传下载的功能。。客户没装minio包括日志持久化也得做文件挂载可以自己实现啊，就是上传生成下载url，可以下载的功能，但是我们框架的头像等上传功能基于file-service，要想这些功能不受影响，可以自己重新file-service其实我就想知道chats怎么设置文件挂载。。关于文件挂载，建议你先看下 k8s文件挂载相关内容VolumesChart进行了简单封装，可以在 chart/template/deployment.yaml中修改为你想要的内容。这个是job失败导致部署不成功。你先点击【查看实例详情】，看部署详情的各个job有没有报错日志；如果没有，再进k8s上看有没有其他报错信息。我们在下个版本中会支持更多job执行信息展示。实例详情里没有报错信息，k8s里报错信息是第一个图，而且失败后job就消失了，k8s容器也没有，能帮我看一下吗？ 插入mgmt_service_config表时，config_version为空请问你部署的服务是自己的服务，还是选择模板生成的服务？生成的模板，微服务请截下生成的代码库中，chart目录下的templates目录下的 pre-config-config.yaml 文件的脚本部分代码
例如：
你好，猪齿鱼平台是哪个版本？0.7.0最新的，以前是没有这个问题的manager-service是0.7.0版本吗？pre-config里的registry.cn-hangzhou.aliyuncs.com/choerodon-tools/dbtool是0.5.2吗？manager-service 0.6.0请给一下git仓库地址可以吗https://code.choerodon.com.cn/hzero-hpay/hpay-back要么manager-service升级到0.7.0，要么dbtool版本降到0.5.0，版本统一就可以了健康检查没过，看下pod的日志一般健康检查，会检查些什么方面？
部署values里面 ，端口号填的多少
bootstrap.yml里面加上security.enabled=false，关掉actuor的security认证Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
开发了一个微服务，使用logback收集日志文件，如何把日志文件从容器里挂载出来可以在配置里指定logging.path。然后将该路径通过chart value 挂载到出来遇到一个问题，
为何要将日志挂载出来呢，你需要对日志进行额外的操作吗。主要是为了方便查看日志，如果在容器里，容器重新启动，日志就没了，mysql，文件都是这样做的认证流程 – 跳转到oauth-server进行登录 – 登录成功获取access-token – 请求到api-gateway – 转发到gateway-helper进行鉴权，生成JWT – 路由到真实服务。真实服务的请求校验是根据头部的JWT_Token，如果是feign调用其他服务，则在调用端添加choerodon-starter-feign-replay依赖即可Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：部署流水线 >> 容器 >> 查看日志，当容器日志输出过多，浏览器崩溃😖浏览器：Chrome  67.0.3396.99，Safari 11.1.2报错信息(请尽量使用代码块的形式展现)：
Chrome直接崩溃
Safari 还可以支持一会，然后。。。
感谢提出，我们已紧急优化，会在这周发布。重新安装choerodon，执行一键部署时，一直报错choerodon-manager-service" not found
重复安装了好几次，还是卡这在这里！
请问，这里应该怎么操作，下面是获取所有pod状态
kubectl get pod  --all-namespaces
请重新执行安装命令，按提示执行删除操作后再进行一键安装，下面命令你需要执行3遍第一遍会提示你删除helm release
第二遍会提示你删除job
第三遍才会进行正式安装curl -o choerodon-install.sh \ https://file.choerodon.com.cn/choerodon-install/install-0.7.sh && \ sh choerodon-install.sh values.sh你好，刚才我就是按照你这个步骤执行的，不过curl -o choerodon-install.sh 
https://file.choerodon.com.cn/choerodon-install/install-0.7.sh 这一步我没有执行，因为第一次执行之后choerodon-install.sh文件以及存在我的服务器上了，所以我就直接执行了sh choerodon-install.sh values.sh，但重复好几遍之后的确还是报错。我现在再执行一遍吧你好，在我重复执行一键安装多次之后还是报这个错误
下面是执行 kubectl describe   job/choerodon-manager-service-init-db  --namespace=choerodon-devops-dev 输出
curl -o choerodon-install.sh \ https://file.choerodon.com.cn/choerodon-install/install-0.7.sh 这一句必须执行的，因为某些服务的小版本可能不一样哦，另外报这个错应该是数据没删干净。你好，按照你说的操作，我删除了猪齿鱼的所有组件，k8s的job，和values.sh配置的文件路径里面的数据之后，重新执行一键部署，在第九步 create database for Choerodon的时候就会 输出Job failed: DeadlineExceeded 这个信息，无法继续执行下去

下面是我执行 kubectl describe   job/create-choerodon-databases  --namespace=choerodon-devops-dev 的输出，似乎是job什么地方初始化的时候超过了时间限制，导致失败。
另外， 如果我开始的时候没有删除values.sh配置的文件路径里面的数据，只是删除猪齿鱼的所有组件和k8s的job就重新执行一键部署的话这一步就不会报错，但是就会到第19步的时候报错choerodon-manager-service" not found请在部署出错以后查看一下mysql数据库是否正常我后面把nfs重新安装一遍，再执行一键部署的时候安装就可以了。谢谢！另外我想请问一下
文档里面提到的使用项目创建者的角色登录系统，我想请问一下我怎么创建这个拥有项目创建者角色的
用户，因为我在菜单里面找不到这个功能，目前我只知道使用admin:admin登录，其他用户是从哪里来的？使用组织管理员角色即可创建项目，文档部分写错了，不好意思我们记录一下马上修改。不好意思，我还是不懂怎么创建组织管理员角色这个用户，因为文档里面没有提到这个角色用户从哪里来的， 能不能给一个详细的执行步骤，因为我刚安装好choerodon，很多都不熟悉！谢谢搭建好系统后会有一个默认的组织，选择该组织后可以进行创建项目；
若您需要给其他用户授权该角色，请先在组织设置-用户管理创建用户后，在组织角色分配给该用户分配组织管理员角色。
admin用户默认是默认组织的组织管理员。组织管理员这是角色，我们系统已内置，可以在平台管理-角色管理里查看我们所有预置的角色：平台管理员、组织管理员、项目所有者、项目成员、部署管理员。有一点建议，就是这个admin:admin 这个用户我也是自己随便猜输入，结果发现能够登录的，文档里面也没有提到有这个这个用户名密码，所以建议这个应该在文档里面提到默认用户名密码是多少，当部署完毕之后使用这个用户名密码登录！不然开始接触的话，可能会不知道这个感谢提醒我在github上的choerodon上没有找到源代码的内容，能指点一下吗？The open source PaaS for Kubernetes.往下滑，都是项目Choerodon平台版本：0.7.0运行环境：公司搭建问题描述：1.问题管理 怎么按人搜索问题
2.怎么查询历史冲刺1、目前在该界面无法按人搜索问题，谢谢反馈。问题管理主要用于其他界面的issue跳转查看，目前该部分功能直接使用的体验还需要进一步的优化，我们会在后面的版本进行优化改善。
2、历史冲刺可以在报告菜单下的sprint报告中进行查看，可以在报告中选择历史的冲刺Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
使用event服务进行事务一致性控制，可以正常发送信息，但消费者报错不能进行回滚。
请问时间服务有相关的使用demo吗？按照这个https://github.com/choerodon/choerodon-starters/tree/master/choerodon-starter-event-producer  0.5.3没有这个controller的接口
Choerodon平台版本：0.7.0运行环境：https://choerodon.com.cn/问题描述：敏捷管理中重新分配责任人，人员列表刷不出来。主要与之相关的界面都有这样的情况。
注意：是重新分配，或者修改。谢谢反馈，该问题会在下个版本进行修复Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：自主搭建问题描述：
我连续创建了8个应用（微服务的）,之后又两个应用 状态一直处于 创建中。查看 devops-service，发现又报错报错信息(请尽量使用代码块的形式展现)：devops-service的版本是0.7.0吗?是 0.7.0gitlab-service有报错吗？没有找到错误报错的地方是应用更新develop分支为默认分支的时候报错，目前正在排查错误，可能是因为gitlab-service返回值有问题，0.8.0已经去掉了develop分支，不会有更新默认分支的逻辑，升级之后不会有这个问题了。好的  谢谢Choerodon平台版本: 0.7.0
使用的是admin:admin的用户登录遇到问题的执行步骤:安装完成之后报错日志: 疑似部分菜单没有初始化成功，页面上看不到比如说没有创建用户的菜单，如果我想要创建用户如何创建，或者说创建项目呢?
下面的图片是我使用admin:admin的用户登录能够看到的菜单，下面是我安装的脚本的的版本信息部分服务已经升级到 0.7.1 了尝试升级一下。GO_REGISTER_SERVER_VERSION=“0.7.1”
MANAGER_SERVICE_VERSION=“0.7.0”
CONFIG_SERVER_VERSION=“0.7.0”
API_GATEWAY_VERSION=“0.7.0”
IAM_SERVICE_VERSION=“0.7.0”
GATEWAY_HELPER_VERSION=“0.7.0”
OAUTH_SERVER_VERSION=“0.7.1”
EVENT_STORE_SERVICE_VERSION=“0.7.0”
FILE_SERVICE_VERSION=“0.7.0”
DEVOPS_SERVICE_VERSION=“0.7.0”
GITLAB_SERVICE_VERSION=“0.7.0”
AGILE_SERVICE_VERSION=“0.6.1”
CHOERODON_FRONT_VERSION=“0.7.1”
AGENT_VERSION=“0.7.0”请问，目前只能通过重装的这种方式解决吗？升级指定服务即可,可以参考这里升级指定服务
http://choerodon.io/zh/docs/installation-configuration/update/0.6-to-0.7/
因为某些服务的0.7.0有一些bug，建议先升级。好的谢谢，我重新再安装一遍！Choerodon平台版本：0.7.0运行环境：公司环境问题描述：问题看板对于已有状态更新时类别时，提示名称重复。这是我们系统的bug，我们会尽快修复Choerodon只能管理使用Choerodon开发部署的应用服务和资源吗？是的。因为目前Choerodon的部署端需要有镜像，以及charts文件等，这些都是需要在Choerodon的开发流水线上生成的。如果自己去实现的话，理论上也是可以的，但是比较的麻烦。可能是我没说明白，Choerodon不是在spring cloud上封装了一套微服务框架嘛，假如我不用这一套，我自己直接用spring cloud原生那一套开发的应用是不是就不能用Choerodon的开发流水线了。换句话说能不能只用Choerodon的敏捷管理和开发流水线，而不用Choerodon的微服务开发框架可以的Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：自主搭建问题描述：
自主搭建的猪齿鱼平台中 ，应用市场没有 应用导入 导出功能。使用的是 admin账号登陆的原因分析：
怀疑可能是没有权限。查看下用户是否有该项目的部署管理员权限。Choerodon平台版本：0.6.0运行环境：官方提供问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。提交代码之后，会自动进行持续集成，但是提示“ This job is stuck, because you don’t have any active runners that can run this job.
Go to Runners page ”，runner需要自己创建吗？还是在猪齿云上提供了配置的方法？执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作请参考文档  http://choerodon.io/zh/docs/installation-configuration/steps/gitlab-runner/



持续集成问题 User Guide


    请参考文档  http://choerodon.io/zh/docs/installation-configuration/steps/gitlab-runner/



目前，可以使用共享的runner进行持续集成，但是一般出现这种情况是您的服务器资源不足。检查下您服务器的资源情况。@xinghao  在0.6版本中，docker-build脚本在gitlab-ci.yml的最下方, chart_build在devops-service中。


github.com


choerodon/devops-service/blob/0.6.5/src/main/resources/shell/front.sh
export GROUP_NAME={{ GROUP_NAME }}
export PROJECT_NAME={{ PROJECT_NAME }}
export CI_COMMIT_TAG=$(GetVersion)
function node_config(){
    npm config set registry ${NODE_REGISTRY:-"http://nexus3.deploy.saas.hand-china.com/repository/handnpm/"}
}
function node_module(){
    mkdir -p /cache/${CI_PROJECT_NAME}-${CI_PROJECT_ID}-${CI_COMMIT_REF_NAME}-${CI_COMMIT_SHA}
    python ./boot/structure/configAuto.py ${1}
    cp -r config.yml /cache/${CI_PROJECT_NAME}-${CI_PROJECT_ID}-${CI_COMMIT_REF_NAME}-${CI_COMMIT_SHA}/
    cd boot && npm install && cd ../${1} && npm install && cd ..
}
# 开发使用devbuild，构建镜像使用build
function node_build(){
    cd boot
    ./node_modules/.bin/gulp start
    cnpm run ${1:-"build"}
    find dist -name '*.js' | xargs sed -i "s/localhost:version/$CI_COMMIT_TAG/g"
}
function cache_dist(){


  This file has been truncated. show original





请问.auto_devops.sh文件在什么地方这是请求devops服务获取到的shell脚本。经过devops服务加工后返回给runner的。参考这里的几个文件devops-service - DevOps Service is the core service of Choerodon. It integrated several open source tools to automate the DevOps process of planning, coding, building, testing, and deployment, oper...创建项目之后会自动在harbor仓库中创建一个项目，我们可以直接在.gitlab-ci.yml文件中使用docker_build脚本将镜像上传到这个项目中，而不需要用户名和密码。
这是为什么，会不会出现镜像到其他项目的情况？
如果要删除harbor中自己项目的镜像需要怎么做(没有这个harbor项目的用户名和密码)？
这个docker_build脚本时在.auto_devops中定义的，这个文件是否可以修改？
当部署github-runner时已经集成了harbor的授权，所以无需docker login, 这样存在一定安全隐患我们在考虑实现另一种更加安全方式, 正常情况下不会推送到其他的项目下，如果需要修改docker_build方法，在auto_devops下方编写即可。下边是示例代码：里面的镜像是否可以删除？如果您使用官方环境则无法删除，如果使用您自建的harbor，可以在harbor管理界面中删除。恩，好的，明白。感谢@vinkdong
红色一般情况下为健康检查未通过。apiVersion: apps/v1beta2
kind: Deployment
metadata:
name: {{ template “hapimpdata.fullname” . }}
labels:
app: {{ template “hapimpdata.name” . }}
chart: {{ template “hapimpdata.chart” . }}
release: {{ .Release.Name }}
heritage: {{ .Release.Service }}
spec:
replicas: {{ .Values.replicaCount }}
selector:
matchLabels:
app: {{ template “hapimpdata.name” . }}
release: {{ .Release.Name }}
template:
metadata:
labels:
app: {{ template “hapimpdata.name” . }}
release: {{ .Release.Name }}
spec:
containers:
- name: {{ .Chart.Name }}
image: “{{ .Values.image.repository }}:{{ .Values.image.tag }}”
imagePullPolicy: {{ .Values.image.pullPolicy }}
ports:
- name: http
containerPort: 8080
protocol: TCP我的deploy文件是这样的，并没有做健康检查啊，而且pod正常启动，服务也可以访问了@vinkdong 使用官方的choerodon平台的时候，我可不可以把镜像传到自己的harbor仓库呢？可以的 修改项目或组下的的环境变量DOCKER_REGISTRY  docker push 前需添加 docker login -u xxx -p xxx 命令。Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：版本的归档是什么意思，文档（http://choerodon.io/zh/docs/user-guide/agile/release/）中没写。另外，不太明白 创建问题时，那个  影响的版本    和    修复的版本    这两个的具体使用场景，以及该用哪个？你好，归档版本是指发布时间较久并且不再更改的版本，作为版本状态周期的标记使用，主要是业务标记。
影响版本一般适用于bug类型的issue，指发现该bug的版本。
修复的版本指该issue生效的版本，因为issue本身有问题的含义，所以这里用修复的版本来说明问题的生效。明白，建议补充到文档中去。好的，感谢反馈Choerodon平台版本：0.6.0运行环境：正式环境问题描述：执行的操作：建议：已修复，将在下个迭代上线，这是我们的失误，不好意思，谢谢～Choerodon平台版本: 0.7.0遇到问题：考虑到部署区机房无外网的情况，能否将agent chart 配置推送到我们自己的chart仓库，这个可以提供下吗？或者使用deployment.yml文件直接部署可以吗,但是我对--host=tiller-deploy.kube-system:44134这个参数不是很理解。你好 可以提供的哈 请下载一下agent chart 的tgz包放在你仓库中就可以了哈使用deployment.yml文件直接部署可以吗,但是我对 --host=tiller-deploy.kube-system:44134 这个参数不是很理解关于这个问题，我们是不建议使用deployment直接部署的哈，因为这个涉及到serviceaccount权限问题，不仅仅只是一个deploment对象而已哈。再者部署时对应的环境变量是不一样的，你使用deploment时其实并不能提前得知。--host指的是helm服务端tiller的地址，这里我们配置默认kube-system下的tiller。界面上创建环境后，提供了环境变量的，可直接拿过来。对于sa的权限，我看命名都与ns以及环境编码一样，既然不建议，那我就当个人尝试下好了是可以的的哈，请注意是三个关于权限的对象（serviceaccount，role，rolebinding）哈。   下面给你个实例：Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：镜像自动构建时，一样也出现了+8时区问题，时间不一致这个时间是commit提交的时间。时区是提交代码的机器的时区。不太明白，为什么一会是正常的时间，一会差了8，需要怎么修改如果差了8小时，先检查下提交代码的这个用户的电脑时区是否正确，如果他的时区正确配置但仍然差8小时再反馈哦。建武的mac电脑是时间没错的，我们先自行排查下吧，不过昨天提到的日志那里的+8时区以及有些pod会自行加上前缀时间的问题，还请继续研究研究。那个加上前缀时间那个，是我们获取日志时候某个参数设置的问题，我们现在把它去掉。OK，那麻烦后续版本修复一下，谢谢。Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：平台上创建的ingress默认都是http，现在配置需要自己手动去k8s里修改ingress，添加证书secret。配置完成之后，但是只要界面上修改了一下域名的某个属性，就会把ingress的配置还原了。还原了，又得去edit ingress。暂时不支持。敬请关注后续版本。背景：部署了https
问题描述：/oauth/oauth/authorize，跳验证，未登录情况下，跳oauth/login, 结果跳转的是http://***/oauth/login,而不是https://***/oauth/login，
不知道是我配置问题还是框架bug，目前解决方案，nginx80转443。你访问地址是http还是https呢肯定是https
这个帖是要沉的吗?响应这么慢的吗?! 自己玩儿一下不就不知道嘛你好，我看了下，是我们oauth-server的bug，你可以先在ingress 的nginx上设置强制跳转，下一个版本我们会修复这个问题我能问下什么时候能改好吗，可能等不及请问 前端 环境变量中的 PRO_HTTP 你配的是 https吗。只有后端，没有前端，前端不用Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost问题描述：使用微服务模板开发项目，发现部署之后占用的内存特别大。在无访问量的时候 ，基本要占用到2.5G。
这样对我们的服务器压力比较多。是否有方案可以减少单个服务占用的内存你好，我们已经计划优化服务性能。你可以先在values 里面定义JAVA_OPTS 来优化服务jvm我在 values里面添加了 JAVA_OPTS: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Xmx1024m
但部署后并没有生效 ，查询这个 pod的 8091/metrics
里面的 “mem”:3117329,“mem.free”:920776。就是内存未被限制可以看下dockerfile 里面有没有定义JAVA_OPTSENTRYPOINT exec java $JAVA_OPTS -jar xxx.jarChoerodon平台版本：0.7.0运行环境：自主搭建问题描述：在helm chart模板中都加上了这个，是何含义。可以不写吗。如果你不需要监控和收集日志可以不写的。metrics 和logs 分别是用来监控和收集日志的，我们会将对应的说明添加到README中这个收集日志是在猪齿鱼平台实例那里查看日志，需要配置该值才能看到日志，还是说是另外搭的一套日志收集系统。是相对独立的另一套日志系统Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost问题描述：部署了一个微服务应用，显示部署成功了 。查看实例运行状态 ，发现在不断重启。
发现回报 OOMKilled 错误今天有把一个node进行过 增加内存的操作，从 16G -》32G,增加后 重启过这个 node建议参考下面这个帖子提高限制内存： limits.memory  , 一般情况下当容器使用的内存超过限制内存之后会发生OOMKilled。
我提高了 limits.memory  3G ，可以正常启动了。今天发现昨天部署的微服务 状态不稳定 。重启次数有点多追踪 pod的状态 会出现如下情况
这是内存不足导致的。集群里面还有 7G的 free内存
查看pod时候加上  -o wide 参数，可以看到pod所在的主机。查看该主机是否有足够的内存。configs-server是否也运行在node2上?是的这个pod运行到node2之后是否还继续重启呢。这个pod 一直是在 node2上面的 。我们的node2昨天加过 内存 。其他node 内存都不足了。尝试删除这个pod,如果仍然重启，建议加大限制内存。我删除了pod 仍然不断重启 config-service这个 也在重启。
尝试调大限制内存我重新部署了一个应用 ，我把 limits.memory 和 request.memory都注释掉 就没有报错了 。
我感觉这个问题 可能和我单独 扩容了一个node的内存有关 ，放到这个node上的pod 如果有内存限制 都不稳定。
就是说 这个node扩容的状态 并没有同步到 其他的matser 节点上Choerodon平台版本：0.7运行环境：自主搭建问题描述：用choerodon部署的应用查看日志时发现有一段时间戳是pod日志中没有的，而且时区也是不对，我pod中时区是对的。请问这是在哪配置的From null只要没有取错pod，那么返回的数据一定是那个pod的信息。
这个pod是有多个container吗？503同学，直接贴下POD的日志出来 @503From null这是pod的日志，pod里只有一个containerChoerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost k8s都一样遇到问题时的前置条件：服务注册到注册中心问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：用新的模板生成的项目注册到注册中心上去修改的数据：
application.yml
bootstrap.yml
java
resources报错信息(请尽量使用代码块的形式展现)：
Network:
api 403

原因分析：403,用了。0.7.0的微服务模板，微服务环境为：0.6.0疑问：怎么解决http://forum.choerodon.io/t/topic/443/23新注册到注册中心的服务（eureka），到swagger报403错误，手动将 iam_permission里的 permission刷进去后，本地能用，部署到服务器的仍然是403，怀疑是api-gateway、gateway-helper的配置文件里zuul没有新注册服务网管。注：原来研发没有升级之前没有此问题服务端也调手动刷新的接口了吗？看看服务端的数据库iam_permission有没有数据看下gateway-helper的日志，如果提示是找不到服务的路由，请在路由管理页面添加你们服务的路由建议您更新服务至最新的0.7，再配合对应0.7版本的应用模板使用；或是现有情况下使用0.6版本的应用模板go-register-server,oauth-server和choerodon-front用0.7.1因为我们这边要混合开发，所以用的是java版的注册中心，这个对升级会有影响吗？访问一下你们接口，403时gateway-helper会打印日志，贴下该403请求的日志我们线上是不使用eureka-server的,我们的一些信息获取如版本,metrics-port等信息是从go-register-server中获取，在eureka-server是拿不到的，首先和版本相关的功能如swagger啥的会有影响，我们线上也并未使用eureka进行任何测试，还可能会有其他异常那我们开发怎么办？你好，能描述一下具体的场景吗？我现在理解是这样的，你们线上用的是eureka 作为注册中心，然后关于权限还有路由都不会自动初始化到数据库对吧？关于混合开发，是指本地连接线上环境开发吗？如果是这样，也可以在线上启动go-register-server，然后本地服务通过vpn，注册到线上地址，这样本地服务可以获取线上服务列表，线上也不会扫描到本地服务。如果是本地开发服务，服务器上通过swagger扫描本地服务的注解时，请先确保服务器的pod能够连接到本地，而且本地防火墙是处于关闭状态的。同时，本地开发一般是不需要权限的，可以将服务的@EnableChoerodonResourceServer 注解注释掉。关于路由，模板里并没有提供注册路由的接口，需要自己在管理 -> 微服务管理 -> 路由管理 里面对路由进行配置，或者按照文档，添加对应的配置
http://choerodon.io/zh/docs/development-guide/backend/intergration/run/#服务注册Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：
在k8s集群中 新建了一个 namespace，在这个namespace下 添加了 微服务开发框架 。
可以访问新namespace下的swagger问题描述：
1、在这个namespace下 部署了一个微服务应用，部署成功后，查看swagger ，发现里面没有新部署的应用信息。
该新应用是使用 微服务模板 修改使用的。修改了CustomExtraDataManager这个类里面的信息。2、发现swagger的Authorize功能不可用 ，点击Authorize按钮进行登陆页面，登陆后就报错了iam 数据库执行下这个sql
update oauth_client set web_server_redirect_uri = ‘’ where name = ‘client’swagger 授权OK了
怎么把我的微服务加到 swagger中你指这里面？
是的请问要怎么处理本地开发的话比较简单，再manager-service数据库里面的route表加上自己的路由就行了，只需要填写name, path和service_id就行了。
如果是部署的服务，自己的微服务要加上如下的类和注解这样部署这个微服务会自动将route信息写入到route表里。
如果上面的类也加了，但是重新部署刷不出来路由，试一试手动调用添加路由的接口
以iam-service为例，post body如下：手动添加成功了。请问什么时候可以自动添加。
你给出的json有错误  service_id 应该是 serviceId正常的情况是每次部署这个服务，都会发送消息刷新路由的，如果没刷出来，目前是手动调用这个接口
一键部署脚本
GO_REGISTER_SERVER_VERSION=“0.7.1”
CHOERODON_FRONT_VERSION=“0.7.1”
OAUTH_SERVER_VERSION=“0.7.1”
换成0.7.1试试Choerodon平台版本: 0.7.0遇到问题的执行步骤:按照一键部署安装的choerodon文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):
报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:请问一下这个应该怎么处理，是我少了是步骤吗？首先 执行 kubectl get po --all-namespaces 查看是否有不是Running状态的POD.如果可以的话，粘贴一下返回的结果。你好， 下面是执行结果，直接粘贴文本格式打乱了，帖的图片。的确有不正常的pod这是 执行kubectl describe  pod  choerodon-harbor-harbor-jobservice --namespace=choerodon-devops-dev的 返回结果这是执行kubectl describe  pod  choerodon-harbor-harbor-adminserver --namespace=choerodon-devops-dev 返回的结果查看pod日志 感觉应该是mysql pod的问题，其他几个CrashLoopBackOff好像是依赖这个mysql pod
导致，启动不起来，但是不知道为什么这个mysql启动不起来
问： 为什么这个MySql启动不起来
答： 因为你的存储IO速度跟不上harbor-mysql的要求，所以启动不起来。请更换成本地存储后重试。页面403不是这个引起的，与这个无关。请问一下这个具体的操作步骤是什么？admin登陆所有界面都是403吗？看看permission表里面有没有数据，没有数据在swagger-ui界面调用如下接口
你好，按照你的设置，菜单栏里面的平台设置是可以打开了，但是微服务管理还是报错403choerodon的版本是0.7微服务管理是manager-service，所以再手动调下那个接口，传参manager-service和对应的版本号请问一下这是因为我少操作了什么步骤吗？如果是因为我少操作了什么的话，请问一下这个这文档是写在什么地方的？以下操作默认在master节点执行，选择node1的本地磁盘作为存储在node1上创建目录编辑harbor-mysql StatefulSet对象编辑PV对象删除harbor mysql的pod让他重新启动看看一键部署的脚本GO_REGISTER_SERVER_VERSION是不是0.7.1GO_REGISTER_SERVER_VERSION是 0.7.0
好的 我试试嗯，我们这边也要改下一件部署脚本，0.7.1修复了这个问题你是说我这边在choerodon-install.sh的GO_REGISTER_SERVER_VERSION手动改成0.7.1，然后重新执行吗？GO_REGISTER_SERVER_VERSION=“0.7.1”
CHOERODON_FRONT_VERSION=“0.7.1”
OAUTH_SERVER_VERSION=“0.7.1”Choerodon平台版本: 0.7.0遇到问题的执行步骤:文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息): master ，node1报错日志:
Mounting command: systemd-run
Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/pods/e9ba463a-83ee-11e8-9225-0297889229db/volumes/kubernetes.io~nfs/choerodon-mysql-pv --scope – mount -t nfs nfs.example.choerodon.io:/data/u01/io-choerodon/mysql /var/lib/kubelet/pods/e9ba463a-83ee-11e8-9225-0297889229db/volumes/kubernetes.io~nfs/choerodon-mysql-pv
Output: Running scope as unit run-48709.scope.
mount.nfs: mounting nfs.example.choerodon.io:/data/u01/io-choerodon/mysql failed, reason given by server: No such file or directory[是不是我没有在master 和worker node上创建对应的目录需要在你的nfs服务的对应目录上创建。 如果是自己搭的nfs在该NFS主机上对应的目录创建。如果不是自己搭建的nfs你需要先将NFS绑定到任一机器后，在绑定的目录中创建。好的谢谢！不过建议这个在文档里面提醒一下用户，要不然大家不知道需要自己创建，据我所知文档里面应该没有提到这点，后面再报错又撤回的重新执行的话比较耗费时间！好的，谢谢提醒。Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：localhost问题描述：
我们采用了4台阿里云服务器 16G的  ，完整部署了一套猪齿鱼平台（未部署监控）。
现在为了开发，新建了一个 namespace ,在这个里面部署了 redis，kafka ，zk
和 微服务开发框架 ( register server、 config server、 manager service、 iam service、 api gateway、 gateway helper、 oauth server、event-store-service)。
之后部署了一个 自己开发的java应用 ，报错提示内存不足。
使用free -m 发现 free 很少 ，但 available 还有好几个G报错信息(请尽量使用代码块的形式展现)：
疑问：
我们这种情况时要添加服务器吗 还是可以通过相应配置 来提高可以部署服务的数量建议你减少申请内存。在部署时候将 requests.memory 调低一些Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。如：BaseMapper中的selectCount()和多语言起冲突，造成ambiguous错误。报错信息(请尽量使用代码块的形式展现)：; SQL []; Column ‘uom_id’ in field list is ambiguous; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Column ‘uom_id’ in field list is ambiguous不好意思，这个确实是一个Bug，我们将在这周发布0.5.4的时候修复，谢谢反馈Choerodon平台版本: 0.7.0遇到问题的执行步骤:
在使用分步部署时，发现下载docker镜像比较慢，查看了pod的详情，发现image和一键部署时不一致
如manager-service这个 ，一键部署时为  registry.cn-shanghai.aliyuncs.com/choerodon/manager-service:0.7.0
而分步部署为 choerodon/manager-service:0.7.0文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/#添加choerodon-chart仓库原因分析:
是不是因为一键部署时 可以设置镜像仓库地址。而分步部署时没有这一步 ，就需要手动把 image.repository这个参数加上。我们在提供了 dockerhub 及阿里云镜像仓库，一般情况下使用一键安装脚本使用的阿里云镜像库，相比于阿里云镜像仓库多数用户更熟悉dockerhub及兼顾境外用户, 分布部署则使用dockerhub镜像地址。嗯 好的 那我分步部署的时候也是可以手动修改image仓库地址的 是吗可以的Choerodon平台版本: 0.7.0遇到问题的执行步骤:一键部署choerodon文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息): node1（master），node2报错日志:Error from server (NotFound): namespaces “choerodon-devops-dev\r” not found
➜ choerodon-mysql not ready,sleep 5s,check it.
Error from server (NotFound): namespaces “choerodon-devops-dev\r” not found
➜ choerodon-mysql not ready,sleep 5s,check it.
Error from server (NotFound): namespaces “choerodon-devops-dev\r” not found
➜ choerodon-mysql not ready,sleep 5s,check it.下面是values.sh 我修改了namesapce为choerodon-devops-dev 其他基本没变NAMESPACE=“choerodon-devops-dev”
我查看了一下我的K8S内的namespace，发现是有choerodon-devops-dev的
疑问：是不是跟我修改的namesapce 有关？这个可能是您在windows中的编辑器导致 \n 变成了 \n\r 而你在linux系统中执行 无法识别 \r建议您换个编辑器编辑。或替换掉所有的 \r是的，刚刚我也查到这个了，说的是编辑器的问题，非常感谢。Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：k8s问题描述：
在应用部署的时候，选择应用及版本后 ，选择了 环境，跳出 配置信息。这时 下一步的按钮是灰的 无法点击。修改了配置 这个按钮仍然是灰的 无法点击报错信息(请尽量使用代码块的形式展现)：
原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问请查看一下环境状态是否为“未连接”，即有个小红点？环境是激活状态的请问network有报错信息吗？你重新部署一次，把刚才那个截图里的value那个api的数据截图出来。请问你的devops-service的版本是多少devops-service-0.6.3平台前端和devops前端的版本是多少？choerodon-front-0.6.4 前端只有这个这个问题是你的前devops-service和choerodon-front的版本不一致导致的，我们devops-service修复了yaml格式错误问题，你的devops-serivce的版本没有这个功能，可以升级devops-service至0.6.4及以上。我是上周 使用 一键部署 按照的 。我上周部署没有问题 。但为什么现在出现了。。
而且我现在更新 devops-service 0.6.5 时 又发生问题了
把devops-user表里面的1-1 先改成1-2 然后更新 ，更新完之后在改回来 ，这个问题要0.7.0的才解决@younger
成员角色关闭表中member_type字段除了user还有什么type？与本topic不相干问题，请另发topic。Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：已经部署的实例，删除实例后重新创建。状态一直未正常，如下图：
原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问你好，请先删除再重新安装，实例应该是被删掉了。Choerodon平台版本：0.7.0运行环境：https://choerodon.com.cn问题描述：敏捷管理的创建子任务的UI界面又BUG已在当前版本修复，由于不影响功能使用未进行hotfix修复，感谢指出:grinning:Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost我们现在想使用猪齿鱼平台微服务开发，哪些服务是必须的。后端：前端：有每个微服务的API手册吗？你可以将这些服务部署起来，然后通过manager的swagger查看所有接口。后续我们会推出API管理功能eureka-server和go-register-server有什么区别，我本地跑服务只起eureka-server可以吗？go-register-server 是运行在k8s集群的，本地使用eureka-server就可以了go-register-server
api-gateway
gateway-helper
oauth-server
config-server
manager-service
iam-service
eureka-server
这些服务的调用关系是什么如果我们自己开发如何来使用微服务中的权限认证和角色权限认证。@bubuchoerodon 基于RBAC 的模式来进行权限认证。只要保证你的服务注册到我们的注册中心，而且用户有分配对应的角色权限，就可以调用自己服务的接口。关于各服务的功能可以参考



GitHub



choerodon/choerodon-framework
choerodon-framework - Choerodon Microservices Framework.





choerodon-framework - Choerodon Microservices Framework.有iam-service和manager-service的表的需求详细设计吗，他们的表关系和字段是什么意思？后面我们会出对应的表结构设计文档，你可以先在github上看到他们的表结构。iam-service - IAM Service is used for the management of user, role, permission, organization, project, password policy, fast code, client, menu, icon, multi-language , and supports for importing th...manager-service - This service is the management center of the choerodon microservices framework.为什么没有notification-service这个服务？对的，暂时是没有notification-service这个服务的我写的deom，可以注册到eureka上，在api中也加入了权限，为什么swagger访问不了manager-service数据库route表里面要加上对应的路由本地只启动这五个服务不行吗？
看下manager-service里面的route表里面有没有数据没有创建组织的接口吗？页面操作对应的是分页查询的接口啊@bubu 你好，可以参考这个帖子。@Fan
您好，在源码中没有对密码的加密方式吗？如果有在什么地方，怎样来对密码进行加密Choerodon平台版本: 0.7.0遇到问题的执行步骤:文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:
根据Kubernete步骤做完之后，截图左边是master，所有pod都是running状态，但是右边的work node kubectl似乎没有启动，请问这是正常的吗？还是我缺少了哪一步没有操作？是正常的，我们安装的kubernetes 非master节点是不能不执行kubectl命令的。另外 你的集群节点hostname都是一样的吗？ 这样可能在某些情况下出问题。我执行hostname命令看了一下，不一样。谢谢！
另外，还请问一下这些work node上面没有启动的k8s，应该在教程的哪一步才启动，具体包括启动那些组件呢？安装完成之后就自动启动了。Choerodon平台版本: 0.7.0遇到问题的执行步骤: NFS安装文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/nfs/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:请问一下，NFS server端应该安装到那一个节点上？k8s的master 还是 worker，或者单独的服务器安装？内网互通的任意主机都可以好的，谢谢！Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：打开一个问题，然后搜索一个不存在的问题，结果提示页面的显示错误，建议优化一下。好的，感谢指出Choerodon平台版本：0.7.0运行环境：https://choerodon.com.cn/问题描述：猪齿鱼的SaaS环境速度太慢，导致创建两个同样的问题。截图如下：这是通过哪里创建的
1.问题管理界面的创建问题
2.问题管理界面的快速创建
3.待办事项的快速创建点击一次创建出来两个还是说因为卡顿多次点击产生的原因？这个是在待办事项中的，快速创建，创建的点击一次，卡顿，然后又点击。感谢指出，会进行修复Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：开发流水线中创建应用的时候，不选择模板，创建的代码库默认是没有任何代码的，那么就不存在分支。此时，打开 “分支管理” ，就会报错500，另外这里没有错误提示，一直在刷新嗯，这个bug我们发现了，新版本会修复这个问题给研发团队点赞哈哈，这个我提过了:smile:Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：在建立冲刺之后，右上角的统计均为0报错信息(请尽量使用代码块或系统截图的形式展现)：
你好，图示位置显示的统计为（待办、处理中、已完成）故事点统计好的，明白了，谢谢Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：是否考虑在待办事项列表中，增加问题所属模块的描述，否则查看问题时，需要点进问题的详细描述中，才能看到其所属的模块。已记录该需求，会在后续的版本迭代中进行讨论，谢谢反馈Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：使用的猪齿鱼的前端标准模板创建的工程，发现生成的工程没有iam文件夹我们正在查看这个问题 建议您先将 .gitlab-ci.yml 文件中13行和26行中的 iam 改为你的模块名字。Choerodon平台版本: 0.6.0遇到问题的执行步骤:
在访问搭建好的Choerodon的api， api.choerodon.com/manager/swagger-ui.html ，选择 iam_service>> client-controller >>  创建client.
在swagger界面认证报错文档地址:环境信息(如:节点信息):
[root@node1 ~]# kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
node1     Ready     master    1d        v1.8.5
node2     Ready     master    1d        v1.8.5
node3     Ready     master    1d        v1.8.5报错日志:
添加Gitlab Client步骤时，使用admin/admin登录报错，如下
OAuth Error
error=“invalid_grant”, error_description=“Invalid redirect: http://api.choerodon.com/manager/webjars/springfox-swagger-ui/o2c.html does not match one of the registered values: [please edit]”原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问不好意思，这是我们初始化数据库时候iam-service.oauth_client表里name=client那条记录的web_server_redirect_uri是please edit的原因，你把这个重定向url置为null就好了。我们下个版本会修复这个问题的已修改，
mysql> update oauth_client set web_server_redirect_uri=’’ where id=1;
Query OK, 1 row affected (0.01 sec)
Rows matched: 1  Changed: 1  Warnings: 0mysql> select id,name,web_server_redirect_uri from oauth_client;
±—±----------±------------------------+
| id | name      | web_server_redirect_uri |
±—±----------±------------------------+
|  1 | client    |                         |
|  2 | localhost | http://localhost:9090   |
±—±----------±------------------------+
2 rows in set (0.00 sec)但是报另外的错误。
认证的选择的是default scope
在登录页面输入用户名和密码后报invalid_scopename=client的那条记录改了scope字段吗？目前这条记录的数据如下：
mysql> select id,name,scope,web_server_redirect_uri from oauth_client where id=1;
±—±-------±--------±------------------------+
| id | name   | scope   | web_server_redirect_uri |
±—±-------±--------±------------------------+
|  1 | client | default |                         |
±—±-------±--------±------------------------+scope为default，需要修改吗？贴一下oauth-service的报错日志贴一下oauth-service的报错日志2018-07-05 20:06:27.096  INFO [oauth-server,60c1dce876736ae4,70efb39b30642947,true] 1 — [ XNIO-3 task-14] o.s.s.o.p.e.AuthorizationEndpoint        : Handling OAuth2 error: error=“invalid_scope”, error_description=“Invalid scope: default,vendorExtensions”, scope=“default”谢谢。
按您的方法，认证已经成功。但是gitlab添加认证后，无法登陆
认证的post报文如下：
curl -X POST --header ‘Content-Type: application/json’ --header ‘Accept: application/json’ --header ‘Authorization: Bearer 293608f4-7f49-453e-ba8c-db67536caf9f’ -d ‘{ \
“accessTokenValidity”: 60, \
“additionalInformation”: “”, \
“authorizedGrantTypes”: “implicit,client_credentials,authorization_code,refresh_token”, \
“autoApprove”: “default”, \
“name”: “gitlab”, \
“objectVersionNumber”: 0, \
“organizationId”: 1, \
“refreshTokenValidity”: 60, \
“resourceIds”: “default”, \
“scope”: “default”, \
“secret”: “secret”, \
“webServerRedirectUri”: “http://gitlab.choerodon.com” \
}’ ‘http://api.choerodon.com/iam/v1/organizations/1/clients’认证的返回如下
{
“id”: 3,
“name”: “gitlab”,
“organizationId”: 1,
“resourceIds”: “default”,
“secret”: “secret”,
“scope”: “default”,
“authorizedGrantTypes”: “implicit,client_credentials,authorization_code,refresh_token”,
“webServerRedirectUri”: “http://gitlab.choerodon.com”,
“accessTokenValidity”: 60,
“refreshTokenValidity”: 60,
“additionalInformation”: “”,
“autoApprove”: “default”,
“objectVersionNumber”: 1
}同时更新了gitlab的mysql数据
mysql> INSERT INTO gitlabhq_production.identities(extern_uid, provider, user_id, created_at, updated_at)
-> VALUES (‘1’, ‘oauth2_generic’, 1, NOW(), NOW());
Query OK, 1 row affected (0.00 sec)之后登录gitlab报500错误同时oauth报错如下
2018-07-05 21:22:49.110  WARN [oauth-server,7384d77bd5258188,062a3b49bfe9b487,true] 1 — [ XNIO-3 task-31] i.c.o.i.c.u.CustomClientDetailsService   : parser addition info error: {}com.fasterxml.jackson.databind.JsonMappingException: No content to map due to end-of-input
at [Source: ; line: 1, column: 0]
at com.fasterxml.jackson.databind.JsonMappingException.from(JsonMappingException.java:270) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3838) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3783) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2842) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at io.choerodon.oauth.infra.common.util.CustomClientDetailsService.loadClientByClientId(CustomClientDetailsService.java:59) ~[classes!/:0.7.0.RELEASE]
at sun.reflect.GeneratedMethodAccessor284.invoke(Unknown Source) ~[na:na]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_121]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_121]
at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:133) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:121) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at com.sun.proxy.$Proxy151.loadClientByClientId(Unknown Source) [na:na]
at org.springframework.security.oauth2.provider.approval.ApprovalStoreUserApprovalHandler.checkForPreApproval(ApprovalStoreUserApprovalHandler.java:113) [spring-security-oauth2-2.0.13.RELEASE.jar!/:na]
at org.springframework.security.oauth2.provider.endpoint.AuthorizationEndpoint.authorize(AuthorizationEndpoint.java:160) [spring-security-oauth2-2.0.13.RELEASE.jar!/:na]
at sun.reflect.GeneratedMethodAccessor424.invoke(Unknown Source) ~[na:na]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_121]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_121]
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:687) [javax.servlet-api-3.1.0.jar!/:3.1.0]
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar!/:3.1.0]
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:85) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) [spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110) [spring-boot-actuator-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:200) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.security.oauth2.client.filter.OAuth2ClientContextFilter.doFilter(OAuth2ClientContextFilter.java:60) [spring-security-oauth2-2.0.13.RELEASE.jar!/:na]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:105) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.session.web.http.SessionRepositoryFilter.doFilterInternal(SessionRepositoryFilter.java:167) [spring-session-1.3.0.RELEASE.jar!/:na]
at org.springframework.session.web.http.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:80) [spring-session-1.3.0.RELEASE.jar!/:na]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186) [spring-cloud-sleuth-core-1.2.5.RELEASE.jar!/:1.2.5.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106) [spring-boot-actuator-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106) [micrometer-spring-legacy-1.0.2.jar!/:1.0.2]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:211) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:809) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]麻烦帮忙看看  谢谢看报错信息，可能是你的additionalInformation配置的有问题，确保它是一个 Map<String,Object>类型的json字符串additionalInformation在mysql数据库中，我按照id为1的配置，将id为3对应的additionalInformation从空值修改为NULL，但是依然报错。
官方文档这块也没有明示要填什么内容。
mysql> select * from oauth_client\G;
*************************** 1. row ***************************
id: 1
name: client
organization_id: 1
resource_ids: default
secret: secret
scope: default
authorized_grant_types: password,implicit,client_credentials,authorization_code,refresh_token
web_server_redirect_uri:
access_token_validity: NULL
refresh_token_validity: NULL
additional_information: NULL
auto_approve: default
object_version_number: 1
created_by: 0
creation_date: 2018-07-04 10:05:59
last_updated_by: 0
last_update_date: 2018-07-04 10:05:59
*************************** 2. row ***************************
id: 2
name: localhost
organization_id: 1
resource_ids: default
secret: secret
scope: default
authorized_grant_types: password,implicit,client_credentials,authorization_code,refresh_token
web_server_redirect_uri: http://localhost:9090
access_token_validity: NULL
refresh_token_validity: NULL
additional_information: NULL
auto_approve: default
object_version_number: 1
created_by: 0
creation_date: 2018-07-04 10:05:59
last_updated_by: 0
last_update_date: 2018-07-04 10:05:59*************************** 3. row ***************************
id: 3
name: gitlab
organization_id: 1
resource_ids: default
secret: secret
scope: default
authorized_grant_types: implicit,client_credentials,authorization_code,refresh_token
web_server_redirect_uri: http://gitlab.choerodon.com
access_token_validity: 60
refresh_token_validity: 60
additional_information: NULL
auto_approve: default
object_version_number: 1
created_by: 1
creation_date: 2018-07-05 21:15:11
last_updated_by: 1
last_update_date: 2018-07-05 21:15:11没看出来异常原因，你可以试下把additionalInformation修改为{“name”:“test”}啥的多测试一下{“name”:“test”}已改
mysql> update oauth_client set additional_information="{name:test}" where id=3;
Query OK, 1 row affected (0.01 sec)
Rows matched: 1  Changed: 1  Warnings: 0但是还是不行。gitlab有报错如下：
==> /var/log/gitlab/gitlab-rails/production.log <==Faraday::ConnectionFailed (Failed to open TCP connection to api.choerodon.com:80 (getaddrinfo: Temporary failure in name resolution)):
config/initializers/customize_oauth.rb:61:in build_access_token' lib/gitlab/middleware/multipart.rb:93:incall’
lib/gitlab/request_profiler/middleware.rb:14:in call' lib/gitlab/middleware/go.rb:18:incall’
lib/gitlab/etag_caching/middleware.rb:11:in call' lib/gitlab/middleware/read_only.rb:31:incall’
lib/gitlab/request_context.rb:18:in call' lib/gitlab/metrics/requests_rack_middleware.rb:27:incall’##oauth报错如下
2018-07-06 12:34:50.438  WARN [oauth-server,7c12b6414c5a9e24,25ce336bd3e9643b,true] 1 — [ XNIO-3 task-28] i.c.o.i.c.u.CustomClientDetailsService   : parser addition info error: {}com.fasterxml.jackson.core.JsonParseException: Unexpected character (‘n’ (code 110)): was expecting double-quote to start field name
at [Source: {name:test}; line: 1, column: 3]
at com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1702) ~[jackson-core-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:558) ~[jackson-core-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.core.base.ParserMinimalBase._reportUnexpectedChar(ParserMinimalBase.java:456) ~[jackson-core-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddName(ReaderBasedJsonParser.java:1771) ~[jackson-core-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextFieldName(ReaderBasedJsonParser.java:921) ~[jackson-core-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.deser.std.MapDeserializer._readAndBindStringKeyMap(MapDeserializer.java:493) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:362) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.deser.std.MapDeserializer.deserialize(MapDeserializer.java:27) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3798) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2842) ~[jackson-databind-2.8.8.jar!/:2.8.8]
at io.choerodon.oauth.infra.common.util.CustomClientDetailsService.loadClientByClientId(CustomClientDetailsService.java:59) ~[classes!/:0.7.0.RELEASE]
at sun.reflect.GeneratedMethodAccessor284.invoke(Unknown Source) ~[na:na]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_121]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_121]
at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:333) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:133) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:121) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:213) [spring-aop-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at com.sun.proxy.$Proxy151.loadClientByClientId(Unknown Source) [na:na]
at org.springframework.security.oauth2.provider.approval.ApprovalStoreUserApprovalHandler.checkForPreApproval(ApprovalStoreUserApprovalHandler.java:113) [spring-security-oauth2-2.0.13.RELEASE.jar!/:na]
at org.springframework.security.oauth2.provider.endpoint.AuthorizationEndpoint.authorize(AuthorizationEndpoint.java:160) [spring-security-oauth2-2.0.13.RELEASE.jar!/:na]
at sun.reflect.GeneratedMethodAccessor424.invoke(Unknown Source) ~[na:na]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_121]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_121]
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:687) [javax.servlet-api-3.1.0.jar!/:3.1.0]
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) [javax.servlet-api-3.1.0.jar!/:3.1.0]
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:85) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55) [spring-boot-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110) [spring-boot-actuator-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:317) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:127) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.access.intercept.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:91) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.access.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:114) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.session.SessionManagementFilter.doFilter(SessionManagementFilter.java:137) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.authentication.AnonymousAuthenticationFilter.doFilter(AnonymousAuthenticationFilter.java:111) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:170) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.savedrequest.RequestCacheAwareFilter.doFilter(RequestCacheAwareFilter.java:63) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.authentication.AbstractAuthenticationProcessingFilter.doFilter(AbstractAuthenticationProcessingFilter.java:200) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.authentication.logout.LogoutFilter.doFilter(LogoutFilter.java:116) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.header.HeaderWriterFilter.doFilterInternal(HeaderWriterFilter.java:64) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.context.SecurityContextPersistenceFilter.doFilter(SecurityContextPersistenceFilter.java:105) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter.doFilterInternal(WebAsyncManagerIntegrationFilter.java:56) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.security.web.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:331) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:214) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177) [spring-security-web-4.2.2.RELEASE.jar!/:4.2.2.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.security.oauth2.client.filter.OAuth2ClientContextFilter.doFilter(OAuth2ClientContextFilter.java:60) [spring-security-oauth2-2.0.13.RELEASE.jar!/:na]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:105) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.session.web.http.SessionRepositoryFilter.doFilterInternal(SessionRepositoryFilter.java:167) [spring-session-1.3.0.RELEASE.jar!/:na]
at org.springframework.session.web.http.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:80) [spring-session-1.3.0.RELEASE.jar!/:na]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186) [spring-cloud-sleuth-core-1.2.5.RELEASE.jar!/:1.2.5.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106) [spring-boot-actuator-1.5.3.RELEASE.jar!/:1.5.3.RELEASE]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106) [micrometer-spring-legacy-1.0.2.jar!/:1.0.2]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.3.8.RELEASE.jar!/:4.3.8.RELEASE]
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:131) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104) [undertow-servlet-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:211) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:809) [undertow-core-1.4.13.Final.jar!/:1.4.13.Final]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_121]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
at java.lang.Thread.run(Thread.java:745) [na:1.8.0_121]2018-07-06 12:35:27.341  INFO [oauth-server,] 1 — [  XNIO-2 task-5] c.c.c.ConfigServicePropertySourceLocator : Fetching config from server at: http://config-server.choerodon-devops-prod:8010/
2018-07-06 12:35:27.348  INFO [oauth-server,] 1 — [  XNIO-2 task-5] c.c.c.ConfigServicePropertySourceLocator : Located environment: name=manager-service, profiles=[default], label=null, version=null, state=null
2018-07-06 12:36:46.134  INFO [oauth-server,] 1 — [trap-executor-0] c.n.d.s.r.aws.ConfigClusterResolver      : Resolving eureka endpoints via configurationweb_server_redirect_uri换成正确的gitlab域名啊，你设置http://gitlab.choerodon.com成肯定不行啊本地写了host了
解析应该是没有问题的。确实存在这个问题。我先看下kube-dns的配置gitlab.choerodon.com配了内部dns，依然不行
[root@node1 ~]# kubectl exec -n kube-system kube-dns-79d99555df-hkmz9 – nslookup gitlab.choerodon.com
Defaulting container name to kubedns.
Use ‘kubectl describe pod/kube-dns-79d99555df-hkmz9’ to see all of the containers in this pod.
nslookup: can’t resolve ‘(null)’: Name does not resolveName:      gitlab.choerodon.com
Address 1: 10.201.143.51 charts.choerodon.com
[root@node1 ~]# more /etc/hosts
127.0.0.1 localhost localhost.localdomain
::1 localhost6 localhost6.localdomain10.201.143.51	Choerodon1.blappteam.com Choerodon110.201.143.51 node1 node1.cluster.local
10.201.143.52 node2 node2.cluster.local
10.201.143.53 node3 node3.cluster.local10.201.143.55 nfs.choerodon.com
10.201.143.51 api.choerodon.com
10.201.143.51 devops.choerodon.com
10.201.143.51 gitlab.choerodon.com
10.201.143.51 charts.choerodon.com
10.201.143.51 minio.choerodon.com
10.201.143.51 registry.choerodon.com
10.201.143.51 iam.choerodon.choerodon.comChoerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：现在参考 choerodon-front-iam 编写了一个前端界面，现在有一个需求在 Table控件中实现 原始数据为数字类型的时间戳 显示需要渲染成日期格式，请问该如何实现。修改的数据：报错信息(请尽量使用代码块的形式展现)：原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问找到了 加  render 这个key 。有使用文档，你可以去参考ANT D https://ant.design/index-cn，我们的在官网上也会尽快的把这一块的内容添加上去。Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：由于网络原因（有些npm依赖在AWS上，自带的Runner经常下载失败）需要自己建Runner构建项目。我们自己在Kubernetes上搭建了Gitlab Runner，按照这篇文章配置：
https://docs.gitlab.com/runner/install/kubernetes.htmlRunner启动之后出现了下面的错误：
ERROR: Checking for jobs… forbidden               runner=KhyM_JZH
ERROR: Checking for jobs… forbidden               runner=KhyM_JZH
ERROR: Checking for jobs… forbidden               runner=KhyM_JZH
ERROR: Runner https://code.choerodon.com.cn/这里是Token is not healthy and will be disabled!试着访问上面的地址，得到的是404页面。
请问应该如何操作@ShadowPower  请参考我们文档中的runner搭建方式。http://v0-7.choerodon.io/zh/docs/installation-configuration/steps/gitlab-runner/客户使用源码搭建的时候，客户对每个微服务都配置了域名，且必须采用https访问。
看到manager-service的配置文件里有一个
发现swagger测试的时候，登录跳转回swagger时，采用的是http，就会出现问题。有什么好的解决方法吗直接采用 api-gatewayip+端口/oauth/oauth/authorize，访问这个地址可以访问的通
你这是本地跑的还是线上？这个变量线上生效的是环境变量配置，环境配置成正确的gateway域名+/oauth/oauth/authorize。那你gateway域名+/manager/swagger-ui.html可以访问的通的吧？晚上推代码，构建镜像一直报错，是不是公共的runner有问题？
Running with gitlab-runner 10.7.2 (b5e03c94)
on choerodon-runner 9b204dac
Using Kubernetes namespace: tools
Using Kubernetes executor with image registry.saas.hand-china.com/tools/devops-ci:1.1.0 …
Waiting for pod tools/runner-9b204dac-project-291-concurrent-09m5b8 to be running, status is Pending
Waiting for pod tools/runner-9b204dac-project-291-concurrent-09m5b8 to be running, status is Pending
Waiting for pod tools/runner-9b204dac-project-291-concurrent-09m5b8 to be running, status is Pending
…
Waiting for pod tools/runner-9b204dac-project-291-concurrent-09m5b8 to be running, status is Pending
Waiting for pod tools/runner-9b204dac-project-291-concurrent-09m5b8 to be running, status is Pending
ERROR: Job failed (system failure): timedout waiting for pod to start这个是因为服务器资源有限，runner在执行其他job，可以等待一会儿或过一段时间重试Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：大佬们，可以麻烦提供一下chart_build里的脚本吗？报错了就很懵逼，不知道做了啥操作。感谢大佬们！function chart_build(){
# 查找Chart.yaml文件
CHART_PATH=find . -maxdepth 3 -name Chart.yaml
# 重置values.yaml文件中image.repository属性
sed -i "s,repository:.$,repository: ${DOCKER_REGISTRY}/${GROUP_NAME}/${PROJECT_NAME},g" ${CHART_PATH%/}/values.yaml
# 构建chart包，重写version与app-version为当前版本
helm package ${CHART_PATH%/} --version ${CI_COMMIT_TAG} --app-version ${CI_COMMIT_TAG}
TEMP=${CHART_PATH%/}
FILE_NAME=${TEMP##*/}
# 通过Choerodon API上传chart包
curl -X POST 
-F “token=${Token}” 
-F “version=${CI_COMMIT_TAG}” 
-F “file=@${FILE_NAME}-${CI_COMMIT_TAG}.tgz” 
-F “commit=${CI_COMMIT_SHA}” 
-F “image=${DOCKER_REGISTRY}/${GROUP_NAME}/${PROJECT_NAME}:${CI_COMMIT_TAG}” 
“${CHOERODON_URL}/devops/ci”
# 判断本次上传是否出错
if [ $? -ne 0 ]; then
echo “upload chart error”
exit 1
fi
}了解，Thank you再问一下 CI_COMMIT_TAG作为版本值，这个格式有什么要求，需匹配什么正则呢http://choerodon.io/zh/docs/development-guide/basic/gitlab-ci/Contribute to cibase development by creating an account on GitHub.Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：创建实例的时候报错，貌似路径中多了一个空格，请问这个是什么原因报错信息：CI没有问题已经构建通过了。charts结构如下:以下是我的CI文件：那个空格就是单词中间的正常空格，表示在仓库里面找不到该版本的chart包，可能将 Chartmuseum  Pv    和choerodon-devops-service的 pv没有绑定 ，详情见文档http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/chartmuseum/http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-devops/问题找到了，不是pv的问题，而是chart的名称与项目的名称不一致。恩 chart.yaml下的name创建应用的时候会自动渲染的，不用改改了就找不到tgz包了Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。改了配置文件中的clientId用// config.js
const config = {
local: true, //是否为本地开发
clientId: ‘myclint’, // 必须填入响应的客户端（本地开发）
titlename: ‘Demo’, //项目页面的title名称
// favicon: ‘favicon.ico’, //项目页面的icon图片名称
theme: {
‘primary-color’: ‘#3F51B5’,
},
cookieServer: ‘’, // 子域名token共享
server: ‘http://api.xxx.xxx’, // 后端接口服务器地址
port: 9090 // 端口
};module.exports = config;oauth/oauth/authorize?response_type=token&client_id=localhost&state=如果不想用 localhost呢 因为某些原因，我把localhost地址指向了其他的ip。不用localhost的话你可以自己建个client,配置自定义web_server_redirect_uri，就是登陆成功后的重定向地址Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：这个地方与安装runner时定义的环境变量不一样。好的，马上修改Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：创建一个前端应用示例的同时也需要创建后端spring boot库吗?那是要先部署后端示例是吗？
想要实践开发的流程，是需要前端后端的搭配的。
开发提供了一个Spring boot后端模板的示例。但这不是必须的。前端的模板理论上是可以搭配任意的后端进行开发的这是我们文档的一些错误，创建前端是可以不需要后端对应服务的Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：创建项目时，不选择任何模板。然后点击分支管理则报错。原因是git仓库中不存在任何分支，但是没有任何提示。还有当git库中没有develop分支时，创建feature分支也会报错，仍然没提示。报错信息：
建议：我觉得即使是不选择模板的git仓库，也可以默认创建master和develop分支，并加入README和 .gitignore文件目前，不选模板情况下，用户可以根据自己的需求将需要的分支推至仓库。您的建议我们已经记下了，非常感谢~Choerodon平台版本：0.7.0运行环境：SaaS环境问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。执行的操作：
上传应用图标,点击上一步,再点击下一步报错信息(请尽量使用代码块或系统截图的形式展现)：
get，我们会在新版本修复该问题Choerodon平台版本：0.7.0运行环境：https://choerodon.com.cn问题描述：建议开启冲刺中的开始时间可以任意选择，不要做当前时间的限制。我们会斟酌一下该需求，由于这里开启冲刺的操作是对冲刺开启的一个时间预估，所以做了不能选择低于当日的限制，但是开启时间可以在待办中的冲刺信息中进行修改。Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：待办事项中的问题拖入到冲刺中，之后再拖出来，左上角的人员列表没有变化收到，已记录该问题，会在下版本进行修复Choerodon平台版本：0.7.0运行环境：https://choerodon.com.cn问题描述：在待办事项界面，如果将浏览器的窗口缩小，然后点击“问题”，弹出侧边界面，滑动滚动条，出现界面BUG。如下截图：get，我们会尽快修复该问题当前虚拟化三台集群，k8s-master、k8s-node1、k8s-node2，在k8s-master中安装了NFS服务端，并已开启，k8s-node1、k8s-node2安装了NFS客户端k8s-master上通过自定义DNS，也就是按照文档中的方式，初始生成了example.choerodon.io这个域名按照分部部署方式，比如说部署Chartmuseum中  nfs.example.choerodon.io这个域名该如何配置如下charts.example.choerodon.io等等域名也同样，麻烦请给个示例，谢谢哈！比如说部署Chartmuseum中 nfs.example.choerodon.io这里的域名可以使用你nfs服务器的IP。如果你想进行配置，那么请在集群的每台节点的/etc/hosts中增加nfs服务器的IP nfs.example.choerodon.io这样的一条记录。对于Chartmuseum的域名charts.example.choerodon.io无需手动配置，执行安装命令即可。ok，好的哈，谢谢了Choerodon平台版本:agile-service: 0.6.1遇到问题的执行步骤:直接部署agile-service: 0.6.1版本初始化数据库是会报表不存在的错误，但是先部署0.5.3版本，然后再升级到0.6.1，则没问题。文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-agile/报错日志:谢谢反馈，我们会尽快处理解决Choerodon平台版本：0.7.0运行环境：https://choerodon.com.cn问题描述：该描述是指，在设置中的快速搜索进行定义，然后会在以下位置进行生效：
1、待办事项的快速搜索工具栏
我觉的这句话是有语法错误的。我们再斟酌一下，想一个更清楚的描述这有助于在进一步提升团队管理水平，提高交付质量。Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：1.如截图，第一个“您”可以去掉2.如截图，第二个红框，“工作工作”3.如家图，第三个红框，“ 这有助于在团队管理方面取得更进一步的掌控和把握”，这句话有问题。谢谢反馈，已经记录该问题，会在下个版本进行修复改善Choerodon平台版本：0.7.0运行环境：https://choerodon.com.cn问题描述：敏捷管理的项目设置功能，“项目Code”建议写成“项目编码”get，会在下个版本进行修改！Choerodon平台版本：0.7.0运行环境(如localhost或k8s)：https://choerodon.com.cn问题描述：请问在哪里可以查看Choerodon的版本？各微服务在发布至GitHub时有对应的版本号，但目前整体平台的版本还需以推文为准  之后我们会将此版本信息体现在系统上~Choerodon平台版本：0.7.0请问Choerodon支持哪些浏览器？目前我使用搜狗浏览器有问题，可以登录，但是进不去。目前我们主要支持谷歌浏览器，其他浏览器或许有一些小小的不兼容。我觉的这个是不是可以写到文档中。k8s主节点上，执行如下：执行报错：Error: release dnsmasq failed: namespaces "choerodon-devops-prod" is forbidden: User "system:serviceaccount:kube-system:default" cannot get namespaces in the namespace "choerodon-devops-prod"k8s集群是正常运行的我这个就是在master节点上执行的，而且我是在root用户请问你安装Helm的时候是按这里教程安装的 http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/helm/还是其他方式？是的，不过如下的这个步骤一开始的时候我没有执行，跳过了
你搭建集群是按照这里搭建的吗  http://choerodon.io/zh/docs/installation-configuration/steps/kubernetes/如果是那请执行这一步,此安装教程是默认开启RBAC的，然后重启tiller是滴，不过我这只是安装测试环境那部分来的这个K8S安装教程不管是哪个环境都是默认开启RBAC认证的。请执行：然后强制更新tillertiller重新运行后再执行dnsmasq安装命令试过了，还是不行，同样报那个问题则个命令执行了吗？之前执行过了现在你复制我发的这个命令执行下   给我下日志哈再执行以下以下语句反馈下返回的信息哈第一条命令：第二条命令：
请执行以下命令添加serviceAccount属性添加serviceAccount属性然后等tiller成功运行后  你就可以执行 dnsmasq 安装命令了可以了哈，谢谢大佬 Choerodon平台版本: 0.6.0遇到问题:请问0.6版本如何升级到0.7版本？请参考文档：
http://choerodon.io/zh/docs/installation-configuration/update/0.6-to-0.7/猪齿鱼SAAS版本新建问题后创建时间和更新时间不正确，好像是晚了一个月。这是一个bug
原因是前端为了处理特殊样式造成的数据错误，数据存储是ok的，今晚会发布新的版本修改该问题2018年6月29日，开源企业级数字化服务平台——Choerodon猪齿鱼发布0.7版本。0.7版本主要新增敏捷管理服务、持续交付和微服务开发框架的部分功能，并对他们的服务进行了优化，同时修复了0.6版本若干bug。如何升级Choerodon平台，请参阅升级指南。敏捷管理服务新增了如下的功能:并且，在敏捷管理中，0.7版本还增强了如下部分功能：持续交付服务新增了如下的功能：同时，在持续交付中，0.7版本还增强了部分功能：另外，还增强了其它功能，例如：最后，0.7版本修复了0.6版本的缺陷：微服务开发框架增加了如下的功能：同时，在微服务开发框架中，0.7版本还增强了部分功能：最后，0.7版本还修复了0.6版本中的bug。更加详细的内容，请参阅Release Notes和官网。欢迎通过我们的GitHub和猪齿鱼社区进行反馈与贡献，帮助Choerodon猪齿鱼不断成长，我们将持续迭代优化，敬请期待。请问这个菜单栏直接在代码里可以删除或修改么
你好，系统中自带的菜单不可以在页面上删除，你可以通过创建一个角色，然后给角色不指定对应菜单的权限来屏蔽这个菜单这个清楚了，麻烦再问一个问题，现在我想创建一个菜单，先创建了一个自设目录，然后怎么在里面添加菜单呢，用过前端项目里的Menu.yml配置么，配置了以后还是没有显示，是还需要配置权限么你可以将系统自带的菜单拖到自设目录下，也可以通过Menu.yml 初始化创建菜单。
Menu.yml 里面每个菜单会有对应的permission列表，对应到后台接口的权限code。
如果要查看该菜单，需要满足下面条件：Choerodon平台版本：0.7.0运行环境：自主搭建问题描述：刚升级到0.7版本。发现新增加了3个预定义角色，但是这三个角色下都没有已权限。执行的操作：我把原来我自定义的角色给停用掉，再去项目里重新分配一下项目角色，然后，敏捷模块就没权限访问了。报错信息：麻烦大佬们指导一下，哪里权限需要手动刷一下吗好吧，已解决，老毛病，还是要手动去swagger，给所有服务发一遍刷权限的请求。好的。go-register-server用的是啥版本？需要用0.7.1版本，否则要手动配置正确的REGISTER_SERVRE_NAMESPACE。可以看下go-register-server日志，是否发送服务启动消息到消息队列中？REGISTER_SERVRE_NAMESPACE用的是0.7.0版本。刚看了下register-service的deployment文件，发现有多个配置namespace的环境变量，不知道哪个才是正确的，内容如下：其中，REGISTER_SERVICE_NAMESPACE这个变量是在0.6版本安装时指定的。而在本次0.6~0.7升级的脚本并未增加环境变量。在deployment环境变量中出现了两个REGISTER_SERVER_NAMESPACE是怎么回事。是不是要把其中一个去掉。我目前的ns的名称是choerodon-devops-prod。日志信息如下:将版本换成0.7.1吧，REGISTER_SERVER_NAMESPACE是指此注册服务所在的namespace，0.7.1在chart自动替换为环境变量，不需要手动指定，0.7.0需要手动指定正确的环境变量。换成0.7.1就可以了。REGISTER_SERVICE_NAMESPACE指的是注册在这个注册中心的服务所在的namespace，多个可以用逗号分开，变量在README里简单说明0.7.1版本你们的仓库并未推送。您好 执行 helm update  再试一下Choerodon平台版本：0.7.0问题描述：
请问 0.7.0 的猪齿鱼平台 可以添加 组织吗你好，当前版本暂时不支持添加组织，如果你对此有相关的需求或意见，可以向我们反馈，谢谢Choerodon平台版本: 0.6.0遇到问题的执行步骤:在升级choerodon front 执行脚本报错文档地址:
http://choerodon.io/zh/docs/installation-configuration/update/0.6-to-0.7/环境信息(如:节点信息):报错日志:麻烦执行 “helm update” 再试一下OK了  但进入页面后 布局有点小问题
清一下缓存，这个问题我们会马上解决我用无痕打开 就OK了Choerodon平台版本: 0.6.0遇到问题的执行步骤:
git clone https://github.com/choerodon/choerodon-front-iam.git
cd choerodon-front-iam/iam
npm install
npm start报错日志:
原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在
按照文档上安装好 Choerodon环境后，clone代码执行run start的时候报错权限问题。试试start之前执行这条
sudo chmod -R u+x .非常感谢您的回复，可以运行成功。registry.cn-hangzhou.aliyuncs.com/choerodon-tools/hugo:0.40.3这个镜像里面的hugo命令的源码是什么啊？我想看下此网站的构建工具看到了  用了https://gohugo.io/该镜像中的hugo命令就是官方的二进制文件，我们未做任何修改哈。hugo - The world’s fastest framework for building websites.Choerodon平台版本：0.6.0运行环境：公司提供开发是基于hapcloud的，在本地开发的时候，需要搭建那些hapcloud服务吗？
请问，您是打算将原来使用HapCloud开发的应用使用Choerodon管理吗？我打算基于choerodon进行开发，使用choerodon的微服务开发框架。请问那些微服务框架中如registry server、config server等这些服务还需要在本地安装吗。还是可以使用混合开发模式？请参考开发手册
http://choerodon.io/zh/docs/development-guide/如果想要使用choerodon的微服务框架，需要怎么做？建议您先阅读完整文档，在根据具体的问题提问。
http://choerodon.io/zh/docs/concept/是否是在master节点执行的呢？是的，我同事配置的是单节点集群的
那这个又是什么问题呢执行  kubectl describe pod tiller-deploy-85654f7c66-b674v -n kube-system  查看下pod信息。参考文档：http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon/
我搭建微服务框架的时候，搭建了这几个服务。命名空间为hitom-devops-dev。
暴露服务后通过IP:8000/eureka/端口访问报404错误。查看日志出现这种错误。
麻烦粘贴一下部署 register-server 的命令。helm install c7n/go-register-server 
–set service.enable=true 
–set service.name=register-server 
–set env.open.REGISTER_SERVICE_NAMESPACE=hitom-devops-dev 
–set env.open.KAFKA_ADDRESSES=“kafka-0.kafka-headless.hitom-devops-dev.svc.cluster.local:9092” 
–name register-server 
–version=0.6.0 --namespace=hitom-devops-devpod启动后，暴露服务后，为什么106.15.178.19:8000/eureka/ 出现404错误，不是访问这个链接吗？怎么验证register是否能访问？注意每一行结尾的 / 不能删除。
访问 /eureka/ 确实是404，这是正常的。
没有删，粘贴就没了。那我后台调用，怎么配置？是直接配置 ip:端口 吗？服务中配置eureka地址为 http://register-server.[命名空间]:8000/eureka/ 即可。验证请访问  ip:8000/eureka/apps， 注意配置时候无需添加 apps。choerodon微服务config-server作用是什么，其他微服务怎么使用这个config？不用指向一个外部的gitlab仓库吗？(类似于hapcloud)。choerodon微服务config-server作用是什么，其他微服务怎么使用这个config？不用指向一个外部的gitlab仓库吗？(类似于hapcloud)。config-server - Configuration center for unified management of service configuration files.Choerodon平台版本：0.5.0运行环境：在线环境 https://choerodon.com.cn/#问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。部分人员在分配好角色后，任务分配界面也选不到该成员执行的操作：
给成员分配了项目所有者、项目成员、部署管理员三个角色，在：活跃冲刺-选中问题- 指派人员，无法选择到该成员。
已维护项目成员30人，只有3人无法选择到，其他成员都能够在指派人员中选择到。报错信息(请尽量使用代码块的形式展现)：
建议：提出您认为不合理的地方，帮助我们优化用户操作抱歉，此为系统bug。我们会尽快修复！请问有一个预计的修复时间嘛？无法分配到指定人员，导致敏捷管理在项目内做不到全员推广，会比较麻烦。今天会发布新的版本，之后就可以了Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：之前部署的go-register-server,今天起在另一个节点上的时候，报拉取镜像失败
之前的节点
修改的数据：原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问请删除启动失败的pod，可能由于网络原因拉取镜像失败根据安装步骤本地k8s集群安装
k8s集群安装正常helm安装
在节点node1中安装helm正常，然后初始化，命令与文档一致，如下：异常如下：node1 是master节点吗， 执行 kubectl get no 检查下kubernetes集群是否正常。(⊙o⊙)…重新启动集群挂了。。。fatal: [node1]: FAILED! => {"changed": true, "cmd": "kubectl -n kube-system get ds -l 'k8s-app=kube-proxy' -o json | jq '.items[0].spec.template.spec.containers[0].command += [\"--masquerade-all\"]' | kubectl apply -f - && kubectl delete pods -n kube-system -l 'k8s-app=kube-proxy'", "delta": "0:00:00.282591", "end": "2018-06-29 16:26:49.782440", "msg": "non-zero return code", "rc": 1, "start": "2018-06-29 16:26:49.499849", "stderr": "W0629 16:26:49.732478 14373 factory_object_mapping.go:423] Failed to download OpenAPI (Get https://192.168.56.11:6443/swagger-2.0.0.pb-v1: dial tcp 192.168.56.11:6443: getsockopt: connection refused), falling back to swagger\nThe connection to the server 192.168.56.11:6443 was refused - did you specify the right host or port?\nerror: no objects passed to apply", "stderr_lines": ["W0629 16:26:49.732478 14373 factory_object_mapping.go:423] Failed to download OpenAPI (Get https://192.168.56.11:6443/swagger-2.0.0.pb-v1: dial tcp 192.168.56.11:6443: getsockopt: connection refused), falling back to swagger", "The connection to the server 192.168.56.11:6443 was refused - did you specify the right host or port?", "error: no objects passed to apply"], "stdout": "", "stdout_lines": []}建议你直接删除虚拟机重新创建虚拟机 重新进行k8s集群部署。同意哈，k8s集群真是难搞，太费劲了，老是出各种问题。。。Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：猪齿鱼上新建的实例如何使用自定义的数据库链接，环境流水线中已经和k8s建立好关系了执行的操作：
目前报错的配置如下报错信息(请尽量使用代码块的形式展现)：
看起来像是数据库连接信息有问题，请问下是否已经部署mysql已经部署了 mysql,本地测试过是可以连接上的，初始数据也都导了数据库的host叫做mysql？你可以直接用数据库的ip+端口
如果是部署在k8s上，可以用service 的名称+端口数据库是部署在k8s上面的
请贴一下values的内容，部署的时候如果有配置job，也会使用到数据库



GitHub



choerodon/choerodon-microservice-template
choerodon-microservice-template - This is a choerodon microservice template.





choerodon-microservice-template - This is a choerodon microservice template.配置如下，麻烦大神看看:sweat_smile:
preInitDB he preConfig 请填写成自己真实的数据库地址，还有其他的变量可以参考上面发的README你好，preJob.preConfig.mysql{}这个初始化配置所需manager_service数据库信息是什么作用？我这边只有几个存储业务数据的database已经可以了 谢谢基于组织部门，创建的项目，使用微服务模版，去掉多余pom依赖及代码，测试demo，本地运行可通，平台上部署出现如上问题，试了很多次麻烦贴一下.gitlab-ci.yml 文件项目结构如下：
@fuchen  环境变量中设置了DOCKER_USER, DOCKER_PWD 这两个变量吗?(⊙o⊙)…，这个需要怎么设置，我是在我们公司的线上环境整的。。。使用组管理员，在Gitlab中选择所在的组，点击设置–> CI/CD 中可以自主添加环境变量。
那对于这两个变量值是可以随意的填还是说填工号和密码？你要推送的docker仓库的用户名及密码。我这用的公司的线上环境平台，应该是公司的私有库，这个用户名和密码我也不知道。。。 如果使用官方的平台。直接删除 docker login -u ${DOCKER_USER} -p ${DOCKER_PWD} ${DOCKER_REGISTRY} 这一步即可 。看应用的运行状态有些不太方便，是否可以考虑增加一个视图，按照应用分组，可以一眼看出该项目组都部署了哪些应用，每个应用下面都有几个容器POD，每个的状态是否可用，可以方便的进去查看容器日志而且，这个界面不能区分这些容器都分别属于哪个环境的，不方便按环境查看日志这个功能下次迭代会发布，请放心~点赞的表情是哪个，手工赞了有个爱心。。Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：跑了一个springboot的微服务应用，对应的容器日志没有。
直接通过Dockerfile的EntryPoint中 执行 java -jar 跑应用去服务器上 通过docker logs  和docker attach 命令连对应容器是有日志的。你打开调试，看一下console会不会有报错，然后你前端部署时devops host那个环境变量配置了没，去服务器上 通过docker logs 和docker attach 命令连对应容器是有日devops-service这个域名地址通的吗还要如果确认地址通的，看下devops-service服务日志有报错吗Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。后端开发demo程序搭建完成，通过本地浏览器访问Controller层时访问不了。修改的数据：未修改数据报错信息(请尽量使用代码块的形式展现)：
This application has no explicit mapping for /error, so you are seeing this as a fallback.Wed Jun 27 20:12:43 CST 2018There was an unexpected error (type=Unauthorized, status=401).No Jwt token in request.原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问服务访问需要JWT，本地调试可以注掉@EnableChoerodonResourceServer可以取消JWT校验，或者请求添加头部一个JWT_Token注掉了@EnableChoerodonResourceServer再次访问要求输入用户名和密码，不知道输入的是什么用户名弹窗时security自带的认证，可以再application-default.yml里面关掉ok，可以了，谢了之前出现该问题将原有配置3个node更改为2个node 还是报cpu资源不足问题，电脑是4核8线程 硬件资源上应该是支持的首先查看这个服务申请了多少的CPU，然后查看每个节点是否有充足的CPU（可分配-已分配）。Choerodon平台版本：0.6.0devops-service版本: 0.6.3
choerodon-front-devops版本：0.6.2
choerodon-front版本: 0.6.5运行环境：自主搭建问题描述：修改应用时请求api报404：
然后查看swagger发现并没有 /devops/v1/projects/6/apps/1/detail 这个api，请问是我的devops服务的版本不对吗？/devops/v1/projects/6/apps/1/detail你好，这个api是devops-service版本: 0.6.4才修改上线的，配套的choerodon-front-devops版本也是0.6.3，应该是你的choerodon-front版本过高的原因。
建议访问choerodon-front-devops版本：0.6.2或者升级devops-serviceChoerodon平台版本：0.6.0问题描述：
1、执行 npm install choerodon-front-boot -S
2、 创建config.js
3、执行  choerodon-front-boot start --config config.js 报错报错信息(请尽量使用代码块的形式展现)：这个是引用choerodon-front-boot npm 包以后，在实际项目下执行的命令choerodon-front-boot 在前端中更多承担的是一个脚手架的作用，包含了一些开箱即用的功能。可以使用它来编译，打包，启动前端项目。我现在在尝试进行一个多模块的前端服务开发。（试图把iam这个服务加进来）。
我查看了github上面 choerodon-front这个项目 。
我使用了 git clone https://github.com/choerodon/choerodon-front --recursive 。
之后在choerodon-front目录下 依次 执行了 npm install  和 npm start .
可以顺利进入到页面中，但点击菜单的页面都是404.也就是子模块都没有加载出来。请问需要怎么操作Choerodon平台版本: 0.6.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:
版主你好：
我使用一键部署，部署结束后，主页打不开 报错404，麻烦帮忙指导下怎么解决？原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请执行以下命令粘贴一下返回结果查看你的POD状态，你部署的时候应该一直提示你等待Gitlab启动中，然后你手动取消了一键部署才会出现现在的结果。请重新执行安装命令，按提示执行删除操作后再进行一键安装，下面命令你需要执行3遍第一遍会提示你删除helm release
第二遍会提示你删除job
第三遍才会进行正式安装kubectl get ingress --all-namespaces你好，结果如图：
部署期间，未作任何操作，只是脚本执行过程中，再部署服务的时候，出现出现的日志信息如下：因为已经配置使用Choerodon登录，这里你应该使用Choerodon的用户密码登录，这里应该是  admin / admin建议手动部署前端 或清空现有部署重新执行脚本 ，手动部署前端文档：
http://v0-7.choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon-front/手动部署前端文档太奇怪了，已经重复部署不知道多少次了，每次都会错。。。。。。有报错信息吗，贴下报错信息就是没有什么报错信息，我是用的一键部署，执行完，就这个样子了检查了POD 状态 是RUNNING 的 ，搞的无从下手。。。我有QQ 可以加你咨询下吗？先执行下 kubectl delete job -n [命名空间] --all 然后再执行一键安装脚本。现在我们仅支持论坛提供支持，方便其他用户查找类似问题，暂不支持其他方式方便其他用户查找类似问题，暂不支持其他方式你好，可以贴一下页面控制台和network信息吗，方便定位问题请查看下console和network，截图贴一下详细信息Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：在项目层，持续交付-》应用，创建一个新的应用 ，该应用的状态 直接处于 创建中您好，在该项目下创建应用时，用户是否在对应gitlab组那边是owner角色?有的 owner角色的您好,是0.6版本初始化devops-user表时 平台admin用户和gitlab admin用户id对应关系错误导致admin用户访问gitlab api失败。在0.7版本已修复。0.6版本麻烦修改下devops-service 的devops-user表中第一条对应关系即可。
嗯 好的 修改了数据，创建应用就OK了Choerodon平台版本：0.6.0运行环境：https://choerodon.com.cn问题描述：待办事项中的完成冲刺和结束冲刺用词不一致截图
已记录，会尽快修复！Choerodon平台版本：0.6.0运行环境：https://choerodon.com.cn问题描述：看板的上下一列没有对齐已记录，我们会及时处理修复浏览器最大化，是没有问题的，但是缩小浏览器之后，就会出现这样的情况。Choerodon平台版本：0.6.0运行环境： https://choerodon.com.cn问题描述：看板的列名称没有办法修改截图
修改列名称的功能会在下个发布版本进行支持我看SQL规范（http://choerodon.io/zh/docs/development-guide/platform/sql/ ）这里有了索引的规范，在看mybatis-mapper文档的时候，看到那里有个SQL脚本创建table，建议把创建的那两个索引的命名改成跟索引规范一致，统一下比较好，大家的代码是学习猪齿鱼最好的材料，借鉴价值很大。嗯嗯，我去改一下，感谢建议我这边要开发一个项目，基于choerodon的微服务框架实现，但是我这边项目中有自己独立的用户模块，请问下能否基于平台提供的用户模块进行封装成自己的用户模块，如果可以的话，我需要引用那些依赖，同时平台的用户模块有木有对外的接口？你好，你可以在我们的用户表结构上，关联自己的用户信息。首先要保证两边的用户数据基础字段一致。其次，choerodon创建，修改，起停用用户时，都会向kafka发送事件，可以在自己的服务中监听这一事件，然后消费。如果需要对choerodon 的用户表进行扩展，则需要在源码的基础上进行一定的修改。Choerodon平台版本：0.6.0运行环境：https://choerodon.com.cn问题描述：
待办事项中开启冲刺界面的“Sprint名称”，建议修改成“冲刺名称”。*截图：
好的~我们会进行修改读下来，感觉这里应该是数据序列化和反序列化，是data而非date 马上改Choerodon平台版本：0.6.0运行环境：https://choerodon.com.cn问题描述：敏捷管理中的待办事项，按shift键+鼠标点击实现多选，拖拽到某一史诗中，例如选择了3个问题，但是只有两个问题可以拖进去。已记录，会在下个版本对该功能进行优化
ctrl进行多选，shift进行批量多选，与日常电脑操作方式一致这个是一个bug。请测试一下下。Choerodon平台版本: 0.5.0遇到问题的执行步骤:尝试过官方的文档但是失败过N次了文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问请问你是否安装文档顺序一步步安装呢。请注意硬件需求，及每一个步安装完成后的验证操作。是的，配置到猪齿鱼的时候总是报出tiller not ready ,这是怎么回事首先确定下k8s集群是否正常PS: 请不要在多个帖子里面问同一个问题，这样比较混乱。在搭建集群完成后，kubectl get po -n kube-system能够查看到信息（但过后一段时间，再次使用此命令则报出：localhost:8080 connect refused），在此之前使用helm ls命令查看pod信息时报出：can not find tiller pod，使用helm version 命令只能查看到client信息而不见server，这些都是什么原因造成的？参考这个帖子




Kubernetes 安装后运行不稳定 Installation management


    在执行完Kubernetes的集群部署完成后，执行 查询K8S nodes 命令  kubectl get nodes ，可以获得信息，但一段时间后就报错了：The connection to the server localhost:8080 was refused - did you specify the right host or port? 
[image]
  

那么在安装完helm后，为何使用helm version命令检查时，只见client未见server？当k8s正常后 执行  kubectl describe pod [tiller的pod名称] -n kube-system  查看下pod信息。Choerodon平台版本: 0.5.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):

报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问
楼主你好，以上3张截图是我安装后的截图，似乎是没有遇见错误，但不知道怎么访问系统。请您帮忙指导下，我的安装是采取一键部署，配置信息是git下来后，修改hosts 文件内的服务器的IP地址和登陆密码，其余未做任何变动脚本执行完毕后有提示，根据脚本提示，访问你配置的对应的域名即可。请务必详读文档中的说明，后再进行安装。域名需要在您的自行在dns服务商中进行解析和映射。每个dns服务商操作都不一样，具体操作请参考您的DNS服务商的相关文档。后再进行安装。域名需要在您的自行在dns服务商中进行解析和映射。每个dns服务商操作都不一样，具体操作请参考您的DNS你好，我是安装了个测试i环境，我的环境内没有DNS，我只是给主机配置了上海电信的DNS地址，服务之间的解析是使用HOSTS文件解析，这个情况要怎么做配置呢？您好，您需要先了解域名和域名解析相关知识。Choerodon安装必须有域名。如果没有域名，请考虑购买域名或使用自定义的DNS服务器，关于如何配置自定义的DNS服务器，我们将在近期更新。欢迎持续关注我们的文档更新。你好 请参考文档
http://choerodon.io/zh/docs/installation-configuration/steps/dns/Choerodon平台版本: 0.5.0遇到问题的执行步骤:分布部署helm时输入helm version命令没有server.文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/helm/环境信息(如:节点信息):报错日志:can not found to tiller pod原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问执行 kubectl get po -n kube-system | grep tiller 查看 tiller 是否正常运行tiller-deploy-55d5556566-9tk6r              1/1       Running   0          5d可以了，这个问题解决了，但是又出现了新问题 !用curl -o choerodon-install.sh \https://file.choerodon.com.cn/choerodon-install/install-0.6.sh && \sh choerodon-install.sh values.sh这个命令一键部署时出现以下错误:
我的同事问，如果tiller没有执行的话该怎么弄？
执行 kubectl describe pod tiller-deploy-85654f7c66-b674v -n kube-system  查看下pod信息。请问你的截图都是在同一个集群上同一台机器上截的吗？Choerodon平台版本：0.6.0运行环境：https://choerodon.com.cn问题描述：使用springboot模板创建的项目git clone时报错。命令：
git clone https://code.choerodon.com.cn/presentation-gettingstarted/spring-boot.git经过排查，是gitlab 10.2.8的bug，我们在测试环境已经升级至10.8.4，需测试通过后这周随0.7.0发布升级。建议现在先重新创建应用。Choerodon平台版本：0.6.0运行环境：https://choerodon.com.cn/问题描述：如上图，点击菜单，出现404错误。这是由于没有默认跳转第一个菜单导致的问题，我们团队将会在0.7.0版本进行修复。 
感谢您的反馈 Choerodon平台版本：0.6.0运行环境：https://choerodon.com.cn问题描述：点击应用发布，页面一直在加载旋转，本以为是API响应慢问题，查看控制台发现报错403，不清楚该功能是否是确实需要授权才能访问，另外，如果确实需要授权，就交互友好性来说，建议加个提示。另外，本问题，不仅在该页面发现，在其他页面也时有出现，可以考虑优化一下。项目成员看不到这个菜单的。
对于没有权限的提示我们之后的迭代里会完善的，感谢建议！客气客气，感谢。Choerodon平台版本：0.6.0运行环境：自主搭建问题描述：请问一下，猪齿鱼上支持创建组织吗？如果不行，是否能修改组织编码？当前我是用admin用户登录，创建组织时报403错误，response里无任何信息。如下图：报错信息(请尽量使用代码块的形式展现)：你好，请看这个贴子针对上面提到的猪齿鱼只能有一个组织的（即运营组织）的问题，我不太能理解，比如我司（XXX公司，mycompany.com）安装了猪齿鱼平台，默认只能唯一有一个运营组织（code为operation），并且也不允许修改code，那也就是说所有的基于猪齿鱼的项目及所有开发活动都必须属于运营组织下面吗？其实，是否能够自己创建组织，比如组织名为：XXX公司，编码为mycompany。另外，我看猪齿鱼的线上环境中就是有多个组织的，如下图：烦请社区管理员解答下，目前我们在做平台配置，卡在组织这一步。另外补充下，既然不能创组织，为什么组织界面有创建按钮，仅仅只是保存报错，不太能理解。Choerodon平台版本：0.6.0运行环境：线上SaaS环境问题描述：此提示为功能设置，看板可以根据项目需求设定看板列的问题数量约束。当实际问题数量大于最小限制则不能被移出该列，当实际问题数量大于等于最大限制则不能移入新的问题。如需关闭该限制可以在看板右上角的配置中的列约束选择无类型：收到，明白，非常感谢。Choerodon平台版本: 0.5.0遇到问题的执行步骤:
报错日志:
TASK [master : kubeadm | Initialize first master] *********************************************************************************************************************************
Wednesday 20 June 2018  13:02:27 +0800 (0:00:00.847)       0:01:30.387 ********
fatal: [localhost]: FAILED! => {“changed”: true, “cmd”: [“kubeadm”, “init”, “–config=/etc/kubernetes/kubeadm-config.yaml”, “–skip-preflight-checks”], “delta”: “0:30:08.111451”, “end”: “2018-06-20 13:32:35.791113”, “failed_when_result”: true, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-06-20 13:02:27.679662”, “stderr”: “couldn’t initialize a Kubernetes cluster”, “stderr_lines”: [“couldn’t initialize a Kubernetes cluster”], “stdout”: “[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.\n[init] Using Kubernetes version: v1.8.5\n[init] Using Authorization modes: [Node RBAC]\n[preflight] Skipping pre-flight checks\n[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)\n[certificates] Using the existing ca certificate and key.\n[certificates] Using the existing apiserver certificate and key.\n[certificates] Using the existing apiserver-kubelet-client certificate and key.\n[certificates] Generated sa key and public key.\n[certificates] Using the existing front-proxy-ca certificate and key.\n[certificates] Using the existing front-proxy-client certificate and key.\n[certificates] Valid certificates and keys now exist in “/etc/kubernetes/pki”\n[kubeconfig] Wrote KubeConfig file to disk: “admin.conf”\n[kubeconfig] Wrote KubeConfig file to disk: “kubelet.conf”\n[kubeconfig] Wrote KubeConfig file to disk: “controller-manager.conf”\n[kubeconfig] Wrote KubeConfig file to disk: “scheduler.conf”\n[controlplane] Wrote Static Pod manifest for component kube-apiserver to “/etc/kubernetes/manifests/kube-apiserver.yaml”\n[controlplane] Wrote Static Pod manifest for component kube-controller-manager to “/etc/kubernetes/manifests/kube-controller-manager.yaml”\n[controlplane] Wrote Static Pod manifest for component kube-scheduler to “/etc/kubernetes/manifests/kube-scheduler.yaml”\n[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory “/etc/kubernetes/manifests”\n[init] This often takes around a minute; or longer if the control plane images have to be pulled.\n\nUnfortunately, an error has occurred:\n\ttimed out waiting for the condition\n\nThis error is likely caused by that:\n\t- The kubelet is not running\n\t- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)\n\t- There is no internet connection; so the kubelet can’t pull the following control plane images:\n\t\t- registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kube-apiserver-amd64:v1.8.5\n\t\t- registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kube-controller-manager-amd64:v1.8.5\n\t\t- registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kube-scheduler-amd64:v1.8.5\n\nYou can troubleshoot this for example with the following commands if you’re on a systemd-powered system:\n\t- ‘systemctl status kubelet’\n\t- ‘journalctl -xeu kubelet’”, “stdout_lines”: ["[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.", “[init] Using Kubernetes version: v1.8.5”, “[init] Using Authorization modes: [Node RBAC]”, “[preflight] Skipping pre-flight checks”, “[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)”, “[certificates] Using the existing ca certificate and key.”, “[certificates] Using the existing apiserver certificate and key.”, “[certificates] Using the existing apiserver-kubelet-client certificate and key.”, “[certificates] Generated sa key and public key.”, “[certificates] Using the existing front-proxy-ca certificate and key.”, “[certificates] Using the existing front-proxy-client certificate and key.”, “[certificates] Valid certificates and keys now exist in “/etc/kubernetes/pki””, “[kubeconfig] Wrote KubeConfig file to disk: “admin.conf””, “[kubeconfig] Wrote KubeConfig file to disk: “kubelet.conf””, “[kubeconfig] Wrote KubeConfig file to disk: “controller-manager.conf””, “[kubeconfig] Wrote KubeConfig file to disk: “scheduler.conf””, “[controlplane] Wrote Static Pod manifest for component kube-apiserver to “/etc/kubernetes/manifests/kube-apiserver.yaml””, “[controlplane] Wrote Static Pod manifest for component kube-controller-manager to “/etc/kubernetes/manifests/kube-controller-manager.yaml””, “[controlplane] Wrote Static Pod manifest for component kube-scheduler to “/etc/kubernetes/manifests/kube-scheduler.yaml””, “[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory “/etc/kubernetes/manifests””, “[init] This often takes around a minute; or longer if the control plane images have to be pulled.”, “”, “Unfortunately, an error has occurred:”, “\ttimed out waiting for the condition”, “”, “This error is likely caused by that:”, “\t- The kubelet is not running”, “\t- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)”, “\t- There is no internet connection; so the kubelet can’t pull the following control plane images:”, “\t\t- registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kube-apiserver-amd64:v1.8.5”, “\t\t- registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kube-controller-manager-amd64:v1.8.5”, “\t\t- registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kube-scheduler-amd64:v1.8.5”, “”, “You can troubleshoot this for example with the following commands if you’re on a systemd-powered system:”, “\t- ‘systemctl status kubelet’”, “\t- ‘journalctl -xeu kubelet’”]}疑问:
要如何解决啊，有没有官方的群可以加一下，提升下沟通效率请执行然后重新安装。试过了，还是卡在一样的地方请执行下面命令后贴一下日志Jun 20 15:41:34 localhost kubelet[5155]: E0620 15:41:34.949754    5155 kubelet_node_status.go:107] Unable to register node “localhost” with API server: Post https://192.168.3.78:6
Jun 20 15:41:34 localhost kubelet[5155]: E0620 15:41:34.992142    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:35 localhost kubelet[5155]: E0620 15:41:35.127514    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:35 localhost kubelet[5155]: E0620 15:41:35.751227    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:35 localhost kubelet[5155]: E0620 15:41:35.992927    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:36 localhost kubelet[5155]: E0620 15:41:36.128235    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:36 localhost kubelet[5155]: W0620 15:41:36.298412    5155 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Jun 20 15:41:36 localhost kubelet[5155]: E0620 15:41:36.298550    5155 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady messag
Jun 20 15:41:36 localhost kubelet[5155]: E0620 15:41:36.752093    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:36 localhost kubelet[5155]: E0620 15:41:36.993667    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:37 localhost kubelet[5155]: E0620 15:41:37.128922    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:37 localhost kubelet[5155]: W0620 15:41:37.724310    5155 status_manager.go:431] Failed to get status for pod "kube-apiserver-localhost_kube-system(3d80f271e2fc36c7de
Jun 20 15:41:37 localhost kubelet[5155]: W0620 15:41:37.724646    5155 status_manager.go:431] Failed to get status for pod "kube-scheduler-localhost_kube-system(37592247b4fbf8bd02
Jun 20 15:41:37 localhost kubelet[5155]: W0620 15:41:37.725022    5155 status_manager.go:431] Failed to get status for pod "kube-apiserver-localhost_kube-system(1d927e96a1608df9b0
Jun 20 15:41:37 localhost kubelet[5155]: W0620 15:41:37.725298    5155 status_manager.go:431] Failed to get status for pod "kube-controller-manager-localhost_kube-system(01c23873f
Jun 20 15:41:37 localhost kubelet[5155]: W0620 15:41:37.725602    5155 status_manager.go:431] Failed to get status for pod "kube-controller-manager-localhost_kube-system(3d12815d9
Jun 20 15:41:37 localhost kubelet[5155]: E0620 15:41:37.752826    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:37 localhost kubelet[5155]: E0620 15:41:37.994380    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:38 localhost kubelet[5155]: E0620 15:41:38.129647    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:38 localhost kubelet[5155]: E0620 15:41:38.278640    5155 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node “localhost” not fou
Jun 20 15:41:38 localhost kubelet[5155]: E0620 15:41:38.753687    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:38 localhost kubelet[5155]: E0620 15:41:38.995210    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:39 localhost kubelet[5155]: E0620 15:41:39.130318    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:39 localhost kubelet[5155]: E0620 15:41:39.754566    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:39 localhost kubelet[5155]: E0620 15:41:39.996076    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:40 localhost kubelet[5155]: E0620 15:41:40.130990    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:40 localhost kubelet[5155]: E0620 15:41:40.755263    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:40 localhost kubelet[5155]: E0620 15:41:40.997070    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:41 localhost kubelet[5155]: E0620 15:41:41.131696    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:41 localhost kubelet[5155]: E0620 15:41:41.192042    5155 event.go:209] Unable to write event: 'Patch https://192.168.3.78:6443/api/v1/namespaces/kube-system/events/k
Jun 20 15:41:41 localhost kubelet[5155]: W0620 15:41:41.300019    5155 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Jun 20 15:41:41 localhost kubelet[5155]: E0620 15:41:41.300188    5155 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady messag
Jun 20 15:41:41 localhost kubelet[5155]: E0620 15:41:41.756143    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:41 localhost kubelet[5155]: I0620 15:41:41.950002    5155 kubelet_node_status.go:280] Setting node annotation to enable volume controller attach/detach
Jun 20 15:41:41 localhost kubelet[5155]: I0620 15:41:41.952617    5155 kubelet_node_status.go:83] Attempting to register node localhost
Jun 20 15:41:41 localhost kubelet[5155]: E0620 15:41:41.953067    5155 kubelet_node_status.go:107] Unable to register node “localhost” with API server: Post https://192.168.3.78:6
Jun 20 15:41:41 localhost kubelet[5155]: E0620 15:41:41.997863    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:42 localhost kubelet[5155]: E0620 15:41:42.132807    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:42 localhost kubelet[5155]: E0620 15:41:42.756814    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:42 localhost kubelet[5155]: E0620 15:41:42.998716    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:43 localhost kubelet[5155]: E0620 15:41:43.133682    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:43 localhost kubelet[5155]: I0620 15:41:43.723943    5155 kubelet_node_status.go:280] Setting node annotation to enable volume controller attach/detach
Jun 20 15:41:43 localhost kubelet[5155]: E0620 15:41:43.726786    5155 kubelet.go:1612] Failed creating a mirror pod for "etcd-localhost_kube-system(d76e26fba3bf2bfd215eb29011d552
Jun 20 15:41:43 localhost kubelet[5155]: E0620 15:41:43.757927    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:43 localhost kubelet[5155]: E0620 15:41:43.999413    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:44 localhost kubelet[5155]: I0620 15:41:44.027152    5155 kuberuntime_manager.go:500] Container {Name:etcd Image:gcr.io/google_containers/etcd-amd64:3.0.17 Command:[e
Jun 20 15:41:44 localhost kubelet[5155]: E0620 15:41:44.028475    5155 pod_workers.go:182] Error syncing pod d76e26fba3bf2bfd215eb29011d55250 ("etcd-localhost_kube-system(d76e26fb
Jun 20 15:41:44 localhost kubelet[5155]: E0620 15:41:44.134461    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:44 localhost kubelet[5155]: E0620 15:41:44.758804    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://
Jun 20 15:41:45 localhost kubelet[5155]: E0620 15:41:45.000202    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168
Jun 20 15:41:45 localhost kubelet[5155]: E0620 15:41:45.135216    5155 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.
Jun 20 15:41:45 localhost kubelet[5155]: I0620 15:41:45.723988    5155 kubelet_node_status.go:280] Setting node annotation to enable volume controller attach/detach
Jun 20 15:41:45 localhost kubelet[5155]: E0620 15:41:45.726974    5155 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-localhost_kube-system(c993f46fb8e
lines 1-54请在安装k8s的主机上执行下以下docker命令  看看什么反应Choerodon平台版本: v0.6遇到问题的执行步骤:这个地方的--name mysql-pv是不是写错了。文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/monitoring/#安装监控组件是的 感谢你指出问题的这里名字可以是任意名字，但为了方便管理 这里叫monitoring-pv 即可Choerodon平台版本: 0.5.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):报错日志:
提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问
请重新执行安装命令，按提示执行删除操作后再进行一键安装，下面命令你需要执行3遍第一遍会提示你删除helm release
第二遍会提示你删除job
第三遍才会进行正式安装Choerodon平台版本: 0.5.0遇到问题的执行步骤:文档地址:环境信息(如:节点信息):
[root@node1 u01]# kubectl get pods -n choerodon-devops-prod
NAME                                                     READY     STATUS    RESTARTS   AGE
choerodon-chartmuseum-chartmuseum-559f7f8c89-2gkw2       1/1       Running   0          6h
choerodon-config-server-544df4dc65-7clk5                 1/1       Running   0          1h
choerodon-devops-redis-64bcd4db76-sh9fw                  1/1       Running   0          6h
choerodon-gitlab-85444f77dc-khxlm                        1/1       Running   0          4h
choerodon-gitlab-redis-6756dddf8d-8b9lt                  1/1       Running   0          6h
choerodon-harbor-harbor-adminserver-0                    1/1       Running   0          6h
choerodon-harbor-harbor-clair-68ddd76d5-cp5xh            1/1       Running   1          6h
choerodon-harbor-harbor-jobservice-749947d84-55cjn       1/1       Running   0          6h
choerodon-harbor-harbor-mysql-0                          1/1       Running   0          6h
choerodon-harbor-harbor-notary-db-0                      1/1       Running   0          6h
choerodon-harbor-harbor-notary-server-8497d57c44-4444w   1/1       Running   6          6h
choerodon-harbor-harbor-notary-signer-5c45d448b-fs62k    1/1       Running   0          6h
choerodon-harbor-harbor-registry-0                       1/1       Running   0          6h
choerodon-harbor-harbor-ui-74684c76b4-4c8d6              1/1       Running   0          6h
choerodon-harbor-postgresql-6cd79cdfcb-qn4sl             1/1       Running   0          6h
choerodon-kafka-0                                        1/1       Running   0          7h
choerodon-kafka-1                                        1/1       Running   0          6h
choerodon-kafka-2                                        1/1       Running   0          6h
choerodon-minio-684c84c6fc-pnslt                         1/1       Running   0          7h
choerodon-mysql-dcb9b88cd-mnrzh                          1/1       Running   0          7h
choerodon-register-server-7cb84dfdb5-ssbrt               1/1       Running   0          6h
choerodon-zookeeper-0                                    1/1       Running   0          7h
choerodon-zookeeper-1                                    1/1       Running   0          7h
choerodon-zookeeper-2                                    1/1       Running   0          7h报错日志:
curl -s $(kubectl get po -n choerodon-devops-prod -l choerodon.io/release=config-server -o jsonpath="{.items[0].status.podIP}"):8011/health | jq -r .status
error: error executing jsonpath “{.items[0].status.podIP}”: array index out of bounds: index 0, length 0Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found
? choerodon-manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found
? choerodon-manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found
? choerodon-manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found
? choerodon-manager-service not ready,sleep 5s,check it.
Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好，请问你安装完成后使用上是遇到什么异常了吗？分布部署文档中提供的检查命令是针对分布部署而写的，查看你提供的信息，你安装是使用一键部署脚本部署的，所以在检查时需要对检查命令做出修改，替换choerodon.io/release标签的值为一键部署时的值，默认情况下一键部署的所有服务都会比分布部署的choerodon.io/release标签的值多choerodon-前缀。正确检查命令如下：你好：
我是使用一键部署的，脚本执行过程中，出现了 Error from server (NotFound): deployments.extensions “choerodon-manager-service” not found 错误，我又通过分步部署里面的脚本建立manager-service ，提示出错了
检查了下POD的状态是RUNNING的，请问问题出在什么地方了吗？
Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：两个账号A,B，都授予了组织管理员的权限。
A创建了项目X，在项目X下创建应用，选择模板创建应用等操作没有问题。
B在项目X下选择模板创建应用，会一直处于创建中的状态修改的数据：报错信息(请尽量使用代码块的形式展现)：原因分析：在gitlab上账号B不是项目X这个group的成员。这种情况下，它连源代码库都没有创建。在gitlab上把账号目加入项目X这个group后，源代码库可以创建，但是指定模板的代码没有拉去过来；但是模板是public的对于猪齿鱼平台用账号操作gitlab这方面，看起来存在一些问题，你们有没有什么最佳实践？您好，首先，B在我们平台上不属于项目X的成员，故项目层次的操作都无法进行。您可以给B在项目X下分配项目所有者/项目管理员（该角色需分配gitlab.owner标签），这样平台会自动同步至gitlab将B加入项目X的group，无需您手动去操作gitlab。好的，明白了，谢谢Linux（Centos）服务器通过docker-compose.yml部署启动相关基础组件如上，可正常启动。本地idea运行案例，相关配置如下启动运行demo，报错如下：你的配置看起来缺少了zk 的配置，可以在配置文件中添加zk的配置demo程序并不需要连接kafka，麻烦提供下你看到的文档地址，我们这边修改一下http://choerodon.io/zh/docs/development-guide/backend/demo/create_project/#添加默认配置文件我的基本上采用的是这个http://choerodon.io/zh/docs/quick-start/microservice-backend/
你说的这个我之前也弄过，但是里面代码太杂了，有些都不通，所以就没采取了。怎么修改哈，给贴出来哈:joy:还有一个问题就是，访问这个swagger，认证的时候跳转报错
swagger点击接口旁边红点登录。docker-compose开发不方便，要是你要调试啥的最好起服务源码，我们本地也是启动的源码。文档正在修改。关于zookeeper，kafka，mysql如下：(⊙o⊙)…，我看你这个跟我的没多少差异呀，就改了下kafaka-0的hostname，还有通过接口的那个认证也是一样的，跳转一样的问题oauth地址要通过manager的环境变量CHOERODON_SWAGGER_OAUTH_URL配置。你可以先试下我这个docker，因为我本地一直就是用这个，zookeeper没问题我这不是本地起，而是把这些基础镜像在centos服务器上运行起来，然后我本地demo案例连接哦，你还弄了个混合开发啊，拿你本地ping的通172.20.10.10？你centos防火墙允许9092了没？不是这些网络原因造成的吧？应该不是网络原因吧，我ping过，可以通的哈。。。不是防火墙9092端口没过吧。刚才本地试下了，我这是可以连上没问题的，我们现在zookeeper和kafka都是docker-compose运行的，docker-compose文件就是我上边贴的那个@fuchen 如果你的docker地址不是localhost,则需要将kafka和zookeeper的hostname修改为对应的地址如：这个zk的由zookeeper-0改成服务器ip地址后，docker-compose运行时都起不来了 
是否端口被占用呢，检查下相关的端口。virtualbox单启的centos服务器，防火墙我都直接给他禁掉了有日志吗，查看下日志。demo案例连接时，我查看了下eureka-server的日志，如上：
demo案例运行的控制台日志如下：oauth地址要通过manager的环境变量 CHOERODON_SWAGGER_OAUTH_URL 配置。你可以先试下我这个docker，因为我本地一直就是用这个，zookeeper没问题你说的这个swagger这个认证，需要怎么配置这个环境变量哈，具体给一下哈@fuchen  因为你是在服务器上启动的环境，kafka如果绑定到localhost则本机无法访问，改为服务器的ip之后则需要同步修改服务器上的所有服务kafka地址，这样比较麻烦。
另外有一种方案：使用原来的docker-compose文件，修改本机的 hosts文件添加如下内容 192.168.99.100换成你的服务器地址，本地demo使用 kafka-0,zookeeper-0配置连接。欢迎大家在猪齿鱼论坛上提交Cloopm云运维平台使用、系统功能等相关的问题和建议。欢迎大家在论坛上提交Cloopm云运维平台相关的问题和建议Choerodon平台版本：0.6.0运行环境：公司提供问题描述：搭建kubernetes集群时，如果集群是一台主节点和多台从节点，只有主节点上安装etcd服务，这样无法在猪齿云上激活。如果是所有主机都安装主节点、从节点和etcd等服务就可以激活，请问这是为什么？@xinghao 您好，多个问题请分别提问。这样有利于我们分类和及时回复。标题尽可能反映问题。提交前注意选择合适的分类。请问是不是一个kubernetes集群只能在猪齿云上激活一次？
可不可以在一个k8s集群上激活多个环境，比如开发、测试等。一个集群是可以安装多个环境客户端的。但是为什么我无法激活多个，执行后集群那些pod已经启动了，但是显示未连接，如上述的staging。
我在另一个集群中试了一下，可以正常激活，如上述的prod
同时还请帮忙解答下最上面的那个问题，是不是kubernetes集群只能使用那种不分主从的，就是一台主机上同时安装主从服务。从你的截图中，我并没有看到你在这个集群执行安装staging 环境客户端的命令，namespace都没有创建，
staging只是一个名称，它的编码是front-test执行命令为
if ! [ -x “$(command -v kubectl)” ]; then
echo ‘Error: kubectl is not installed.’ >&2
exit 1
fi
if ! [ -x “$(command -v helm)” ]; then
echo ‘Error: helm is not installed.’ >&2
exit 1
fi
kubectl create namespace prod
helm install --repo=http://chart.choerodon.com.cn/choerodon/framework/ 
–namespace=prod 
–name=prod 
–version=0.6.0 
–set config.connect=ws://devops.service.choerodon.com.cn/agent/ 
–set config.token=72dcee37-34b5-4a78-8693-bc355fcacffb 
–set config.envId=47 
–set rbac.create=true 
choerodon-agent你贴一下这个这个prod 客户端的日志。[root@master backend]# kubectl logs prod-d497bb58d-77v4m -n prod
I0620 14:34:44.170965       1 controller.go:81] Started “configmap”
I0620 14:34:44.171068       1 controller.go:81] Started “replicaset”
I0620 14:34:44.171129       1 controller.go:81] Started “pod”
I0620 14:34:44.171216       1 controller.go:81] Started “endpoint”
I0620 14:34:44.171237       1 controller.go:81] Started “service”
I0620 14:34:44.171261       1 controller.go:81] Started “secret”
I0620 14:34:44.171294       1 controller.go:81] Started “ingress”
I0620 14:34:44.171321       1 controller.go:81] Started “deployment”
I0620 14:34:44.171352       1 controller.go:81] Started “job”
I0620 14:34:44.171395       1 client.go:51] Started agent
I0620 14:34:44.171939       1 configmap_controller.go:68] Starting Pod controller
I0620 14:34:44.171949       1 configmap_controller.go:71] Waiting for informer caches to sync
I0620 14:34:44.171985       1 replicaset_controller.go:67] Starting Pod controller
I0620 14:34:44.171989       1 replicaset_controller.go:70] Waiting for informer caches to sync
I0620 14:34:44.172376       1 pod_controller.go:69] Starting Pod controller
I0620 14:34:44.172395       1 pod_controller.go:72] Waiting for informer caches to sync
I0620 14:34:44.172481       1 endpoints_controller.go:147] Starting endpoint controller
I0620 14:34:44.172499       1 controller_utils.go:1019] Waiting for caches to sync for endpoint controller
I0620 14:34:44.172545       1 service_controller.go:68] Starting Pod controller
I0620 14:34:44.172549       1 service_controller.go:71] Waiting for informer caches to sync
I0620 14:34:44.172595       1 secret_controller.go:68] Starting Pod controller
I0620 14:34:44.172599       1 secret_controller.go:71] Waiting for informer caches to sync
I0620 14:34:44.172621       1 ingress_controller.go:68] Starting Pod controller
I0620 14:34:44.172626       1 ingress_controller.go:71] Waiting for informer caches to sync
I0620 14:34:44.172657       1 deployment_controller.go:68] Starting Pod controller
I0620 14:34:44.172663       1 deployment_controller.go:71] Waiting for informer caches to sync
I0620 14:34:44.172713       1 job_controller.go:72] Starting Pod controller
I0620 14:34:44.172718       1 job_controller.go:75] Waiting for informer caches to sync
I0620 14:34:44.272208       1 configmap_controller.go:109] Started workers
I0620 14:34:44.272490       1 pod_controller.go:111] Started workers
I0620 14:34:44.272491       1 replicaset_controller.go:108] Started workers
I0620 14:34:44.272632       1 controller_utils.go:1026] Caches are synced for endpoint controller
I0620 14:34:44.272653       1 secret_controller.go:109] Started workers
I0620 14:34:44.272698       1 service_controller.go:109] Started workers
I0620 14:34:44.272766       1 job_controller.go:113] Started workers
I0620 14:34:44.272798       1 ingress_controller.go:109] Started workers
I0620 14:34:44.272831       1 deployment_controller.go:109] Started workers
E0620 14:35:24.178329       1 client.go:68] dial error ws://devops.service.choerodon.com.cn/agent/?version=0.6.0&envId=47&key=env:prod.envId:47: dial tcp: lookup devops.service.choerodon.com.cn on 10.233.0.10:53: read udp 10.233.66.117:60148->10.233.0.10:53: i/o timeout
E0620 14:36:09.181156       1 client.go:68] dial error ws://devops.service.choerodon.com.cn/agent/?version=0.6.0&envId=47&key=env:prod.envId:47: dial tcp: lookup devops.service.choerodon.com.cn on 10.233.0.10:53: read udp 10.233.66.117:47389->10.233.0.10:53: i/o timeout
E0620 14:36:54.182931       1 client.go:68] dial error ws://devops.service.choerodon.com.cn/agent/?version=0.6.0&envId=47&key=env:prod.envId:47: dial tcp: lookup devops.service.choerodon.com.cn on 10.233.0.10:53: read udp 10.233.66.117:54618->10.233.0.10:53: i/o timeout
E0620 14:37:39.185208       1 client.go:68] dial error ws://devops.service.choerodon.com.cn/agent/?version=0.6.0&envId=47&key=env:prod.envId:47: dial tcp: lookup devops.service.choerodon.com.cn on 10.233.0.10:53: read udp 10.233.66.117:37344->10.233.0.10:53: i/o timeout
E0620 14:38:24.187057       1 client.go:68] dial error ws://devops.service.choerodon.com.cn/agent/?version=0.6.0&envId=47&key=env:prod.envId:47: dial tcp: lookup devops.service.choerodon.com.cn on 10.233.0.10:53: read udp 10.233.66.117:36292->10.233.0.10:53: i/o timeout
E0620 14:39:09.188671       1 client.go:68] dial error ws://devops.service.choerodon.com.cn/agent/?version=0.6.0&envId=47&key=env:prod.envId:47: dial tcp: lookup devops.service.choerodon.com.cn on 10.233.0.10:53: read udp 10.233.66.117:40565->10.233.0.10:53: i/o timeout
E0620 14:39:54.190649       1 client.go:68] dial error ws://devops.service.choerodon.com.cn/agent/?version=0.6.0&envId=47&key=env:prod.envId:47: dial tcp: lookup devops.service.choerodon.com.cn on 10.233.0.10:53: read udp 10.233.66.117:35678->10.233.0.10:53: i/o timeout没有连接成功，你看下，该容器中能ping 通 [devops.service.choerodon.com.cn]这个地址吗连接上的可以ping通，连接不上的是ping不通的。我发现如果这个pod在运行在一个节点上可以激活，在另一个节点上就无法激活。这是为什么呢？节点都是一样的，安装的主、从和etcd服务。
这应该是您的集群网络存在问题。所有节点是否都能访问外网或者是否有修改某个节点的iptables?所有节点都可以访问外网，修改iptables是指什么？防火墙吗？我使用的是centos7。firewalld防火墙已经关闭。执行 kubectl get po -n kube-system -o wide 查看下各组件的状态。只有node1无法激活使用下面命令尝试重启flannel网络再试一下是否能ping通。可以了，谢谢了。为什么flannel启动了还会出现这种问题？flannel在某些机器上偶尔会出现问题，可能是网络，资源等造成的。RT我们的情况就是这样。本身自建了一个gitlab，并且上面已经建了几十个项目，那猪齿鱼平台如果配置使用这个外部gitlab，现有的项目信息是不是都拿不到。这种情况最好是，在猪齿鱼平台上新建对应应用并把项目代码推到新库上。所以得推送几十个。包括每个项目还有好多分支:rofl:git push -u origin --all
git push -u origin --tagsChoerodon平台版本: 0.6.0遇到问题的执行步骤:ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml执行了这个命令后，卡在了下面的这个节点这。。。。。TASK [node : Join to cluster if needed] *******************************************************************************************************************************************
Tuesday 19 June 2018  17:21:03 +0800 (0:00:00.740)       0:01:06.820 **********过了一个小时也没啥反应。。。求解答你取消掉重新执行命令搭建哈试过了，还是卡在这。。请执行然后重新安装。 @akkyy12Harbor 集成了Clair进行漏洞扫描，当扫描到漏洞时可以在仓库下查看具体漏洞信息，根据漏洞信息安装相应的安全更新。Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：部署与配置文档中的这些组件 是不是都要自己事先在linux里装好的啊？然后再进行安装步骤里面的东西？有没有什么小白安装指南分享下？多谢@akkyy12  文档里面都包含了这些组件的安装, 按照文档顺序进行安装即可。好的谢谢～～～～～Choerodon平台版本: 0.6.0遇到问题的执行步骤:
分步部署devops-service没有配置
文档地址:http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问由于文档的错误给你带来不必要的麻烦，敬请谅解。我们将尽快将文档修改正确，你可以参考下面命令将devops service进行更新，谢谢。Choerodon.com.cn使用：
在待办事项页面中的任务输入描述时，如果鼠标点击到任意其他任务，描述不会置为空。不知是Bug还是就是这么设计的？
为bug，已记录，会在下个版本进行修复在试用过程中，创建完项目，与环境建立链接时，提示拷贝脚本到k8s上，而部署完k8s后，好像dashboard并不能访问请把问题描述清楚一些，例如，1.你的k8s环境是是否能够和Choerodon互联互通；2.你有没有在k8s环境中安装choerodon-agent，即在k8s上执行脚本等。通过https://master-ip:6443/ui 报
如何方便修改apiserver的配置，可以在外网无权限访问k8s dashboard我下午通过修改/etc/kubernetes/manifests/kube-apiserver.yaml,  添加-- anonymous -auth=false，修改完后，系统自动把apiserver的容器删掉了。。后面还没有解决。我们部署中并没用包含dashboard。看部署k8s的脚本里面有安装dashboard@qianyiliushang @yifanyou 使用Choerodon文档中提供的K8S部署默认是部署了dashboard的，如果要访问，可以自行创建ingress进行绑定svc，在kube-system命名空间有个名为kubernetes-dashboard的svc，绑定它后进行访问即可。能够看下您ci文件是怎么写的吗？上面那个问题已经解决了，但是部署的时候出现了问题。
@xinghao 您的chart名称和您的应用名称是一样的吗？在组织下新建项目之后，会自动在harbor仓库新建一个组织-项目（ams-hitom）的项目，请问这个项目怎么使用，通过什么账号呢？直接使用猪齿云的账号/密码好像使用不了@xinghao  harbor仓库密码为搭建harbor时指定的密码,参见文档
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/harbor/我使用的是猪齿云下的一个子账号，原始harbor密码是不清楚的。而且文档上面写的是admin账号的密码，我也不能使用这个账号。抱歉，不能给你提供我们harbor的密码，但您可以在项目中使用ci推送镜像。这个需要怎么做呢？chart名称和项目名称并不一样，这样有什么影响吗？我是在项目下创建应用httpd，然后部署应用。项目名称是hitom。这个会影响部署吗？你在choerodon界面上创建的应用code、代码中   Chart.yaml   name属性、文件目录  nginx-demo都必须一致，不能随意修改。在Gitlab上看你创建的仓库名为 httpd，故应以此为准三个地方都应是httpd，请修改后重试。好的，已经可以了。感谢
请问是不是只要需要自动部署都需要有chart这个文件夹？在项目中使用ci推送镜像，具体怎么做呢？不需要在文件中写这个吗？
对  必须要有 @xinghao在项目中使用ci推送镜像，具体就是你截图的这样操作Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：去服务器上执行脚本，如果不是运维人员， 感觉不太方便。或者就是每次都要叫运维去执行@yifanyou 请问可持续环境具体指什么？或者就是每次都要叫运维去执行不是必须要在服务起上执行脚本，你本地装个kubectl 和helm客户端，配置好kube config也能装。不好意思，就是应用部署环境所以其实还是运维人员做这个比较好。理想情况下 创建环境的时候，猪齿鱼就直接把脚本执行了创建环境时候，choerodon平台是无法连接到新环境上的，所无法在创建环境时执行脚本。必须在新集群上执行脚本建立连接。标准的这些表赶紧重构吧。参考下HAP吧，或者照EBS来也行。HAP刚开始搞的时候就没有tag，parent_id这些，现在这套标准表也没有，不知道这几个标准表对应的服务是不是已经有搞好。今天看lookup_value里面只有个description，meaning没有，tag没有，排序也没有。。。每搞一个框架表结构设计上就要造一遍轮子，真是心累。。。这个是我们框架服务的一些探索目前还不会做多语言和lookup，各个服务之间也无法共享lookup，微服务之间也不能直接用EBS的一些设计同学你好，请教下你说的tag指的是什么tag是用来标记记录，常用于归类本地环境系统为Linux，首先docker-compose启动相关组件容器，此处正常，然后idea中启动demo案例代码，此处基本正常，访问http://localhost:8963/swagger-ui.html通过swagger方式查看demo相关服务及api，如下：
上述的问题点应该在于网关api-gateway没正确调用到demo案例的服务资源接口，麻烦相关研发人员给瞅瞅，看看我这里是哪有问题，如果是配置的问题，请正确贴出来哈，谢谢！看起来好像并没有什么问题，postman 直接测接口有没有返回？可以看下docker logs看下api-gateway的日志，应该是api-gateway没有配置这个demo服务的路由造成的，可以在docker-compose修改api-gateway添加环境变量设置一下你的demo服务的路由，添加规则可以参照下面：postman测试的话，采用basic auth认证，提示jwt token的问题。。。postman测试需要添加Authorization头部，token 的值可以在swagger调接口时拿到因此api-gateway添加环境变量zuul.routes.test.path=xxx和zuul.routes.test.serviceId=xxx，试下ok，好的，我试试哈docker-compose.yml添加了配置，如下：
试了下，结果还是不行。。。跟demo案例中的这个有没有关系？
这个路由本地不会生效，只有线上通过config-server时会生效。去除运行主类的@EnableChoerodonResourceServer，直接调用test服务接口能通不？能通的话docker logs看api-gateway和gateway-helper日志去掉后，还是通过swagger调，api-gateway无相关日志，gateway-helper有，如下：gateway-helper | 2018-06-14 14:32:03.706 INFO [gateway-helper,4de96db462f2cce6,4de96db462f2cce6,false] 1 --- [ XNIO-3 task-5] i.c.g.h.p.RequestPermissionFilter : error.permissionVerifier.permission, can't find request service route, request uri /test/v1/user, zuulRoutes {event=ZuulProperties.ZuulRoute(id=null, path=/event/**, serviceId=event-store-service, url=null, stripPrefix=true, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=false), devops=ZuulProperties.ZuulRoute(id=null, path=/devops/**, serviceId=devops-service, url=null, stripPrefix=true, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=false), iam=ZuulProperties.ZuulRoute(id=null, path=/iam/**, serviceId=iam-service, url=null, stripPrefix=true, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=false), oauth=ZuulProperties.ZuulRoute(id=null, path=/oauth/**, serviceId=oauth-server, url=null, stripPrefix=false, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=true), notify=ZuulProperties.ZuulRoute(id=null, path=/notify/**, serviceId=notification-service, url=null, stripPrefix=true, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=false), manager=ZuulProperties.ZuulRoute(id=null, path=/manager/**, serviceId=manager-service, url=null, stripPrefix=true, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=false), file=ZuulProperties.ZuulRoute(id=null, path=/file/**, serviceId=file-service, url=null, stripPrefix=true, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=false), org=ZuulProperties.ZuulRoute(id=null, path=/org/**, serviceId=organization-service, url=null, stripPrefix=true, retryable=null, sensitiveHeaders=[], customSensitiveHeaders=false)}通过postman，直接访问，是可以的
那个swagger的问题咋个解决哈。。。不过如果采用直接访问的话，那请求就不会经过api-gateway和gateway-helper这边了，这样的话。。。所以说swagger这个问题还是得解决哈，不然集成这玩意有啥用呀。。。找到原因了，文档的demo案例代码中确实一个注解，如下图：
你们这个文档，编写的人员要用点心呀，整出来的东西，缺胳膊少腿的，简直是坑死人不偿命是吧:joy:不好意思，微服务开发后端文档最好参照开发手册-后端开发手册-开发demo程序的例子，快速入门代码文档不全，文档会尽快更新，谢谢🙏请提供一下你搭建Harbor的全部命令，谢谢helm install c7n/harbor 
–set persistence.enabled=true 
–set externalDomain=registry.hand-ams.com 
–set adminserver.adminPassword=Harbor12345 
–set adminserver.volumes.config.selector.pv="harbor-adminserver-pv" 
–set mysql.volumes.config.selector.pv="harbor-mysql-pv" 
–set registry.volumes.config.selector.pv="harbor-registry-pv" 
–set notary.db.volumes.data.selector.pv="harbor-notary-pv" 
–set postgresql.persistence.enabled=true 
–set postgresql.persistence.existingClaim="harbor-postgresql-pvc" 
–set insecureRegistry=true 
–name=harbor --namespace=choerodon-devops-prod你好，我是通过helm来安装harbor，这是创建harbor的命令helm install c7n/create-pv 
–set type=nfs 
–set pv.name=harbor-adminserver-pv 
–set nfs.path=/data/io-choerodon/harbor-adminserver 
–set nfs.server=192.168.1.102 
–set pvc.enable=false 
–set size=5Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name harbor-adminserver-pv --namespace=choerodon-devops-prodhelm install c7n/create-pv 
–set type=nfs 
–set pv.name=harbor-mysql-pv 
–set nfs.path=/data/io-choerodon/harbor-mysql 
–set nfs.server=192.168.1.102 
–set pvc.enable=false 
–set size=5Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name harbor-mysql-pv --namespace=choerodon-devops-prodhelm install c7n/create-pv 
–set type=nfs 
–set pv.name=harbor-registry-pv 
–set nfs.path=/data/io-choerodon/harbor-registry 
–set nfs.server=192.168.1.102 
–set pvc.enable=false 
–set size=5Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name harbor-registry-pv --namespace=choerodon-devops-prodhelm install c7n/create-pv 
–set type=nfs 
–set pv.name=harbor-notary-pv 
–set nfs.path=/data/io-choerodon/harbor-notary 
–set nfs.server=192.168.1.102 
–set pvc.enable=false 
–set size=5Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name harbor-notary-pv --namespace=choerodon-devops-prod
helm install c7n/create-pv 
–set type=nfs 
–set pv.name=harbor-postgresql-pv 
–set nfs.path=/data/io-choerodon/harbor-postgresql 
–set nfs.server=192.168.1.102 
–set pvc.name=harbor-postgresql-pvc 
–set size=1Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name harbor-postgresql-pv --namespace=choerodon-devops-prod
这是创建pv和pvc的命令failed to connect to tcp://harbor-harbor-mysql:3306, retry after 2 seconds :dial tcp: lookup harbor-harbor-mysql: Temporary failure in name resolution
报这个错是因为什么？初步判断应该是Mysql没有启动起来，请执行下以下语句再做排查：这算mysql启动了吗？一般有三个pod无法启动，后面两个日志大概的意思就是无法连接到harbor-adminserver。现在状态所有PVC都已绑定PV成功，建议执行下面语句清空挂载出来的数据，重新启动：
现在adminserver不会重启了，但是还有三个服务会不停重启，下面是日志创建版本缺陷时，全屏编辑确认后，但是没有保存成功现在还有这个问题么？刚刚操作了一下没有发现这个问题，能详细描述一下bug步骤么？本地拉取相关镜像通过docker-compose构建启动
本地demo案例
如上若本地运行，会存在端口冲突，demo案例虽然可以启动，但是服务无法正常注册，修改端口后导致manager-service中swagger方式访问无法正确得到资源数据，如图：
修改下本地demo案例的端口就可以了。swagger配置的gateway的domain默认为127.0.0.1:8080，所以直接改了gateway端口会访问不通，需要修改manager的choerodon.gateway.domain的环境变量，比如127.0.0.1:8090我这是根据这个配置启动的，demo案例资源服务在swagger还是无法调用成功，这个有啥问题吗？还是说我需要更改docker-compose.yml这个配置？，当前如下：需要修改manager的choerodon.gateway.domain的环境变量??这个怎么改？？manager加个environment啊老哥，你能否看下我上面的配置，你那边验证好再发我哈，这个swagger还是无法调通demo的服务资源choerodon.gateway.domain设置的是api-gateway的地址，你要是改了api-gateway的端口需要设置这个；你要是改了demo的端口不需要配置这个按照你所说的，我这边只改demo的server.port地址就行了吧？？但是我改了之后swagger是调不通的api-gateway和gateway-helper都需要配下你的demo服务路由，可以通过环境变量加上。可以看下api-gateway和gateway-helper的日志。本地docker-compose配置不是很方便，而且往往内存不够，我们本地开发直接跑的代码报错如下
Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start embedded container; nested exception is java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘webMetricsFilter’ defined in class path resource [io/micrometer/spring/autoconfigure/web/servlet/ServletMetricsConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [io.micrometer.spring.web.servlet.WebMvcMetricsFilter]: Factory method ‘webMetricsFilter’ threw exception; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘org.springframework.security.oauth2.config.annotation.web.configuration.AuthorizationServerEndpointsConfiguration’: Unsatisfied dependency expressed through field ‘configurers’; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘authorizationServerConfig’ defined in file [D:\project\choerodon\oauth-server\target\classes\io\choerodon\oauth\infra\config\AuthorizationServerConfig.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘authenticationManager’ defined in class path resource [org/springframework/boot/autoconfigure/security/AuthenticationManagerConfiguration.class]: Unsatisfied dependency expressed through method ‘authenticationManager’ parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘org.springframework.security.config.annotation.authentication.configuration.AuthenticationConfiguration’: Unsatisfied dependency expressed through method ‘setGlobalAuthenticationConfigurers’ parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘globalAuthenticationConfig’: Unsatisfied dependency expressed through field ‘choerodonAuthenticationProvider’; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘choerodonAuthenticationProvider’: Unsatisfied dependency expressed through field ‘organizationRepository’; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘organizationRepositoryImpl’ defined in file [D:\project\choerodon\oauth-server\target\classes\io\choerodon\oauth\infra\repository\impl\OrganizationRepositoryImpl.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘organizationMapper’ defined in file [D:\project\choerodon\oauth-server\target\classes\io\choerodon\oauth\infra\mapper\OrganizationMapper.class]: Unsatisfied dependency expressed through bean property ‘sqlSessionFactory’; nested exception is org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name ‘sqlSessionFactory’ defined in class path resource [org/mybatis/spring/boot/autoconfigure/MybatisAutoConfiguration.class]: Unsatisfied dependency expressed through method ‘sqlSessionFactory’ parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name ‘dataSource’ defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Tomcat.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.apache.tomcat.jdbc.pool.DataSource]: Factory method ‘dataSource’ threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Cannot determine embedded database driver class for database type NONE. If you want an embedded database please put a supported one on the classpath. If you have database settings to be loaded from a particular profile you may need to active it (no profiles are currently active).
2018-06-13 21:09:18.509 ERROR [-,] 1388 — [           main] o.s.b.d.LoggingFailureAnalysisReporter   :请问下是源码运行还是镜像运行？看起来好像是没有配置数据库连接这是本地跑的吧？配置文件指定正确的数据库的JDBC，用户名，密码这是本地跑的吧？配置文件指定正确的数据库的JDBC，用户名，密码本地跑的，数据库设置是正确的，
改成这样就可以正常运行我们web容器用的是undertow，所以需要把tomcat排除掉，跟这个应该无关。报错是JDBC的错，请问你有没有修改配置或依赖，我们刚才启动了下0.6.0代码是可以正常启动不会报错的看报错信息是数据库类型为NONE，然后然后你加载下数据库配置，应该是没有配置数据源配置问题。
Choerodon平台版本：0.6.X运行环境(如localhost或k8s)：localhost问题描述：
根据文档的操作步骤，已完成开发新模块的操作并运行成功。
但根据文档：http://choerodon.io/zh/docs/development-guide/front/new-func/new_page/
执行到添加菜单这个步骤，在项目根目录下运行python 脚本报错，具体报错信息如下：报错信息(请尽量使用代码块的形式展现)：
疑问：
请问是否是脚本有问题，还是执行的路径有问题？执行python --version 查看下版本然后执行下pip list 查看下安装的插件您好，python --version执行查看版本为Python 2.7.12
pip list 安装的插件如下图所示：
你好，可以贴一下 /demo/src/app/src/config/Menu.yml 文件吗文档里这里是做为示例，可以用下面的格式我修改一下文档嗯，明白了。执行完后，可以在http://localhost:9090/#/demo/hello看到demo页面。不过文档有一处需要做修改：
新问题：后台表中可以看到新建菜单的记录了，但如果需要体现在界面上，是不是还需要在本地启动别的微服务才可以？这里其实并不需要包含实际的JS文件名，会自动去文件夹下的index.js中获取。demo中关于菜单的permission 实际应该对应到后台服务的api接口上，这里只是提供了一个样例。同时，登录的用户需要有对应permission 权限，才能够访问到菜单。或者该用户为ROOT用户，可以再管理菜单下设置root用户。Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：使用了一下迭代管理，到冲刺发布以后就结束了。之前还想着是需求和feature相关之类的。迭代发布会和release分支关联在一起执行的操作：
如:创建了一个新用户并给用户分配了项目管理员权限，使用该用户登录系统报错信息(请尽量使用代码块的形式展现)：建议：提出您认为不合理的地方，帮助我们优化用户操作这个关联我们正在实现，敬请期待~ Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：按照官方文档无法正确运行前端代码，自己摸索了下，clone choerodon-front-boot 这个项目，npm install 后，用bin目录下的start脚本可以运行，不过只有主页可以看到，其他关键功能页面都是404，不知道如何正确运行代码。你好，最新的文档我们会尽快更新到官网，你可以先参考下这一篇帖子按照这个步骤可以用localhost:9090打开页面，不过会跳转到下面这个地址http://api.choerodon.com.cn/oauth/login想问下是否有提供账号密码，我想进去看下相关功能界面，谢谢。需要自行修改server: 'http://localhost:8080', // 后端接口服务器地址 为你们api服务器的地址 http://api.choerodon.com.cn 是不对外提供的哦Choerodon平台版本: 0.5.0遇到问题的执行步骤:
swagger里面没有agile-service
然后到敏捷管理模块调用连接失败，不知道哪里有问题。。
文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问@yifanyou  需要您手动添加路由。噢，知道了，管理页面有配置路由的地方，谢谢比如一个spring应用，需要用到redis, mq等中间件，猪齿鱼平台也是将其做为一个应用来管理，对吗？是这样的，我们尝试过把中间件当作服务的“资源”来做处理，但是发现可配置性、灵活性会很差，所以现在我们也把中间件作为应用进行管理Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：如果我要自己新增一个前端应用模板，文档的目录结构是否要和平台下载的前端应用模板保持一致，如果不需要，请问哪些配置文件以及文件夹和路径是必须要保留的，是不是一定要有boot目录来作为存放编译后的文件用于部署前端应用。
另外自己开发的前端应用模板可以不使用平台提供的ant design版本吗，因为目前平台的模板缺少我们需要的组件，而样式也会有冲突修改的数据：报错信息(请尽量使用代码块的形式展现)：
Network:原因分析：疑问：现在平台内置的前端模板是针对猪齿鱼平台的，开发模板，你的应用目录可以随意，只要按照我们ci方式构建镜像，生成chart，就可以通过我们平台部署。Choerodon平台版本: 0.5.0遇到问题的执行步骤:
按照0.6版本的分布部署，最后配置完oauth以后，gitlab打不开了。
文档地址:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问解决了。忘加了这个
不过，现在的问题是创建项目，在gitlab上没有同时创建group需要在平台层-角色管理界面配置组织管理员、项目管理员、项目成员、部署管理员，并且打上对应的标签。角色标签：用于定义角色的特定逻辑的功能，需与代码开发结合。例如：
（1）organization.owner：当用户创建组织时会自动为其分配拥有该角色标签的角色。如组织管理员。
（2）project.owner：当用户创建项目时会自动为其分配拥有该角色标签的角色。如项目所有者。
（3）gitlab.owner：当给某用户在某项目下分配拥有该标签的角色后，平台会为该用户在gitlab对应的group下分配owner角色。
（4）gitlab.developer：当给某用户在某项目下分配拥有该标签的角色后，平台会为该用户在gitlab对应的group下分配developer的角色。所以，根据我们平台的业务逻辑，需给组织管理员打上organization.owner标签；
创建项目所有者，分配相关权限，并打上project.owner、gitlab.owner的标签；
创建项目成员，分配相关权限，并打上gitlab.developer的标签；
创建部署管理员，分配相关权限，无需打标签。或者您不用这么完备的方式，可以只给系统预定义的组织管理员打上organization.owner标签；
给项目管理员打上gitlab.developer的标签。就可以成功创建项目了。嗯，问题解决了。重新更新了下gitlab-service服务之前创建了一个应用B， 今天又重现创建了一个应用A；都显示在“创建中”A应用，可以正常打开代码库，
B应用， 应该是没有实际创建成功应用A，目前状态已经正常， 应用B 是否可以提供删除操作你的猪齿鱼版本是0.5.0还是0.6.0。现在版本不支持删除创建中应用，我们正在优化这个问题。在官方的SaaS版本的，应该是0.6.0吧？  好的，了解。包括 已失败的部署实例，也可以提供删除功能这个功能我们已提供 如上图所示， 就是没有可以删除的选项 ；（还是必须把原部署的环境连接上来，才可以删除？）对的，只有环境是运行中的时候，我们进行的操作才能生效，如删除实例。若是环境已经被提前“清理”掉了呢？    或者环境就是 永远“失联”了。是否还是能提供某些选项，进行垃圾信息的清理。（1）对于环境的清理，请使用我们的环境停用功能，我们会校验该环境下是否有实例、网络、域名相关内容：如果有，应先删除实例及清理配置。
（2）系统只能确定是否“失联”，没法定义什么是“永远”，因此暂且从操作规范上进行思考吧
（3）对于垃圾信息的清理的功能我们认为不能随意添加，因为对于环境上的实例、网络、域名操作需要慎重考虑，我们目前提供的对于环境未连接的判断及资源对象一致性校验机制已经可以避免猪齿鱼与环境数据不一致的情况，也可以避免用户在不知情或不注意时的误操作。这样会不会解决那您的疑惑了呢？或者是否还有一些其他建议吗?好吧。是否可以考虑增加 “危险操作”的开关， 控制在特定权限或者经过confirm的情况进行某些高危操作，或者，某些资源打上特定标签（比如“正式”环境等），特定标签的资源，受控操作。可以的，我们会计划进新的迭代中。
比如您刚刚的场景中有没有什么希望受控的操作？
另外，如果您不经过我们平台，直接清理了环境上的东西，这个目前我们还没法限制… 其实，我是希望非“正式”标签的，都可以直接删除等等。其他没有了，感谢回复好的，明白了~感谢提建议~我们会认真考虑一下的~~  Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost问题描述：
本地使用 docker 安装了 基础的服务 API-GATEWAY GATEWAY-HELPER MANAGER-SERVICE OAUTH-SERVER MANAGER-SERVICE EUREKA-SERVICE
本地使用微服务模板写了一些接口你测试manager-service应该不是403吧，你自己写的服务没有在gateway-helper配置路由，在gateway-helper添加环境变量zuul.routes.demo.path=/demo/**和zuul.routes.demo.serviceId=demo服务的服务名字Choerodon平台版本：0.6.0运行环境：SAAS环境建议：快速搜索时，点击仅我的问题，只会通过用户故事的负责人筛选，子任务的属于我的任务不会被筛选出来。希望是通过子任务的负责人筛选目前正在开发自定义筛选器功能，下个版本中会对该功能进行支持。对的，敏捷看板board的状态流转是基于泳道的，同列的状态流转不属于泳道，所以目前设计禁止了同列的状态流转服务全部本地部署，kafka跟zookeeper也是本地部署。
自动初始化路由是正确的，数据库路由表记录也正确，日志：
2018-06-12 20:26:37.309  INFO [manager-service,] 15740 — [tionScheduler-1] i.c.m.d.service.impl.IRouteServiceImpl   : manager-service autoRefreshRoute
2018-06-12 20:26:37.334  INFO [manager-service,] 15740 — [tionScheduler-1] i.c.m.d.service.impl.IRouteServiceImpl   : manager : 初始化路由成功启动的服务有eureka/config/api-gateway/manager-service/oauth/gateway-helper
求助大佬。。。麻烦请描述清楚问题问题就是http://localhost:8080/manager/swagger-ui.html ERROR PAGE 404你的docker-compose是文档里最新的吗？对比下，如果一样麻烦看下log，是api-gateway的报的404异常？追到，是gateway那里验证没有JWT TOKEN。。。是哪里没有配置正确吗，谢谢8080， 肯定是网关的。。要先登录，点击接口右边红点弹出登录页面先登录你贴下manager-service数据库的路由表，gateway 有别的异常日志吗？gateway那里日志没有JWT TOKEN是正常的，因为那个请求不需要登录不需要JWT,好像只自动注册的了一条框架的。。。把REGISTER_SERVER_NAMESPACE配置为go-register-server所在的k8s的namespaceREGISTER_SERVER_NAMESPACEREGISTER_SERVER_NAMESPACE 你说的这个是指哪个?
go-register-server所在的k8s的namespace 又是指？？
你的命令是0.6.0版本吗？如果是0.5.0不需要加。是0.6.0加个–set env.open.REGISTER_SERVER_NAMESPACE=choerodon-devops-prod 和–set env.open.REGISTER_SERVICE_NAMESPACE=choerodon-devops-prod。参数说明：https://github.com/choerodon/go-register-server明白了，谢谢…因为会有需要临时搭一套demo环境的情况，实际情况微服务一般会有多个应用+多个中间件，如果是一个一个的去部署，再配置之间的调用关系，操作复杂性比较高。请问未来是否会实现这种功能，或着有什么建议？@yifanyou  使用一键安装脚本可以一次性安装完整的环境。http://choerodon.io/zh/docs/installation-configuration/steps/choerodon/@vinkdong
不好意思，不是搭建环境。是搭建好以后。我会有好几个应用。现在不是有一个应用部署的模块吗，我希望有一个应用集合部署的模块。这个意思我们有在设计增加这类功能，能不能表述一下您觉得如何集合部署会比较方便~:grin:Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：文档地址： http://choerodon.io/zh/docs/development-guide/front/new-func/new_module/
根据文档执行操作，在第五步 ：进入到  boot  的目录下, 运行  npm run gulp，执行后报错报错信息(请尽量使用代码块的形式展现)：
原因分析：
根据报错信息，修改boot文件夹下的package.json，添加
“script”: {
“gulp”: “gulp”
}
后仍然报错，具体信息如下：
疑问：
请问这是什么原因导致的呢？你好，你上面列的版本是0.5.0，但是报错信息里面是0.6.1？
请问下具体的版本是多少？因为两个版本的编译脚本略有不同0.6.1的，我修改下版本0.6.x 是在 iam 路径下，执行如下命令最新的文档我们很快会发布到官网上您好，我的问题是出现在开发新模块，是没有iam这个路径的。按照文档来的话，只有boot和demo两个文件夹。1、在本地创建新模块的文件夹。2、在文件夹下创建config.js，webpack.config.js，并修改。3、在子文件夹下创建package.json文件，并修改。4、创建Index文件，文件名为模块名大写+Index。5、在package.json 同级的目录下，安装并启动。6、查看效果在浏览器中键入 localhost:9090，查看页面效果。您好，根据以上文档，执行后页面报错如下，是因为地址拼写的有问题
如果使用localhost，拼接的路径是对的
方便提供下配置吗，或者network的信息，看起来好像是后台的api接口地址填错了您好，我的做法是修改config.js文件中的server，配置如下图所示：
经过尝试，server配置的地址，需要加http://…，谢谢支持！好的，我这边也修改下文档Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost问题描述：
请问猪齿鱼有自动生成后端代码的工具吗你好，猪齿鱼框架暂时不提供后端生成代码的功能，你可以使用平台的微服务模板创建后端应用http://choerodon.io/zh/docs/quick-start/microservice-backend/创建应用以后，生成了新的应用和对应的git仓库地址，但是在git bash中clone提示错误。
生成的仓库地址为：https://code.choerodon.com.cn/techsupport-zhoujundemo/handtest.git。
克隆的指令为：git clone http://code.choerodon.com.cn/techsupport-zhoujundemo/handtest.git
报错提示：密码已经私信你了，请注意查收。我也是同样的问题。。
请问要怎么解决
我也遇到同样的问题，请问这个怎么解决呢？Choerodon平台版本: 0.6.0遇到问题的执行步骤:
使用helm安装redis,mysql等数据库的时候会有权限问题，报如下错误，
redis
[root@master ~]# kubectl logs devops-redis-c57554c5d-6qx9j -n choerodon-devops-prod
1:C 11 Jun 12:29:42.783 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo
1:C 11 Jun 12:29:42.783 # Redis version=4.0.2, bits=64, commit=00000000, modified=0, pid=1, just started
1:C 11 Jun 12:29:42.783 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf
1:M 11 Jun 12:29:42.786 * Running mode=standalone, port=6379.
1:M 11 Jun 12:29:42.786 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.
1:M 11 Jun 12:29:42.786 # Server initialized
1:M 11 Jun 12:29:42.786 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command ‘echo never > /sys/kernel/mm/transparent_hugepage/enabled’ as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.
1:M 11 Jun 12:29:42.786 # Fatal error loading the DB: Permission denied. Exiting.mysql
[root@master ~]# kubectl logs choerodon-mysql-dcb9b88cd-z8pdf -n choerodon-devops-prod
Initializing database
mysqld: Can’t create/write to file ‘/var/lib/mysql/is_writable’ (Errcode: 13 - Permission denied)
2018-06-12T01:42:58.253855Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).
2018-06-12T01:42:58.255613Z 0 [ERROR] --initialize specified but the data directory exists and is not writable. Aborting.
2018-06-12T01:42:58.255628Z 0 [ERROR] Aborting文档地址:
http://choerodon.io/zh/docs/installation-configuration/steps/parts/base/redis/使用该文档的分布安装，使用一键部署时也会出现同样的问题环境信息(如:节点信息):
使用阿里云ecs实例搭建的kubernetes 8.5集群报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问
所有服务器使用的是root用户，而且是通过镜像安装，为什么会报这样的错误@xinghao 调整下对应目录的权限，mysql等启动时默认不是root用户和您执行命令的用户无关。redis，mysql等在容器里运行，会和宿主机上的目录权限有关吗？如果目录只能root读写，而容器中的用户为非root，则容器中的应用无法读写相应目录。那请问怎么控制容器里的使用的用户呢？直接用helm包管理器安装的。官方mysql镜像默认不是root用户，建议你修改相关文件夹权限。@yifanyou 您好，感谢你的反馈，我们已经检查到此问题，现在应该已经修复了，执行 helm update 后再试一下。Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：gitlab ci官网描述不同的stage，不属于git版本管理的文件，是不会保留到下一个stage的。我在猪齿鱼提供的gitlab.ci中看到，对于app.jar，只是建了一个/cache的目录，把target/app.jar拷贝到了cache目录下。然后docker_build 这个stage去cache目录下去取。实际情况下，我自己的项目去跑的时候，在docker_build这个stage是找不到/cache这个路径的，也拿不到app.jar执行的操作：报错信息(请尽量使用代码块的形式展现)：建议：知道了，应该是这里做配置
请问镜像仓库，gitlabci是在哪里做的login，我现在都是显式在gitlabci里写login0.5 升级到 0.6，该怎么升级呢？只重装猪齿鱼部分，还是要重新搭？@yifanyou 您好， 只用升级猪齿鱼部分即可，如何从0.5升级到0.6相关文档最快将于明天(2018年6月12日更新)。Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost问题描述：
文档地址：http://choerodon.io/zh/docs/development-guide/front/new-func/new_page/
执行python ./boot/structure/pythonsql.py 这句脚本有问题。
1、boot/structure路径下不存在pythonsql.py
2、该命令缺少参数指定 database   请加上 -d
3、boot/structure路径下有2个相关的py文件  pythonsqlold.py sql.py，这两个执行完都不报错 ，但数据库没有变化原因分析：
请完善文档，坑太多了感谢您的意见，我们会尽快修改开发文档看到页面已经出现了0.6菜单，但是看了看教程里面的配置，版本还是0.5您可以把教程文档连接发给我吗？http://choerodon.io/zh/docs/installation-configuration/steps/parts/choerodon.devops/这里
–version=0.5.0多谢，我们正在修复安装文档。Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost问题描述：
本地开发时把esline校验关闭了，代码可以正常运行。提交后 gitlab-ci有报错，发现时esline报错的。是否有相关配置可以关闭esline校验
请问是eslint?如果想关闭单个文件的，可以在文件头部添加/*eslint-disable*/, 这个是eslint的标准规范，可以百度查询Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：http://choerodon.io/zh/docs/development-guide/backend/intergration/run/
这篇文档给出的docker-compose.yaml中的镜像下载不了@Waxion 看似是您的docker网络有问题，检查您docker的网络http://registry.cn-hangzhou.aliyuncs.com/  这个域名是访问不了的此域名在浏览器是没法访问的，尝试用docker拉取其他本地没有的镜像看能否成功。我从 https://openchart.choerodon.com.cn/choerodon/c7n/ 这个charts库里找了一下chart 根据里面的镜像地址 可以下载镜像 如registry.saas.hand-china.com/tools/zookeeper:3.4.10-hand尝试把您的DNS 改为 8.8.8.8 再试一下, 如果还是不行试一下重启机器。1.本地运行环境系统Linux（Ubuntu）2.基础组件镜像获取，采用docker-compose方式运行启动，docker-compose.yml如下：问题点如下：初始启动容器时，正常启动，如下图：
组件容器：
等待十来分钟之后，有四个组件容器自动停止退出，如下：
组件容器：
自动停止退出的几个组件容器日志信息如下：
api-gateway
gateway-helper
manager-service
oauth-server
错误原因信息大概一致，由于字符限制原因，采用截图方式，如下：
3.根据文档demo创建本地案例运行案例bootstrap.yml如下：TodoServiceApplication项目启动类：
问题点：
1. 未添加@EnableEurekaClient注解，项目启动不报错，但服务未能注册到注册中心
2. 添加@EnableEurekaClient注解注解后，项目无法正常启动，错误日志如下：注意： 经测试，我的本地案例代码是可以连接到mysql容器数据库的，并可以进行读取数据等操作，重点在于服务无法注册到注册中心。。。。老哥，根据你的这个配置，组件容器的问题倒是解决了，下面的那个服务注册的咋整哈！我刚才测了下是可以注册上去的，可能跟你本机的docker有关，可能端口没有映射，你浏览器访问http://127.0.0.1:8000可以访问通不？浏览器可以访问 Eureka，但是就是本地demo案例一直无法注册上去。。。这些都没问题，哎，实在不行的话，你能把你那边的demo案例发我一份吗？我这试了多次了，1.本地新增项目自己构建一个eureka注册中心，demo案例项目服务可以注册；2.项目在不加@EnableEurekaClient注解的情况下启动，无报错，但是服务无法注册，通过postman访问接口，可以拿到mysql容器数据库表中的数据。理论上如果项目可以正常访问mysql容器组件，那么项目服务应该也可以注册到eureka注册中心组件容器吧，而且注册中心这个我通过浏览器localhost:8000是可以访问到的。咨询下，这个平台线上有运行的环境吗？怎么访问？如上服务注册的问题，已查找到原因，application.yml配置问题，eureka相关配置挂在了spring节点下导致，如下为正确配置：Choerodon平台版本：0.6.0运行环境(如localhost或k8s)：localhost问题描述：在github上 choerodon/config-server这个项目的里面的配置文件src\main\resources\application.yml有错误
嗯，谢谢，去掉一个binder，下版本更正。我们线上用的是环境用的SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS和SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES环境变量注入，所以这个配置没生效没发现Choerodon猪齿鱼是一个开源企业服务平台，是基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理，并提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，来帮助企业聚焦于业务，加速数字化转型。2018年6月10日，开源企业级数字化服务平台——Choerodon猪齿鱼发布0.6版本。0.6版本主要新增敏捷管理服务，并对已有的服务进行了优化，同时修复了若干bug。敏捷管理敏捷管理服务主要用来管理项目的需求、计划和执行，包括问题管理、待办事项、版本发布、活跃冲刺、模块管理等。 问题管理  ： 用户可以以模块、修复版本、标签、史诗、冲刺等方式管理项目中的问题，支持问题查询、创建、编辑、协作处理和添加子任务。 待办事项  ： 管理史诗、版本和冲刺 ， 用户可以构建一个新的待办事项，或者对现有的待办事项进行处理，包括创建、排序和筛选。 发布版本  ： 管理追踪项目版本，查看版本状态，编辑版本详情，并对一个版本进行发布。 活跃冲刺  ： 通过看板来观察和管理工作，展示团队目前正在进行的冲刺，支持问题的创建、更新、筛选、删除和时间追踪，支持问题状态更改，以及列的自定义配置。 模块管理  ： 通过模块对项目问题进行分类管理 ， 例如“后端任务”，“基础架构”等。持续交付持续交付服务新增了如下的功能：增加发布管理，包括应用发布及应用市场。在网络/域名管理中，增加网络/域名状态和操作类型及状态，以便跟踪网络/域名的运行情况。增加容器日志，以便追踪容器运行情况。在环境客户端上增加资源对象一致性机制，同时增加消息发送失败及超时确认机制。同时，在持续交付中，0.6版本还增强了部分功能：重构应用部署页面，移除实例查看功能，增加应用实例页面。在网络管理中区分自身端口和目标端口。改进应用部署方式，从纵向步骤条到横向步骤条。提升实例用户体验使得更简洁直观。修改三个预定义应用模板使其能顺利生成版本及部署成功。另外，还增强了其它功能，例如：改善values的替换方式及yaml主题配色使得用户体验更佳。基于更规范的命名规则修改一些API。为了修改传值模式重构gitlab-service。优化了首次用helm部署的实例扫回机制。微服务开发框架微服务开发框架增加了如下的功能：新增Root管理员，可以管理平台的设置以及平台中所有组织和项目。新增用户修改头像、用户名和邮箱功能，用户个人中心页面优化。新增微服务路由管理功能，用于可视化管理微服务的后端路由。LDAP 支持自定义用户属性，增加页面测试连接和同步用户功能，目前支持OpenLdap 和 Microsoft Active Directory两种目录类型。认证服务添加redis作为存储登录session，用于保证认证服务开启多实例时的用户会话。同时，在微服务开发框架中，0.6版本还增强了部分功能：平台权限校验逻辑完善。注册中心支持指定namespace的服务注册。菜单icon替换，文字间距调整。页面图标间距统一，添加提示文案，按钮操作提示文案优化。页面增加删除确认提示，降低误删几率。最后，0.6版本还修复了0.5版本中的bug。修复组织下创建项目时，项目编码不是组织内唯一，而是全局唯一的问题。修复新增角色分配时，会将用户已有的角色的标签清除的问题。修复注册中心发送事件异常，kafka消息不带有时间戳的问题。修复manager-service有时候权限刷新不进去的问题。修复火狐浏览器下菜单配置功能无法使用的问题。修复角色分配中，无法按照角色查看成员的问题。移除页面中不正确的权限编码，该bug会导致页面无法按照应有的权限。修复菜单配置中，一个自设目录放在另一个自设目录下时，会导致两个目录消失的问题。修复分支管理的版本判断逻辑错误及前台提示错误。修复url出现双斜杠导致代码库无法拉取。修复标记列表不能分页。修复devops和choerodon-agent重启后各对象状态不一致。修复组织管理员不在gitlab template group中的问题。更加详细的内容，请参阅Release Notes。欢迎通过我们的GitHub和猪齿鱼社区进行反馈与贡献，帮助Choerodon猪齿鱼不断成长，我们将持续迭代优化，敬请期待。Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：在一个js文件里，创建两个组件（父与子组件）页面加载时报错修改的数据：import { Table, Button, Popconfirm, Input, Icon } from ‘antd’;
import { observer } from ‘mobx-react’;
import { withRouter } from ‘react-router-dom’;// import asyncRouter from ‘…/…/…/…/…/util/asyncRouter’;
//
// const EditableCell = asyncRouter(() => import(’./EditableCell’));
@observer
class EditableCell extends Component {
state = {
value: this.props.value,
editable: false,
}handleChange = (e) => {
const value = e.target.value;
this.setState({
value,
});
}check = () => {
this.setState({
editable: false,
});
if (this.props.onChange) {
this.props.onChange(this.state.value);
}
}edit = () => {
this.setState({
editable: true,
});
}
render() {
const { value, editable } = this.state;
return (
@observer
class EditableTable extends Component {
constructor(props) {
super(props);
this.columns = [{
title: ‘name’,
dataIndex: ‘name’,
width: ‘30%’,
render: (text, record) => (
<EditableCell value={text} onChange={this.onCellChange(record.key, ‘name’)}/>
),
},
{
title: ‘age’,
dataIndex: ‘age’,
},
{
title: ‘address’,
dataIndex: ‘address’,
},
{
title: ‘operation’,
dataIndex: ‘operation’,
render: (text, record) => {
return (
this.state.dataSource.length > 1 ?
(
<Popconfirm title=“Sure to delete?” onConfirm={() => this.onDelete(record.key)}>
Delete

) : null
);
},
}];
this.state = {
dataSource: [{
key: ‘0’,
name: ‘Edward King 0’,
age: ‘32’,
address: ‘London, Park Lane no. 0’,
}, {
key: ‘1’,
name: ‘Edward King 1’,
age: ‘32’,
address: ‘London, Park Lane no. 1’,
}],
count: 2,
};
}
onCellChange = (key, dataIndex) => {
return (value) => {
const dataSource = […this.state.dataSource];
const target = dataSource.find(item => item.key === key);
if (target) {
target[dataIndex] = value;
this.setState({ dataSource });
}
};
}onDelete = (key) => {
const dataSource = […this.state.dataSource];
this.setState({ dataSource: dataSource.filter(item => item.key !== key) });
}handleAdd = () => {
const { count, dataSource } = this.state;
const newData = {
key: count,
name: Edward King ${count},
age: 32,
address: London, Park Lane no. ${count},
};
this.setState({
dataSource: […dataSource, newData],
count: count + 1,
});
}render() {
const { dataSource } = this.state;
const columns = this.columns;
return (
}warning.js?8a56:33 Warning: Cannot update during an existing state transition (such as within render or another component’s constructor). Render methods should be a pure function of props and state; constructor side-effects are an anti-pattern, but can be moved to componentWillMount.不建议定义多个class，虽然没有语法错误，但是eslint会报错。你的代码很难辨识，请用代码块包裹。Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：我新建了一个demo.less文件，放在如图文件目录下,在我的js文件中要引用这个less文件，不清楚要怎么写修改的数据：报错信息(请尽量使用代码块的形式展现)：
Network:Module not found: Error: Can’t resolve ‘…/css/demo.less’ in ‘D:\zcyworkspace\xfsjk-demo-7\boot\src\app\iam\containers\organization\list_form’import ‘…/css/demo.less’Choerodon平台版本: 0.5.0遇到问题的执行步骤:
部署register server  验证部署文档地址:
http://choerodon.io/zh/docs/installation-configuration/parts/choerodon/#部署register-server环境信息(如:节点信息):
在choerodon-devops-prod命名空间中部署了全套微服务
在新的命名空间中刚部署好 redis kafka 和 zookeeper
在部署完 register-server 后用验证部署命令，发现里面有很多应用0.5.0的register-server是监听所有namespace的，我们这周将要发布的0.6.0才可以在一套集群中搭建多套猪齿鱼。Choerodon平台版本: 0.5.0遇到问题的执行步骤:
Kafka部署文档地址:
http://choerodon.io/zh/docs/installation-configuration/parts/base/kafka/环境信息(如:节点信息):
在这个K8S集群中，在choerodon-devops-prod这个命名空间已经部署一整套的choerodon平台。
现在开辟了一个新的命名空间 operation-test-xfs-test-001
现在要在这个命名空间里面部署一套新的微服务开发环境。
已经部署了 redis ， zookeeper#############   部署Zookeeper
helm install paas/create-pv 
–set type=nfs 
–set pv.name=wx-zookeeper-pv-00 
–set labels.app=wx-zookeeper 
–set nfs.path=/u01/nfs/exports/wx/zookeeper-00 
–set nfs.server=nfs.console.retailsolution.cn 
–set pvc.enable=false 
–set size=3Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name wx-zookeeper-pv-00 --namespace=operation-test-xfs-test-001
helm install paas/create-pv 
–set type=nfs 
–set pv.name=wx-zookeeper-pv-01 
–set labels.app=wx-zookeeper 
–set nfs.path=/u01/nfs/exports/wx/zookeeper-01 
–set nfs.server=nfs.console.retailsolution.cn 
–set pvc.enable=false 
–set size=3Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name wx-zookeeper-pv-01 --namespace=operation-test-xfs-test-001
helm install paas/create-pv 
–set type=nfs 
–set pv.name=wx-zookeeper-pv-02 
–set labels.app=wx-zookeeper 
–set nfs.path=/u01/nfs/exports/wx/zookeeper-02 
–set nfs.server=nfs.console.retailsolution.cn 
–set pvc.enable=false 
–set size=3Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name wx-zookeeper-pv-02 --namespace=operation-test-xfs-test-001
helm install paas/zookeeper 
–set replicaCount=3 
–set persistence.enabled=true 
–set persistence.selector.app=“wx-zookeeper” 
–name=wx-zookeeper --namespace=operation-test-xfs-test-001##########  部署Kafkahelm install paas/create-pv 
–set type=nfs 
–set pv.name=wx-kafka-pv-00 
–set labels.app=wx-kafka 
–set nfs.path=/u01/nfs/exports/wx/kafka-00 
–set nfs.server=nfs.console.retailsolution.cn 
–set pvc.enable=false 
–set size=3Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name wx-kafka-pv-00 --namespace=operation-test-xfs-test-001helm install paas/create-pv 
–set type=nfs 
–set pv.name=wx-kafka-pv-01 
–set labels.app=wx-kafka 
–set nfs.path=/u01/nfs/exports/wx/kafka-01 
–set nfs.server=nfs.console.retailsolution.cn 
–set pvc.enable=false 
–set size=3Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name wx-kafka-pv-01 --namespace=operation-test-xfs-test-001
helm install paas/create-pv 
–set type=nfs 
–set pv.name=wx-kafka-pv-02 
–set labels.app=wx-kafka 
–set nfs.path=/u01/nfs/exports/wx/kafka-02 
–set nfs.server=nfs.console.retailsolution.cn 
–set pvc.enable=false 
–set size=3Gi 
–set “accessModes[0]=ReadWriteOnce” 
–name wx-kafka-pv-02 --namespace=operation-test-xfs-test-001helm install paas/kafka 
–set replicaCount=3 
–set persistence.enabled=true 
–set persistence.selector.app=“wx-kafka” 
–set zookeeperConnect=“zookeeper-wx-zookeeper-0.wx-zookeeper-headless.operation-test-xfs-test-001.svc.cluster.local:2181,zookeeper-wx-zookeeper-1.wx-zookeeper-headless.operation-test-xfs-test-001.svc.cluster.local:2181,zookeeper-wx-zookeeper-2.wx-zookeeper-headless.operation-test-xfs-test-001.svc.cluster.local:2181” 
–name=wx-kafka --namespace=operation-test-xfs-test-001报错日志:
原因分析:
1、可能是kafka对应的pvc未生成
2、zookeeper连接地址不对疑问:
可否说明一下 zookeeperConnect 这个里面的zookeeper的地址是怎么拼接的你好，zookeeper地址为 [pod名称].[service名称].[命名空间]:[端口号]。@Waxion zookeeper是否有报错日志呢？Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：在一个js文件里引用另外一个js文件，用
发现import语法错误，请问这个问题要忽略还是要怎么改，如果忽略的话，那是不是意味着eslint的校验并不能帮助我更好的按照平台的代码规则来写代码修改的数据：报错信息(请尽量使用代码块的形式展现)：
原因分析：疑问：能贴一下.eslintrc的代码吗？{
  “root”: true,
  “parser”: “babel-eslint”,
  “env”: {
    “browser”: true,
    “node”: true,
    “es6”: true
    // “worker”: true
  },
  “parserOptions”: {
    “ecmaVersion”: 6,
    “sourceType”: “module”,
    “ecmaFeatures”: {
      “jsx”: true
    }
  },
  “plugins”: [
    “react”
  ],
  “extends”: “airbnb”,
  “rules”: {
    “react/jsx-no-bind”: [
      “error”,
      {
        “ignoreRefs”: true,
        “allowArrowFunctions”: false,
        “allowBind”: true
      }
    ],
    “react/prefer-stateless-function”: [“off”,
      {
        “ignorePureComponents”: true
      }
    ],
    “import/no-extraneous-dependencies”: “off”,
    “no-else-return”: “off”,
    “linebreak-style”: “off”,
    “import/extensions”: “off”,
    “import/no-unresolved”: “off”,
    “react/prop-types”: “off”,
    “react/jsx-filename-extension”: “off”,
    “jsx-a11y/href-no-hash”: “off”,
    “react/require-default-props”: “off”,
    “no-console”: “warn”,
    “no-debugger”: “off”,
    “jsx-a11y/anchor-is-valid”: [
      “warn”,
      {
        “aspects”: [
          “invalidHref”
        ]
      }
    ]
  },
  “globals”: {
    “HAP”: true
  }
}如果我们关闭验证，会不会提交到平台对页面有影响npm start之后报错，请问如何解决
因为按照文档建的demo文件夹是空的，gulp 运行不会成功，请至少保证新建的文件夹包含以下目录Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：  前端项目已经克隆到本地原因分析：
运行and官网 List组件无法渲染 ，可能是版本不一致造成的 ？项目的是 “antd”: “^2.12.2”,
官网的是3.6.1疑问：为了复用， and版本可以升级？如果可以怎么操作，有何影响？你指的是antd吗？框架使用的是基于antd@3.4.0改造的choerodon-ui， 该组件库持续更新中^^。另外，choerodon-ui和antd同时使用会有样式冲突。能否给个组件api文档什么的参考下，在租户设置中的 客户端修改会报错，导致修改不成功
能否提供一下network 中具体的报错信息你好，请提供下post请求发送的json数据{“name”:“localhost”,“secret”:“secret”,“authorizedGrantTypes”:“implicit,client_credentials,authorization_code,refresh_token”,“accessTokenValidity”:60,“refreshTokenValidity”:60,“webServerRedirectUri”:“http://localhost:9111”,“objectVersionNumber”:1,“organizationId”:“1”}我在本地用这个json新建了一个client，然后调用更新方法，并没有报空指针异常，所以请把日志里面的堆栈信息发一下这个日志 去哪里找本地启动的话就直接看堆栈信息就行了，但是服务器端要看服务端日志更改了地址以后
这样正常吗？
后端启动应该是正常的
清一下缓存再试试改成http://127.0.0.1:8080 就可以了Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：请尽量详细的描述您遇到的问题，以便我们能更快速的提供解决办法。在项目中创建应用，一直处于创建中，内容如下
nginx应用项目  nginx-test1   http://gitlab.test.io//operation-test-1/nginx-test1.git  创建中在域名和operation-test-1多了一个“/”，但是如果在devops配置时不加/，则在应用模板那里，域名与组名就连在一起。执行的操作：
创建了一个应用报错信息(请尽量使用代码块的形式展现)：
18:21:00.545 [XNIO-3 task-58] INFO  i.c.d.i.common.util.GitUserNameUtil - =====admin
18:21:00.549 [XNIO-3 task-58] ERROR io.undertow.request - UT005023: Exception handling request to /v1/project/1/applications/1/pipelines
org.springframework.web.util.NestedServletException: Request processing failed; nested exception is java.lang.IllegalArgumentException: Illegal character in path at index 34: http://gitlab-service/v1/projects/{projectId}/pipelines
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:982)
at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
at io.undertow.servlet.handlers.ServletHandler.handleRequest(ServletHandler.java:85)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:129)
at org.springframework.boot.web.filter.ApplicationContextHeaderFilter.doFilterInternal(ApplicationContextHeaderFilter.java:55)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.choerodon.resource.filter.JwtTokenFilter.doFilter(JwtTokenFilter.java:99)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:208)
at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:177)
at org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:346)
at org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:262)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:99)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:105)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:81)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.cloud.sleuth.instrument.web.TraceFilter.doFilter(TraceFilter.java:186)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:197)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.micrometer.spring.web.servlet.WebMvcMetricsFilter.doFilterInternal(WebMvcMetricsFilter.java:106)
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
at io.undertow.servlet.core.ManagedFilter.doFilter(ManagedFilter.java:61)
at io.undertow.servlet.handlers.FilterHandler$FilterChainImpl.doFilter(FilterHandler.java:131)
at io.undertow.servlet.handlers.FilterHandler.handleRequest(FilterHandler.java:84)
at io.undertow.servlet.handlers.security.ServletSecurityRoleHandler.handleRequest(ServletSecurityRoleHandler.java:62)
at io.undertow.servlet.handlers.ServletDispatchingHandler.handleRequest(ServletDispatchingHandler.java:36)
at io.undertow.servlet.handlers.security.SSLInformationAssociationHandler.handleRequest(SSLInformationAssociationHandler.java:131)
at io.undertow.servlet.handlers.security.ServletAuthenticationCallHandler.handleRequest(ServletAuthenticationCallHandler.java:57)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.security.handlers.AbstractConfidentialityHandler.handleRequest(AbstractConfidentialityHandler.java:46)
at io.undertow.servlet.handlers.security.ServletConfidentialityConstraintHandler.handleRequest(ServletConfidentialityConstraintHandler.java:64)
at io.undertow.security.handlers.AuthenticationMechanismsHandler.handleRequest(AuthenticationMechanismsHandler.java:60)
at io.undertow.servlet.handlers.security.CachedAuthenticatedSessionHandler.handleRequest(CachedAuthenticatedSessionHandler.java:77)
at io.undertow.security.handlers.AbstractSecurityContextAssociationHandler.handleRequest(AbstractSecurityContextAssociationHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.server.handlers.PredicateHandler.handleRequest(PredicateHandler.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.handleFirstRequest(ServletInitialHandler.java:292)
at io.undertow.servlet.handlers.ServletInitialHandler.access$100(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:138)
at io.undertow.servlet.handlers.ServletInitialHandler$2.call(ServletInitialHandler.java:135)
at io.undertow.servlet.core.ServletRequestContextThreadSetupAction$1.call(ServletRequestContextThreadSetupAction.java:48)
at io.undertow.servlet.core.ContextClassLoaderSetupAction$1.call(ContextClassLoaderSetupAction.java:43)
at io.undertow.servlet.handlers.ServletInitialHandler.dispatchRequest(ServletInitialHandler.java:272)
at io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:81)
at io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:104)
at io.undertow.server.Connectors.executeRootHandler(Connectors.java:211)
at io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:809)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Illegal character in path at index 34: http://gitlab-service/v1/projects/{projectId}/pipelines
at java.net.URI.create(URI.java:852)
at org.springframework.cloud.netflix.feign.ribbon.LoadBalancerFeignClient.execute(LoadBalancerFeignClient.java:56)
at feign.SynchronousMethodHandler.executeAndDecode(SynchronousMethodHandler.java:97)
at feign.SynchronousMethodHandler.invoke(SynchronousMethodHandler.java:76)
at feign.ReflectiveFeign$FeignInvocationHandler.invoke(ReflectiveFeign.java:103)
at com.sun.proxy.$Proxy205.listPipeline(Unknown Source)
at io.choerodon.devops.infra.persistence.impl.GitlabProjectRepositoryImpl.listPipeline(GitlabProjectRepositoryImpl.java:37)
at io.choerodon.devops.app.service.impl.ProjectPipelineServiceImpl.listPipelines(ProjectPipelineServiceImpl.java:70)
at io.choerodon.devops.api.controller.v1.ProjectPipelineController.list(ProjectPipelineController.java:50)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:205)
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:133)
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:97)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827)
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738)
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963)
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897)
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970)
… 77 common frames omitted
Caused by: java.net.URISyntaxException: Illegal character in path at index 34: http://gitlab-service/v1/projects/{projectId}/pipelines
at java.net.URI$Parser.fail(URI.java:2848)
at java.net.URI$Parser.checkChars(URI.java:3021)
at java.net.URI$Parser.parseHierarchical(URI.java:3105)
at java.net.URI$Parser.parse(URI.java:3053)
at java.net.URI.(URI.java:588)
at java.net.URI.create(URI.java:850)
… 98 common frames omitted
18:21:31.368 [AsyncResolver-bootstrap-executor-0] INFO  c.n.d.s.r.aws.ConfigClusterResolver - Resolving eureka endpoints via configuration建议：提出您认为不合理的地方，帮助我们优化用户操作根据您贴的报错信息，不是创建应用的报错信息，url的地址展示问题，我们已经在最新版本修复，不影响。我新建了一个devops用户，然后在gitlab里面也创建了一个devops并加到add_template组里去。
kafka使用分布式部署说明里的验证也符合。
但是创建新的应用后，还是一直显示创建中，查看devops-service的日志，跟之前的错误一致。是不是跟http://gitlab-service/v1/projects/{projectId}/pipelines有关系，gitlab-service应该不是对外的域名。http://gitlab-service/v1/projects/{projectId}/pipelines
这个报错是因为创建应用不成功，id没有回写的报错，这是另外一个查询的api，和创建应用不成功没关系。
你在平台创建的项目code是add_template? 你是不是把devops用户加进了模板库(app_template)里面去了？是的。 放模板用的。[app_template / MicroService]是把用户加在你要创应用的那个组里面，角色是owner,  不是模板库的组里面应用？创建应用的时候，是不是会自动在gitlab上创建这么一个组的吗？创应用的时候是在项目下面创建的 ，比如下图的（验收测试项目用这个），在我们平台创建这个项目的时候会自动在gitlab上面创建一个组，需要把用户在该项目下给他分配一个项目所有者角色
可能是按照分布式部署的时候，gitlab做oauth认证时，执行都顺利，但是验证的时候，没有出现choreodon的登录页面。可以详细说明一下吗就是gitlab.test.io打开的页面还是正常的登录界面，还是可以通过root用户登录到gitlab上。Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：创建环境后，查看env-agent对应的po运行正常，但是页面上一直显示未激活。执行的操作：报错信息(请尽量使用代码块的形式展现)：
查看该po的日志，发现有
1 client.go:68] dial error ws://devops.test.io/agent/?key=env:operation-test-1-test2&envId=: dial tcp: lookup devops.test.io on 10.96.0.10:53: no such host建议：
不清楚遇到这个问题需要如何处理。这个host指的是什么，如何修正这个问题是agent连接不上Devops-service，正如日志中报错，你可以看一下devops的这个域名地址是否正确。[root@testserver choerodon]# kubectl get ing --all-namespaces
NAMESPACE              NAME                       HOSTS                   ADDRESS   PORTS     AGE
choerodon-devops-test   devops-service             devops.test.io                   80        15h
域名应该没问题。
如何判断devops.test.io这个服务启动是正常的。现在只是看pod的状态好像是不太准的。
另外，10.96.0.10:53是内部的kube-dns的地址。
如果这个域名有问题，是不是要重新部署devops-service服务。你这个域名下面应该绑定了其他服务吧，我刚访问了一下是一个登陆页面。
应该是你安装部署devops service时域名地址设置错误了。你可以用如下明令更新这个域名是不是在部署devops的时候设定的。我直接访问域名的时候，页面提示This application has no explicit mapping for /error, so you are seeing this as a fallback.Wed Jun 06 22:02:28 CST 2018There was an unexpected error (type=Not Found, status=404).
这个域名是内网的域名，不是外网的域名，只能本地指定ingress服务地址才能访问。Not Found你安装devops时候，指定这个会自动给你建一个ingress，你集群要有可以使用的ingress 域名。我用的就是这个域名。直接访问肯定404啊那有什么办法来判断这个域名是否正常吗。你可以在后面加/v2/api-docs试试显示如下的一些内容
{“swagger”:“2.0”,“info”:{“description”:“Api Documentation”,“version”:“1.0”,“title”:“Api Documentation”,“termsOfService”:“urn:tos”,“contact”:{},“license”:{“name”:“Apache 2.0”,“url”:“http://www.apache.org/licenses/LICENSE-2.0"}},“host”:“devops.test.io:80”,“basePath”:"/",“tags”:[{“name”:“application-market-controller”,“description”:"Application Market Controller”},{“name”:“git-flow-controller”,“description”:“Git Flow Controller”},{“name”:“default-event-back-check-controller”,“description”:“Default Event Back Check Controller”},{“name”:“devops-ingress-controller”,“description”:“Devops Ingress Controller”},…ingress配置正确了Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：
修改数据初始化脚本
修改数据库配置  init-local-database.sh
在根目录执行脚本问题描述：
数据库初始化失败报错信息(请尽量使用代码块的形式展现)：
target/choerodon-tool-liquibase.jar 不存在原因分析：choerodon-tool-liquibase.jar 包不存在疑问：地址写错了？还是换地址了http://nexus.choerodon.com.cn/repository/choerodon-release/io/choerodon/choerodon-tool-liquibase/0.5.0.RELEASE/choerodon-tool-liquibase-0.5.0.RELEASE.jar理解的不是很明白敏捷管理本周上线，敬请期待~Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：
将devops-service部署helm命令中的gitlab的配置中的“/”去掉问题描述：在自己搭建的choerodon平台中，自定义了一个模板。把官方choerodon模板复制进去了。使用这个自定义模板生成应用。页面显示应用生成成功。点击gitlab地址 发现里面没有文件，也没有master分支和dev分支。查看gitlab-service日志 发现有报错发现如果不选择模板 也是报同样的错误创建应用不选模板是不会创建分支，截图上的不是这个错误。Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：镜像已经下载，docker启动本地服务问题描述：docker-compose up -d 启动本地基础服务时报错

zxs@LAPTOP-S1PIIICO MINGW64 /e/Docker
$ docker-compose up -d
Starting oauth-server    … error
Starting eureka-server …
Starting api-gateway   …
Starting redis         …
Starting eureka-server   … error
Starting gateway-helper …
Starting mysql          …
Starting manager-service …ERROR: for oauth-server  Cannot start service oauth-server: driver failed programming external connectivity on endpoint oauth-server (d5d7333967571c6629233c4a2eb556861fadc425640118a1b0cfd44c94Starting zookeeper-0     … errorStarting redis           … error
27d437213): Error starting userland proxy: mkdir /port/tcp:0.0.0.0:8000:tcp:172.18.0.3:8000: input/output errorERROR: for zookeeper-0  Cannot start service zookeeper-0: driver failed programming external connectivity on endpoint zookeeper-0 (e869424f127f06ae9be93113b6b057f72f6c7ce836769b069fea45911952fa8e): Error starting userland proxy: mkdir /port/tcp:0.0.0.0:3888:tcp:172.18.0.4:3888: input/output error
Starting gateway-helper  … error
ERROR: for redis  Cannot start service redis: driver failed programming external connectivity on endpoint redis (997356acc7f93d1211bdd09eacb6fde0dd9a2e79002af0cd111768a46ca57d35): Error starting userland proxy: mkdir /port/tcp:0.0.0.0:6379:tcp:172.18.0.2:6379: input/output errorERROR: for gateway-helper  Cannot start service gateway-helper: driver failed programming external connectivity on endpoint gateway-helper (1ead81d3260e76550a5d171bd916f2a3614b9e24ccb1f2233f64Starting manager-service … error
Starting api-gateway     … error
Starting mysql           … error原因分析：疑问：是配置文件的问题？还是镜像问题@ahthzxs  这个看似docker的问题 尝试执行下面命令并重启docker谢谢，其他服务已经可以启动，只是有两个服务启动后就立即关闭了version: “3”
services:
zookeeper-0:
container_name: zookeeper-0
image: registry.saas.hand-china.com/tools/zookeeper:3.4.10
hostname: zookeeper-0
environment:
- ZK_REPLICAS=1
- ZK_HEAP_SIZE=2G
- ZK_TICK_TIME=2000
- ZK_INIT_LIMIT=10
- ZK_SYNC_LIMIT=5
- ZK_MAX_CLIENT_CNXNS=60
- ZK_SNAP_RETAIN_COUNT=3
- ZK_PURGE_INTERVAL=1
- ZK_LOG_LEVEL=INFO
- ZK_CLIENT_PORT=2181
- ZK_SERVER_PORT=2888
- ZK_ELECTION_PORT=3888
ports:
- “2181:2181”
- “2888:2888”
- “3888:3888”
command:
- sh
- -c
- zkGenConfig.sh && exec zkServer.sh start-foreground
volumes:
- “./kafka/zk:/var/lib/zookeeper”
kafka-0:
container_name: kafka-0
image: registry.saas.hand-china.com/tools/kafka:1.0.0
hostname: kafka-0
depends_on:
- zookeeper-0
links:
- zookeeper-0
ports:
- “9092:9092”
command:
- sh
- -c
- "/opt/kafka/bin/kafka-server-start.sh config/server.properties 
–override zookeeper.connect=zookeeper-0:2181 
–override log.dirs=/opt/kafka/data/logs 
–override broker.id=0 "
volumes:
- “./kafka/kafka:/opt/kafka/data”
mysql:
container_name: mysql
image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/mysql:5.7.17
hostname: mysql
ports:
- “3306:3306”
environment:
MYSQL_ROOT_PASSWORD: root
volumes:
- ./mysql/mysql_data:/var/lib/mysql
- ./mysql/mysql_db.cnf:/etc/mysql/conf.d/mysql_db.cnf
expose:
- “3306”
eureka-server:
container_name: eureka-server
hostname: eureka-server
image: registry.cn-shanghai.aliyuncs.com/choerodon/eureka-server:0.5.0
ports:
- “8000:8000”
links:
- kafka-0
environment:
- spring.kafka.bootstrap-servers=kafka-0:9092
- eureka.client.serviceUrl.defaultZone=http://127.0.0.1:8000/eureka/
- eureka.client.register-with-eureka=false
- eureka.client.fetch-registry=false
- hystrix.stream.queue.enabled=false
- spring.cloud.bus.enabled=false
- spring.sleuth.stream.enabled=false
expose:
- “8000”
api-gateway:
container_name: api-gateway
image: registry.cn-shanghai.aliyuncs.com/choerodon/api-gateway:0.5.0
links:
- eureka-server
depends_on:
- eureka-server
ports:
- “8080:8080”
environment:
- zuul.addHostHeader=true
- zuul.routes.dev.path=/todo/**
- zuul.routes.dev.serviceId=choerodon-todo-service
- eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
- hystrix.stream.queue.enabled=false
- spring.cloud.bus.enabled=false
- spring.sleuth.stream.enabled=false
expose:
- “8080”
gateway-helper:
container_name: gateway-helper
image: registry.cn-shanghai.aliyuncs.com/choerodon/gateway-helper:0.5.0
depends_on:
- eureka-server
- mysql
links:
- eureka-server
- mysql
ports:
- “9180:9180”
environment:
- eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
- spring.datasource.url=jdbc:mysql://mysql/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
- spring.datasource.username=root
- spring.datasource.password=root
- hystrix.stream.queue.enabled=false
- spring.cloud.bus.enabled=false
- spring.sleuth.stream.enabled=false
iam-service:
container_name: iam-service
image: registry.cn-shanghai.aliyuncs.com/choerodon/iam-service:0.5.0
depends_on:
- eureka-server
- mysql
- kafka-0
links:
- eureka-server
- mysql
- kafka-0
ports:
- “8030:8030”
environment:
- eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
- spring.kafka.bootstrap-servers=kafka-0:9092
- spring.datasource.url=jdbc:mysql://mysql/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
- spring.datasource.username=root
- spring.datasource.password=root
- hystrix.stream.queue.enabled=false
- spring.cloud.bus.enabled=false
- spring.sleuth.stream.enabled=false
manager-service:
container_name: manager-service
image: registry.cn-shanghai.aliyuncs.com/choerodon/manager-service:0.5.0
depends_on:
- eureka-server
- mysql
- kafka-0
links:
- eureka-server
- mysql
- kafka-0
ports:
- “8963:8963”
environment:
- spring.kafka.bootstrap-servers=kafka-0:9092
- eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
- spring.datasource.url=jdbc:mysql://mysql/manager_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
- spring.datasource.username=root
- spring.datasource.password=root
- hystrix.stream.queue.enabled=false
- spring.cloud.bus.enabled=false
- spring.sleuth.stream.enabled=false
oauth-server:
container_name: oauth-server
image: registry.cn-shanghai.aliyuncs.com/choerodon/oauth-server:0.5.0
depends_on:
- eureka-server
- mysql
links:
- eureka-server
- mysql
ports:
- “8020:8020”
environment:
- eureka.client.serviceUrl.defaultZone=http://eureka-server:8000/eureka/
- spring.datasource.username=root
- spring.datasource.url=jdbc:mysql://mysql/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
- spring.datasource.password=root
- hystrix.stream.queue.enabled=false
- spring.cloud.bus.enabled=false
- spring.sleuth.stream.enabled=false谢谢回复，这份配置服务很全，就是不定时退出的问题还是存在执行 docker logs kafka-0 是否有报错信息呢信息如下 –override   Optional property that should override values set in
server.properties file开始复制是有问题的，启动不了，我已经编辑过了，才启动的，要错误是启动不了的所以在格式上应该是没有问题，就不知道参数值是不是哪里要修改这个地方 请问你加换行符了吗要不你把邮箱啥的私信给我，我直接发给你，这粘贴的格式会乱掉xianshui.zhu@hand-china.com ，麻烦各位有可以运行的有空发一下，请参考这篇文档 http://choerodon.io/zh/docs/development-guide/backend/develop-env/install_windows/ ，并注意其中换行符 \ 的使用Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
平台已经搭建完毕，切都是正常运行状态。
根据快速入门的教程，创建了choerodon-demo项目，成功在gitlab上创建了group和issue。
然后去环境流水线创建了demo环境，运行中状态，也没有问题
原因分析：
查看了gitlab-service 和 event-store-service的日志，报了相关的错。
疑问：提出您对于遇到和解决该问题时的疑问请问你创建应用用的用户是什么，拥有什么角色，gitlab service报了什么错？1, gitlab service目前没有报错你好，你可以为你这个用户分配一个项目所有者角色，然后在创建应用时，不选模板创建试试1.创建应用的用户是group的owner。也是应用的owener。2.我用的是MicroService的微服务模板。我的创建项目的账号登录上去，是项目管理者角色。
但是我的choerodon平台上项目角色只有项目管理者，这个是不是有问题
是owner的话，你创建一个空的应用试一下？因为你是创建项目的用户，所以自动会分配为owner,
我在想是不是哪里有没有配置kafka的序列化方式导致的？教程里面我没有看到哪里有配置。我上面的截图里也是看到，当我创建应用的时候，报了这个convert的错误但我还不知道怎么去解决devops-service还有这种问题，配置这个平台如果不是熟练工，还是会遇到各种细节上的整合问题。空模板创建后，确实成功了，状态变成了启用。但是伴随着的有下面一些错误event-store-service
还是有convert的错误
gitlab-service
还有这个 updateProject的错误
应该是模板地址本身有问题。
我们正努力完善我们的文档，确保在安装整合框架的过程中，避免因为一些细节造成的问题，感谢你的建议，一起进步。谢谢，重新upgrade了一下devops请问，模板在哪里下载？我在SERVICES_GITLAB_URL 后面加了 ‘/’ 。但是我创建模板生成的git地址是错误的
不好意思，这是我们自定义模板和预定义模板地址拼写错误的bug,目前最新版本已经修复，您可以先使用预定义的模板进行创建应用操作。感谢您的宝贵意见！
由于现在没有提供公共模板库，您可以先拉这个库的模板代码 https://github.com/zhuzhiyang/MicroService.git
推到微服务模板库中，然后就可以选择微服务模板库创建应用了，ps:按照预定义模板地址在gitlab上创建相对应的预定义模板库,记得创建组和创建项目时选择访问等级都设置为public由于现在没有提供公共模板库，您可以先拉这个库的模板代码 https://github.com/zhuzhiyang/MicroService.git
推到微服务模板库中，然后就可以选择微服务模板库创建应用了，ps:按照预定义模板地址在gitlab上创建相对应的预定义模板库,记得创建组和创建项目时选择访问等级都设置为public创建对应组
创建对应项目
Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：docker-compose up -d 命令启动服务问题描述：开始查看都是已经启动的，http://localhost:8000/也可以查看， 一段时间后会自动关闭，http://localhost:8000/也无法打开
原因分析：疑问：跟我这问题一样哈。。。docker logs一下，看看是因为mysql啥的没连上还是内存不足造成的Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：用平台提供的后台应用模板，下载下来以后，没有io.choerodon.test.infra.common.utils.StringUtil这个java类，但是文档中的实体类需要用到，请问要去哪里去找修改的数据：@Override
public String toString() {
return StringUtil.getToString(this);
}这个是后端demo应用，实体类上重写的toString方法，你可以自己实现，StringUtil也是自己写的，没有影响的。Choerodon平台版本: 0.5.0遇到问题的执行步骤:
执行了部署gitlab service的脚本，其中的GITLAB_PRIVATETOKEN使用gitlab的root登陆创建了 Impersonation Tokens。等所有pod都正常运行后 ，在choerodon平台中创建用户，但发现在gitlab中没有生成该用户文档地址:
http://choerodon.io/zh/docs/installation-configuration/parts/choerodon.devops/报错日志:
查看gitlab-service的日志 ，发现有报错
根据您的报错，有可能是找不到gitlab的root用户，请问一下gitlab Administrator用户的用户名是root吗？因为我们这边创建用户都是用root用户去创建的，如果你gitlab默认的Administrator用户的username不是root是创建不成功的？
Administrator用户的username是root
您好，可以把部署gitlab-service的环境变量 
的值贴出来吗？我在本地用了0.5.0的gitlab-service版本，然后用了你发我的gitlab-token和url 环境变量配置项

然后调用同一个api(创建用户你截图报错的那个api)
Ｎot found这个错误我本地测试只有gitlab_url是错误的情况下才会出现,比如我将地址后面的cn改为com
您在仔细检查一下? 或者你在重试创建用户试下？找到问题了，发现并不是gitlab-service部署的问题。
gitlab-service内部会访问 gitlab的api ，我尝试在我的K8S集群里面不同的node执行curl api，发现在node1中执行可以正常返回api信息，但在 node2 和 node3中返回的居然是其他服务的页面信息。根据node2和node3返回的信息，找到是通过choerodon平台部署的应用。
之后我将部署的应用全部删除后 返回api就是正常的信息了。创建用户也可以正常同步到gitlab里面了
之后我在此尝试使用choerodon部署应用，发现我在创建网络时候，如果把外部IP写成我当前K8S的对外IP地址时就会发生上面出现的问题。
这里想再问一下 创建网络时填写的外部IP 具体应该填写什么 有什么作用Choerodon平台版本: 0.5.0遇到问题的执行步骤:按照一键部署完成后，登录front页面，只能看到租户设置、项目设置这两个菜单及下面内容，看不到持续集成等菜单项。文档地址:环境信息(如:节点信息):1master，3个work报错日志:未发现你好，可以参考下




持续交付下详情进去无权限，为什么？ User Guide


    你好，这是因为devops-service的权限没有成功初始化到数据库中，你可以通过swagger界面中，通过 
manager-service -> document-controller ->/docs/permission/refresh/{serviceName} 

接口，手动刷入权限。 
参数为 : 
serviceName : devops-service
version: 0.5.…
  

Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：
使用的是阿里云的VPC环境，平台的搭建已经没有问题。搭建k8s环境的时候，配置的网段如下，一模一样kube_pods_subnet: 172.16.0.0/16
kube_service_addresses: 172.19.0.0/20然后对这两个网段，都配置了
现在的情况是 pod的ip能访问通， svc的clusterIP访问不通原因分析：疑问：你的意思是在同一个VPC下面想搭建两套K8S集群吗？还是你把搭建好的删除了  重新搭？可能是问了一个小白的问题-_-!就是在node上为啥访问不到svc的clusterIP.并不是想搭两套…在集群内部是可以直接访问的，访问时请带上端口号。比如我想访问chartmuseum-chartmuseum，那么就是curl 10.233.41.131:8080你那个nginx的端口号不应该是80吗？噢，这样是可以的。
网络可以设置外部IP为集群中某个节点的IP，然后可以通过这个节点IP：端口号访问。
port则应该设置为所选实例设置的对外暴露的端口号。好的，明白了，谢谢Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：在平台上已经创建了后台项目并把源码下载下来，配置pom.xml文件的时候问题描述：按照文档里的内容拷贝复制到pom文件中以后，出现如图报错
修改的数据：原因分析：是不是这个依赖无法下载疑问：pom文件中要怎么配置才正确你好，请问下是什么文档？麻烦提供下文档地址。谢谢http://choerodon.io/zh/docs/quick-start/microservice-backend/
在： 3. 修改配置信息的地方，修改pom.xml依赖
你好，麻烦把有错误的失效版本 0.1.0 改成 0.5.0.RELEASE
应该是能解决这个问题了Choerodon平台版本: 0.5.0遇到问题的执行步骤:
选择组织=》项目管理=》创建项目=>创建
项目成功后，查看gitlab中 发现并没有生成对应的groups原因分析:
目前不确定是哪里报错了 。先参考http://forum.choerodon.io/t/topic/185
看是不是同类情况Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：k8s问题描述：
在choerodon平台部署发布了一个前端应用，该应用的api地址为我们自己平台上的api地址。
访问页面发现页面有报错，提示该api接口没有访问权限修改的数据：
PRO_API_HOST: api.console.retailsolution.cn报错信息(请尽量使用代码块的形式展现)：
Failed to load localhost:http://api.console.retailsolution.cn/iam/v1/users/self: Cross origin requests are only supported for protocol schemes: http, data, chrome, chrome-extension, https.
点击该连接
<oauth><error_description>Full authentication is required to access this resource</error_description><error>unauthorized</error></oauth>Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：K8S问题描述：
我成功部署了一个前端应用，在K8S集群内部直接访问该pod可以返回html。之后进行创建网络。但创建之后根据IP地址无法进行访问。在集群内部通过curl也访问不到对应的svc报错信息(请尽量使用代码块的形式展现)：
查看了对应的svc 的详情，发现Selector属性为 none
原因分析：
我估计在创建网络时，生成svc未正确挂载pod，麻烦看一下@Waxion  为None是正常的，你是怎么访问的呢？我通过curl ip:port 访问的
curl 192.168.246.27:9393
返回 ：curl: (7) Failed connect to 192.168.246.27:9393; Connection refused直接访问pod的ip:port 可以访问吗我找到问题了 就是创建网络的时候 输入的端口号 对应的时 该应用内部容器里面使用的端口号 。
我这里时个前端应用 因此端口是80 我换成80就可以访问了[INFO]
[INFO] — maven-gpg-plugin:1.5:sign (sign-artifacts) @ eureka-server —
‘gpg.exe’ 不是内部或外部命令，也不是可运行的程序
或批处理文件。
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 8.503 s
[INFO] Finished at: 2018-06-04T11:30:27+08:00
[INFO] Final Memory: 48M/414M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-gpg-plugin:1.5:sign (sign-artifacts) on project eureka-server: Exit code: 1 -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException你好，可以使用mvn package -Dgpg.skip -Dmaven.source.skip=true -Dmaven.javadoc.skip=true 来进行编译你可以参考github的README.mdChoerodon平台版本：0.5.0运行环境(如localhost或k8s)：localhost遇到问题时的前置条件：问题描述：
详见下图、 参照的是 创建一个nginx示例
原因分析：提出您分析问题的过程，以便我们能更准确的找到问题所在疑问：提出您对于遇到和解决该问题时的疑问@yifanyou 您好执行helm list | grep runner  查看你的runner版本是否为 runner-0.1.0如果是的话执行以下命令升级runner谢谢。不过更新后出现了这个问题。需要创建namespace tools？
是的，执行 kubectl create ns tools 创建namespace再尝试一下。创建Gitlab Runner的时候，倒是创建过，但是不在tools这个ns下。是不是还要在tools下创建这两个pvc？那之前创建的有什么用？–
发现创建的时候候还会报命名相同的错。是不是应该在哪里将命名空间tools更新成和choerodon-devops-prod一样由于编写Runner Chart时考虑不周，导致执行后出现很多错误，在此表示诚挚的歉意，请执行以下命令进行修正，谢谢。按照文档中的安装命令Harbor默认是http的,不启用自签名证书。所以需要后期自己配置证书。权限出错请检查Harbor上是否已创建名为“operation-choerodon-test”的Project 。若没有请手动创建，并检查choerodon-devops-service中关于Harbor的变量是否配置正确。之前把harbor地址改成http，现在配置已经更新成https。用户名密码也核对正确
把你CI 文件贴出来一下，谢谢你CI里面有docker login语句吗？第一步为什么还要拉 nginx-demo???请docker push之前添加下面登陆语句那是你自己Dockerfile里面写得啊噢，我是照着创建一个nginx示例复制的…请将你的.gitlab-ci.yml文件改为以下内容谢谢，终于走通了，是菜鸟，多包涵Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：
在激活环境流水线时，执行
helm install --repo=http://charts.saas.choerodon.com.cn/hand-rdc/choerodon/ 
–namespace=techsupport-xfsjk-xfsjk-1 
–name=techsupport-xfsjk-xfsjk-1 
–version=0.5.0 
–set config.connect=ws://devops.service.choerodon.com.cn/agent/ 
–set config.token=9ae509f1-4ce6-4105-b028-0bf92ffe77d1 
env-agent
报错。报错信息(请尽量使用代码块的形式展现)：Error: Looks like “http://charts.saas.choerodon.com.cn/hand-rdc/choerodon/” is not a valid chart repository or cannot be reached: Failed to fetch http://charts.saas.choerodon.com.cn/hand-rdc/choerodon/index.yaml : 404 Not Found原因分析：
可能时chart仓库中缺少该资源疑问：
我昨天晚上做过激活环境，可以成功。但今天又不行了。不好意思，今天早上维护域名证书，出现了刚刚的情况，现已恢复，谢谢您好，
请问猪齿鱼平台是否支持配置多套k8s，部署服务时可以部署于不同k8s集群。
然后我的问题是k8s集群间是怎么通信访问的。比如，部署微服务时，注册中心eureka、网关gateway部署于k8s-A集群，微服务应用A部署于k8s-B集群，但注册于k8s-A集群的eureka上，这个时候是否可以通过k8s-A的gateway去访问微服务应用A。请问有没有这样的场景，以及怎么解决这个网络问题。菜鸟一枚，恳请各位大佬帮忙解惑:joy:你好，choerodon支持一套k8s部署多套环境，或者多套环境部署各自的k8s，但是不支持一套环境跨k8s部署。服务也不可以跨环境访问。而且不建议一套环境分别部署到不同的k8s上，这样在网络，安全上都不好管理。@500 您好，关于kubernetest多集群网络问题可以参考下面这篇文档，在kubernetes中的许多动能定义为Alpha，未明确表示将会发展为GA, 所以不建议在生产环境中使用。
https://kubernetes.io/docs/concepts/cluster-administration/federation/了解了，我们再想想，感谢感谢Get，找时间研究下，谢谢。创建一个前端项目以后，按照文档将项目运行以后，打开localhost:9090/#/iam/demo路径，报js错误，如图，文档中并没有提到修改PromiseObservable.js的相关内容
在设置语言和菜单yml以后，执行指令python .\boot\structure\configAuto.py iam配置config.yml文件时报错，如图
您好，请按照问题的模板提交问题。这样社区其他同学能更方便地定位问题。已按照你的要求修改模板，麻烦描述一下该如何解决抱歉，我们的demo里面有个新修改的未更新。您可以查看目录结构，修改local下的文件名为下图：第二配置config.yml文件报错是脚本中涉及到的依赖module没有安装  所以执行报错（我们会完善文档）。 能可以自行百度安装解决。菜单初始化我们会在部署阶段帮您执行该脚本，推荐您检查好相关文件写正确 然后部署后菜单自然会初始化近数据库。你好，我百度了yam1，好像不存在这样的module，请问这个依赖的名称对吗。或者说应该安装yum？同学纠正一下是yaml不是yam1这里的yaml包的安装方法如下：Choerodon平台版本: 0.5.0遇到问题的执行步骤:环境信息(如:节点信息):报错日志:原因分析:提出您分析问题的过程,以便我们能更准确的找到问题所在疑问:提出您对于遇到和解决该问题时的疑问你好请执行下面命令并反馈给我们一下前端应用的代码修改以后提交git时出现问题，首先代码已经提交到本地的git仓库，如图：
其次，在执行git push origin feature-1.1.0时，也没有报错，如图：
最后，在平台上查看，提交时间并没有改变，进入git明细里面也没有看到修改或者添加的文件提交上来
ps：代码提交完全按照文档：创建一个前端应用中的步骤4“提交代码”执行。您好，你没有完全按照文档操作。
你的提交应该是提交到 develop 分支上了，feature 分支上并没有有效提交。
应该切到 feature 分支进行开发，开发后提交到 feature 分支上，而不是在 develop 分支开发提交。
——————————
建议当前情况，可以先切换至 feature 分支。git checkout feature-1.1.0
在 feature 分支上执行变基操作。 git rebase develop
在远程仓库的develop不更新的情况下，重新push至远程feature分支。git push origin feature-1.1.0
应该就能够在界面上看到提交的改动了。案例部分TodoServiceApplication启动类缺失@EnableEurekaClient注解，不加注解，启动不会报错，但是服务无法注册到注册中心去，加上注解，根据报错信息度娘看了下，还是无法解决，信息如下：您好,请问一下你是本机启动吗,还是还是通过通过平台部署在Kubernetes,看日至报错信息应该是注册中心的地址没有配对.本机启动，没有通过k8s本地启动你需要启动eureka-server。老哥，不是这个问题， 我本地启动了这个组件容器的那你就给demo服务配上你的注册中心地址，对了就行。…就是根据文档来的，而且文档还有问题呢。。FAILED - RETRYING: Copy etcdctl binary from docker container (4 retries left).
FAILED - RETRYING: Copy etcdctl binary from docker container (3 retries left).fatal: [node1]: FAILED! => {“attempts”: 4, “changed”: false, “cmd”: [“sh”, “-c”, “docker rm -f etcdctl-binarycopy; docker create --name etcdctl-binarycopy registry.cn-hangzhou.aliyuncs.com/choerodon-tools/etcd:v3.2.4 && docker cp etcdctl-binarycopy:/usr/local/bin/etcdctl /usr/local/bin/etcdctl && docker rm -f etcdctl-binarycopy”], “delta”: “0:00:00.152531”, “end”: “2018-05-24 17:41:39.987395”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-05-24 17:41:39.834864”, “stderr”: “Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”, “stderr_lines”: [“Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”, “Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”], “stdout”: “”, “stdout_lines”: []}请在集群内任意节点执行以下命令看看会有什么信息查了一下，不是源的问题，是脚本安装的docker服务根本没有起来，自己安装的docker又不认。请教怎么解决？[root@node1 kubeadm-ansible]# systemctl status docker.service
● docker.service - Docker Application Container Engine
Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)
Active: failed (Result: start-limit) since 五 2018-05-25 09:37:54 CST; 7s ago
Docs: https://docs.docker.com
Process: 18211 ExecStart=/usr/bin/dockerd (code=exited, status=1/FAILURE)
Main PID: 18211 (code=exited, status=1/FAILURE)5月 25 09:37:53 node1 systemd[1]: Failed to start Docker Application Container Engine.
5月 25 09:37:53 node1 systemd[1]: Unit docker.service entered failed state.
5月 25 09:37:53 node1 systemd[1]: docker.service failed.
5月 25 09:37:54 node1 systemd[1]: docker.service holdoff time over, scheduling restart.
5月 25 09:37:54 node1 systemd[1]: start request repeated too quickly for docker.service
5月 25 09:37:54 node1 systemd[1]: Failed to start Docker Application Container Engine.
5月 25 09:37:54 node1 systemd[1]: Unit docker.service entered failed state.
5月 25 09:37:54 node1 systemd[1]: docker.service failed.请编辑下面文件删除第三行再按照安装文档进行安装。Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：gitlab配置成Oauth2，登录会跳转。要拉取代码，用户名密码也没有用，怎么弄？登到部署的服务器上，都是可以拉。但是如果在本地做开发，那就有问题了执行的操作：报错信息(请尽量使用代码块的形式展现)：
上面的问题产生在这里。这里描述的nginx-demo项目是猪齿鱼研发下面的一个应用的意思吗？
系统安装好后，缺省的clone代码的密码是abcd1234，如果需要修改，可以在安装好的gitlab个人信息中修改。nginx-demo是演示在猪齿鱼上研发一个简单应用的流程。创建账户没有password。在gitlab上创建密码，就能拉了。谢谢Choerodon平台版本：0.5.0运行环境(如localhost或k8s)：k8s遇到问题时的前置条件：问题描述：按照教程已经安装了10次有余，这一次感觉顺序啥的都没有问题。之前通过一键部署也安装过，虽然持续交付的权限没有，但是创建项目，gitlab上还是能够创建group和issue这次使用分布部署，不管是用admin还是建个用户（可以登录）后去创建项目，gitlab上都没有创建group和issue，不知道该怎么排查这个问题，求教2.gitlab-service 有这种报错
3.系统日志
4.为什么分布部署没有列出hystrix这两项？感觉就是哪里授权配置不对？你好，你可以直接登录本地gitlab，查看下用户有没有在gitlab创建用户创建了。但是group和issue没有创建，是不是因为没有设置token可以把分布式部署gitlab-service服务的helm命令贴出来吗？可能是授权的时候gitlab的token有问题。问：gitlab上没有创建group和issue
答：配置的GITLAB_PRIVATETOKEN参数不正确导致的，请配置正确的一个admin角色用户的impersonation token（且包含以下权限 api、read_use、sudo）。问：gitlab-service 有这种报错
答：配置的GITLAB_PRIVATETOKEN参数不正确导致的，请配置正确的一个admin角色用户的impersonation token（且包含以下权限 api、read_use、sudo）。问：为什么分布部署没有列出hystrix这两项？
答：在编写分步部署文档时考虑到hystrix这两项可以在搭建后Choerodon最基本服务后通过界面进行部署就没有增加这两项的helm安装命令。一键部署脚本是在编写分步部署文档之前就编写好了的，由于当时需求不同就添加了这两项自动部署。问： upgrade 和client有先后之分吗？
答：没有先后之分，但是这两项都必须完成，缺一不可。谢谢，问题已解决。加上了正确的token。gitlab已经有gropu和issue了。不过看了gitlab-service的日志，又有一个新的问题。
在界面上以什么用户操作的时候报这个错？
这个用户在操作的这个项目的角色是什么？不知道需要些什么截图 有需要我后续上传 我在devops上登陆后创建了项目 进入我的gitlab没有对应的group和issue库麻烦您告诉我组织及项目~组织：技术服务事业部
项目：zldhttps://code.choerodon.com.cn/techsupport-zld
您好，请您用自己账号进入该地址，可以看到有group及issue库恩 明白了 谢谢您请问这个是什么地址？？
是你们自己内部的吗？我自己搭建的一套，也出了这个问题，不知道怎么解决Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：已经搭建完毕，可是只有一个运营组织，也找不到创建的地方目前系统仅支持一个组织，这个组织是缺省的组织，即“运营组织”。所以，系统中并没有创建修改组织的功能。项目不可以删除，只能停用。是的，这个我们修改一下文档，你可以在环境变量中设置mysql 和kafka的地址线上版本运行在k8s上，是通过go-register-server进行服务管理的。
由于线下开发一般不会在本地搭建一套k8s，所以我们同时提供了java版本的本地注册中心。现在我们线上的运行环境已经搭好了,swagger也可以正常访问了，但我想查看服务注册情况，访问服务器的８０００端口，出了４０４，


是我访问方法不对么，还是我哪没配置好go-register-server 没有访问界面，可以浏览器访问http://register-server:8000/eureka/apps获取已经注册的服务信息收到了，本地服务的注册ｉｐ，依然是，ｇｏ-register-server的ｉｐ端口号加上/eureka吗？
我启动后，好像没挂载上去，控制台输出是正常的
Choerodon平台版本：0.5.0运行环境：自主搭建问题描述：2.有个 使用Choerodon进行认证登陆 的步骤， upgrade 有什么用？gitlab的 GITLAB_EXTERNAL_URL 和ingress.hosts[0] 这两个地址还不能一样？ 因为一样的话，gitlab通过choerodon登录后，超级管理员root就没入口可以登了？那就安装runner,拿不到token。是不是安装了runner，用户创建的项目才会建到gitlab上面1.根据后端开发手册
docker-compose.yml文件配置如下：version: “3”
services:
zookeeper-0:
container_name: zookeeper-0
image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/zookeeper:3.4.10
hostname: zookeeper-0
environment:
- ZK_REPLICAS=1
- ZK_HEAP_SIZE=2G
- ZK_TICK_TIME=2000
- ZK_INIT_LIMIT=10
- ZK_SYNC_LIMIT=5
- ZK_MAX_CLIENT_CNXNS=60
- ZK_SNAP_RETAIN_COUNT=3
- ZK_PURGE_INTERVAL=1
- ZK_LOG_LEVEL=INFO
- ZK_CLIENT_PORT=2181
- ZK_SERVER_PORT=2888
- ZK_ELECTION_PORT=3888
ports:
- “2181:2181”
- “2888:2888”
- “3888:3888”
command:
- sh
- -c
- zkGenConfig.sh && exec zkServer.sh start-foreground
volumes:
- “/home/guoqingdeng/Desktop/docker-share/kafka/zk:/var/lib/zookeeper”
kafka-0:
container_name: kafka-0
image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/kafka:1.0.0
hostname: 127.0.0.1
depends_on:
- zookeeper-0
links:
- zookeeper-0
ports:
- “9092:9092”
command:
- sh
- -c
- "/opt/kafka/bin/kafka-server-start.sh config/server.properties 
–override zookeeper.connect=zookeeper-0:2181 
–override log.dirs=/opt/kafka/data/logs 
–override broker.id=0 "
volumes:
- “/home/guoqingdeng/Desktop/docker-share/kafka/kafka:/opt/kafka/data”
redis:
container_name: redis
image: hub.c.163.com/library/redis:latest
ports:
- “6379:6379”
mysql:
container_name: mysql
image: registry.cn-hangzhou.aliyuncs.com/choerodon-tools/mysql:5.7.17
ports:
- “3306:3306”
environment:
MYSQL_ROOT_PASSWORD: root
volumes:
- /home/guoqingdeng/Desktop/docker-share/mysql/mysql_data:/var/lib/mysql
- /home/guoqingdeng/Desktop/docker-share/mysql/mysql_db.cnf:/etc/mysql/conf.d/mysql_db.cnf
phpadmin:
container_name: phpadmin
image: registry.saas.hand-china.com/tools/phpmyadmin
ports:
- “8888:80” # 80端口方便浏览器直接访问
environment:
PMA_ARBITRARY: 1 # 用于开启phpadmin关于可否输入host的设置
eureka-server:
container_name: eureka-server
image: registry.cn-shanghai.aliyuncs.com/choerodon/eureka-server:0.5.0
hostname: 127.0.0.1
ports:
- “8000:8000”
manager-service:
container_name: manager-service
image: registry.cn-shanghai.aliyuncs.com/choerodon/manager-service:0.5.0
ports:
- “8963:8963”
api-gateway:
container_name: api-gateway
image: registry.cn-shanghai.aliyuncs.com/choerodon/api-gateway:0.5.0
ports:
- “8080:8080”
environment:
- zuul.addHostHeader=true
- zuul.routes.dev.path=/todo/**
- zuul.routes.dev.serviceId=choerodon-todo-service
oauth-server:
container_name: oauth-server
image: registry.cn-shanghai.aliyuncs.com/choerodon/oauth-server:0.5.0
ports:
- “8020:8020”
gateway-helper:
container_name: gateway-helper
image: registry.cn-shanghai.aliyuncs.com/choerodon/gateway-helper:0.5.0
ports:
- “9180:9180”2.启动问题1.api-gateway 组件容器无法启动，日志报错信息为无法连接zk服务
2.gateway-helper、manager-service、oauth-server组件容器无法启动，日志报错信息为mysql连接被拒其他容器可正常启动3.demo案例问题参考文档案例，运行时，服务未注册到注册中心4.Ubuntu系统下运行麻烦大佬们给解决下哈docker的运行环境麻烦提供下我的电脑系统是ubuntu的，这个应该没多大影响吧。@fuchen 引号是否是英文的"" 而不是中文的“”yml中配置信息？？？英文的粘贴代码的时候使用请使用代码格式， 选中代码后点击编辑框上方的</>，这样便于我们查看。看你贴出来的内容中的引号有点像是中文的引号。(⊙o⊙)…，本来我是想使用markdown语法的，但是那个缩进有问题。。。所以。。。大佬，有查到问题所在吗？你好，你可以参考下这一篇




关于猪齿鱼微服务后端框架搭建问题 Choerodon Framework


    是的，这个我们修改一下文档，你可以在环境变量中设置mysql 和kafka的地址 
SPRING_DATASOURCE_URL: jdbc:mysql://localhost/iam_service?useUnicode=true&characterEncoding=utf-8&useSSL=false
SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS: kafka…
  

好的哈，我瞅瞅看，谢谢了哈最新的文档也已经更新到官网上了哈，现在api-gateway、gateway-helper、manager-service、oauth-server这四个组件容器启动的时候是可以起来的了，但是不知道啥原因，一段时间过后自动停止。。。查看了下日志，信息如下：还有一个问题是如何本地构建的demo案例，服务无法注册到注册中心。。。，yml配置如下：报错是因为无法连接eureka。
可以修改spring.eureka.client.serviceUrl.defaultZone 为本地注册中心的地址
你可以参考下最新的docekr-compose文件
http://choerodon.io/zh/docs/development-guide/backend/intergration/run/还是不行呀，按照文档所改，虽然组件容器启动是可以了，但是还是会自动停止，还有就是我本地的demo案例的服务一直无法注册到注册中心。。。yml配置如下：我将配置中的eureka.client.serviceUrl.defaultZone=http://localhost:8000/eureka/，localhost修改为本地ip也没用，照样过一段时间还是会停止文档中用的不是ip或者localhost，而是http://eureka-server:8000/eureka/, 本地ip和localhost在docker实例里面都无法解析的执行 kubectl get pvc -n [命名空间] 查看下相关的pvc是否正确绑定到pv了。LOST,这种该怎么解决呢已解决。kubectl get pv 看了下发现这些LOST的PVC，绑定了老[命名空间]下的pvkubectl delete pv 这些老命名空间的pv删掉，重新创建就好了。运营管理的菜单在哪里初始化在使用helm安装前端的时候就会初始化相应菜单@LITAONO1 您好，运营管理属于相对独立的一部分很抱歉相关安装文档尚未更新。我们将尽快更新，更新完成之后会通知你。感谢你的支持。请问解决了吗，我也有这个问题你好，这是因为devops-service的权限没有成功初始化到数据库中，你可以通过swagger界面中，通过接口，手动刷入权限。参数为 :权限刷入后，可以重新部署一下前端多谢，菜单已经有了git clone https://github.com/choerodon/kubeadm-ansible.git
这条命令执行不了  求解答  谢谢@li1207116722 是否有报错信息呢，麻烦粘贴一下报错信息。同学，是不是没装git 嗯嗯   可以了 谢谢
但是又出现新问题了
NAME             TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)   AGE
choerodon-mysql  ClusterIP  10.233.30.107         3306/TCP  0s==> v1beta1/Deployment
NAME             DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
choerodon-mysql  1        1        1           0          0s==> v1/Pod(related)
NAME                             READY  STATUS             RESTARTS  AGE
choerodon-mysql-675976c46-wkkcs  0/1    ContainerCreating  0         0schoerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.
choerodon-mysql not ready,sleep 5s,check it.这是设么情况没有报错信息，一直重复这种状态，好像一直在下载mysql镜像。
我只用了一台服务器，只有node1，这种情况不用挂载吧使用  kubectl get po -n [命名空间]  查看下的pod信息前端开发手册-开发新模块中boot目录下无generator-hap您好，因为前端开发目录层次和结构已改变。命名快速创建模块已移除，建议您根据已有模块自建目录和基础文件开发新模块。文档会做相关修改，抱歉。基于猪齿鱼开发全新的前端应用（模块）请通过平台创建一个前端应用开始，选择前端开发模板进行开发。http://choerodon.io/zh/docs/quick-start/microservice-front/你好，之前文档有些错误，现在已经更正了，可以参考文档
使用框架开发前端：http://choerodon.io/zh/docs/development-guide/front/new-func/new_module/
使用平台创建前端项目：http://choerodon.io/zh/docs/quick-start/microservice-front依照前端开发新页面文档，新建一个新的功能文件夹demo及其相关的JS文件:
执行npm start时报错
我们默认开启了eslint 代码检查，这些报错是代码格式不规范报错。照着改正就好，可在ide中配置eslint插件实时提示。你好，请教一下，choerodon-front,choerodon-front-boot,choerodon-front-iam,choerodon-front-devops，源码方式怎么运行？谢谢！你好：你可以通过命令获取代码然后分别进入各子项目，以iam为例启动成功以后打开http://localhost:9090可以访问iam同时运行iam和devops会有端口冲突，你可以修改各自模块下的/boot/bin/server.js 里面的port–recursive感谢回复，但是执行npm install会报错，报没有package.js的错误。进入boot目录下，然后执行命令就OK了，已经解决了，谢谢！启动完成之后，访问localhost:9090，被重定向到了http://api.choerodon.com.cn/oauth/login这个登录页面，这个登录的账号信息？？？首先我是本地运行的，然后我把password字段修改了，密文：$2a$10$ofPkBDUezOJp6Sik63Q/0.QlU8a1itEyzldjSXqfn2nDPqXjN0Ljm，明文：123456a你指的修改是指从git库中拉取下来的代码本地修改这个账号的信息？还是你这修改的就是git库中的？？
还有账号是admin?我是直接修改数据库的，初始化数据库后修改的。(⊙o⊙)…，我这后端部署不起来，文档里面的东西很多都有问题。。。比如，组件模块的镜像我这拉取不到。。。我这边一直不知道怎么进入系统配置。坑大发了。。。您好，如图所示，在选择组织或项目右侧的“管理”按钮，点击即可进入系统配置相关功能。
我的居然没有管理的按钮请问下你是用系统自带的用户登录的吗是的，我用admin登录的。能够提供一下打开页面时，触发/v1/menus/tree?test_permission=true&level=site返回结果吗你可以查看一下后台数据库iam_service/iam_menu 中有没有数据可以把menu, menu_tl, menu_permission 相关的数据都清空一下，重新初始化下菜单
在根目录下运行程序时，提醒缺少package.json，但是克隆下来的应用并没有这个文件，请问从哪里拿还是要自己配置，如果要自己配置的话，应该怎么配置另外，我在iam目录下运行代码也有问题，是不是boot文件中缺少文件，如下图
克隆下来后，在根目录执行 cd boot   npm install按照你的说法，我执行了以后，如上图显示报错See git submodules documentation.Open your browser and visit http://localhost:9090. There is currently no interface for external testing.–recursive前端开发应用的文档里面，没有提到要在clone操作末尾加 --recursive，建议完善一下文档，如图
详细的开发和代码运行在开发手册里 这里疏忽了，我们会加上详细文档跳转链接管理页面怎么进入，进行系统配置。您好，如图所示，在选择组织或项目右侧的“管理”按钮，点击即可。
为什么我没有管理这个按钮这个topic关闭吧，我看你在 前端代码源码方式运行 这里已经有个topic了咨询下你这个是如何构建起来的哈，我这构建环境都有问题哈，能稍微说下吗？
启动了那些镜像容器，需要配置什么？我们是按照猪齿鱼平台文档中的分步部署的方式来构建的
下图是我们已经启动的微服务：
(⊙o⊙)…，我这目前不想依赖这么多，只想在本地启动相关容器，进行开发，我这根据的是开发手册来的。。。，有那么几个组件容器一直无法启动，哎，搞不懂。。。嗯，我们是先搭建了平台，后面会看开发手册来进行学习。建议使用kubectl的相关命令查看下具体日志，找到无法启动的原因。补充报错信息：
您好,你能确认一下devops-service的redis配置正确吗,devops-service所配置的redis host是否正确?
在Redis部署这一章节，部署的名字叫devops-redis。而在部署devops service章节中，helm命令中写的是–set env.open.SPRING_REDIS_HOST=prod-devops-redis。两边命名不一致，导致了这个问题。改正后就解决问题了，多谢帮助！
如图，Issue编码是什么，请举例说明您好，issue编码是敏捷管理中对用户故事、任务、缺陷的编码，当issue创建时平台会自动生成相应的编码。敏捷管理近期上线，届时会更加清晰明白。现在可以先填写数字或字母，以标识这个分支的主要用途。后续我们将支持自动为issue创建分支等功能，敬请期待。请问是因为我配置ingress要走ssl导致的吗？没有传member_id，member_id就是登陆用户的id你好，创建clien 可以参考这个帖子创建client 时 认证失败 报invalid_scope已解决。不知道为什么iam_permission表中只有29条数据，重新装了一次，发现有120来条。重装的时候，我把之前改的一些名字，恢复了回去。比如namespace，mysql的账号等。。是不是choerodon哪些地方还没有根据values.yml中数据库账号做动态更新？iam_permission表里的数据是从iam-servcice服务里初始化进去的，跟mysql关系不大，更多是和kafka还有注册中心有关。可能是注册中心的没有配置正确，所以无法获取已经注册的服务。
你可以给register-server 设置service，然后通过register-server:8000//eureka/apps 接口获取所有已经注册的服务可以新建一个 choerodon-cloud-front 的目录，进入到该目录下，在终端执行执行完部署Runner脚本后有报错，查看日志为：
FATAL: Near line 8 (last key parsed ‘runners.environment’): expected a comma or array terminator ‘]’, but got ‘=’ instead
你好   感谢反馈   请执行以下命令进行更正发现执行部署choerodon front 脚本一直报DeadlineExceeded错误，查看了job详情，发现有错误日志Job was active longer than specified deadline。
追踪日志发现：
container “choerodon-front-init-config” in pod “choerodon-front-init-config-tqgxb” is waiting to start: image can’t be pulled。
查找了choerodon-front-init-config这个job详情中的image为：registry.saas.hand-china.com/hand-rdc-choerodon/choerodon-front:0.5.0
使用docker pull 下载 发现没有该资源
@Waxion 执行helm repo list  查看下您在使用哪个helm仓库。@Waxion 执行 helm update  后再尝试重新安装通过一键部署，登录后菜单显示不完成，持续交付相关的菜单显示不出来，是使用问题还是安装配置问题
你好，前端跑的是哪一个项目不明白前端项目是什么意思。
现在刚完成部署，想新建一个项目看看界面功能，现在是新建项目后，看不到持续交付的菜单。
登录gitlab也没看到新增用户或者项目是在正式环境上吗？还是本地自己搭建的本地自己搭建的
另外平台设置里，组织管理也满意创建组织的功能，是安装的版本不是最新的吗你好，请教一下，你登录成功后跳转的链接是什么？网站文档的默认配置没改，登录后就是example.choerodon.io/#/choerodon的前端在github上一共有三个项目，分别对应总前端项目，iam前端和devops的前端如果要看到所有的菜单，需要部署choerodon-front这个项目一键部署中还少布了两个前端是吧，多谢，我试试只需要choerodon-front一个前端就可以了，包含了iam和devops的前端一键部署装了这个前端，pods看到choerodon-front在运行，界面访问的也是这个服务。
还是配置问题吗？部署的时候有没有执行初始化菜单，或者看一下数据库中初始化的菜单数据有没有持续交付这个ingress.host=charts.example.choerodon.io，改成什么服务的地址？上面的nfs已经琢磨出来了，建了个nfs server…比如您想用chart.example.com 访问这个应用，那么这里就配置 chart.example.com请问 怎么组建一个 nfs server参考文档：http://choerodon.io/zh/docs/installation-configuration/parts/base/nfs/–set persistence.existingClaim=“prod-devops-service-pvc” 
应该改成
–set persistence.existingClaim=“devops-service-pvc” \@Waxion 谢谢您的反馈 现已修正。执行部署devops service的helm命令后，有报错
找到 查看job日志的命令了 但执行报错:
kubectl logs -f -p job/devops-service-init-db -n choerodon-devops-proderror:timed out waiting for the condition这个问卷解决了，可能时网络原因。重新执行就好了如下图：
应该点击下图的感叹号进行授权登陆
请问一下对应的后端服务有没有启动我这边对应的服务都启动成功了，认证服务也过了，后面就进不去了。浏览器请求有没有抛错浏览器也没有错误
应该是前端菜单没有初始化到数据库吧。数据库里面的确没有菜单相关的数据，但前端菜单需要怎么初始化到数据库。本地安装python 2.7，通过命令在部署时候也可通过环境变量进行传递参数谢谢，数据库菜单已经同步成功了，但是登录成功后跳转的链接字段没有，请教一下，登录成功后跳转到哪个链接？请问下本地使用helm安装的吗我是直接运行源码的，没有使用helm。本地打开swagger
选择iam_service -> client-controller -> 创建client认证请使用用户名：admin，密码：admin
提交以下数据，注意正式搭建时请替换以下值为真实值这个真实的环境对应哪个服务的哪个url对应前端页面的地址这个从哪获取
我用helm安装也遇到类似的问题，菜单看不到"kafka-0.kafka-headless.choerodon-devops-prod.svc.cluster.local:2181,kafka-1.kafka-headless.choerodon-devops-prod.svc.cluster.local:2181,kafka-2.kafka-headless.choerodon-devops-prod.svc.cluster.local:2181"脚本中的端口号都是错误，都是zookeeper的端口号，要改成kafka的端口号：9092
正确命令为
“kafka-0.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-1.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092,kafka-2.kafka-headless.choerodon-devops-prod.svc.cluster.local:9092”@Waxion 谢谢你的提醒，现已修正。你好：感谢你的反馈，这是我们文档的一些缺陷，我们会尽快改正并提交到官网上。这里提供一个临时的解决方案：镜像地址统一改为registry.choerodon.com.cn/choerodon-framework，版本为0.5.0，例如：
registry.choerodon.com.cn/choerodon-framework/manager-service:0.5.0。registry.choerodon.com.cn/choerodon-framework/manager-service:0.5.0依然找不到对应的镜像
registry.choerodon.com.cn/choerodon-framework这是我的失误，镜像已经统一放到了共有镜像库上，可以使用registry.cn-shanghai.aliyuncs.com/choerodon/。
例如：registry.cn-shanghai.aliyuncs.com/choerodon/manager-service:0.5.0。你可以使用git pull registry.cn-shanghai.aliyuncs.com/choerodon/manager-service:0.5.0 拉取到镜像。文档已经更新在http://choerodon.io/zh/docs/文档上写的是0.1.0,应该是.0.5.0吧好的，我们这边修改一下文档，感谢你的建议我执行完初始化 manage的脚本，发现数据manager数据并没有被初始化
你这个不是初始化数据库，是初始化配置。初始化数据库的脚本请参照这个。if [ ! -f target/choerodon-tool-liquibase.jar ]
then
curl http://nexus.choerodon.com.cn/repository/choerodon-release/io/choerodon/choerodon-tool-liquibase/0.5.0.RELEASE/choerodon-tool-liquibase-0.5.0.RELEASE.jar -o target/choerodon-tool-liquibase.jar
fi
java -Dspring.datasource.url=“jdbc:mysql://localhost/manager_service?useUnicode=true&characterEncoding=utf-8&useSSL=false” 
-Dspring.datasource.username=choerodon 
-Dspring.datasource.password=123456 
-Ddata.drop=false -Ddata.init=true 
-Ddata.dir=src/main/resources 
-jar target/choerodon-tool-liquibase.jar部署config-server后 ，curl   pod的ip 和 server的ip 都连接不上 配置服务没有ingress 的情况下，是无法直接curl到pod的ip的。
可以给config-server创建service，其他服务通过通过配置环境变量连接config-server在部署api-gateway的时候，有报错，显示连接config-server不成功
除了config-server和manager-service外，像api-gateway这样的服务需要通过choerodon-starter-config-tool将配置初始化进去choerodon-starter-config-tool这个东西怎么使用，可以说具体点吗 没有找到相应的文档你可以参考下choerodon-starter-config-tool 的 README.mdchoerodon-starter-config-tool 的README中提供的jar下载路径是错的：
https://oss.sonatype.org/content/groups/public/io/choerodon/choerodon-tool-liquibase/0.5.0.RELEASE/choerodon-tool-config-0.5.0.RELEASE.jar
我从choerodon-tool-config中下到了正确的jar，执行发现缺少application-default.yml
我写了一个yml ，执行了 jar。之后去部署api-gateway ，发现依然报错：
连接了config-service的pod 发现 连接不成功 。同时查看 该pod的详情，发现这个pod一直在重启。
查看该pod的 日志 未发现有 error
你可以参考下 choerodon-starter-config-tool 的 README.md @Waxion 请问下你的应用是用helm安装的吗，如果是用helm安装的，应该 会帮你初始化的都是用 helm 命令装的可以参考一下http://choerodon.io/zh/docs/installation-configuration/parts/choerodon/，用helm安装是包含初始化初始化数据库的我就是使用的这里的helm命令 执行的麻烦贴一下你执行的helm命令，我这边测试下helm install choerodon/config-server 
–set service.enable=true 
–set env.open.EUREKA_CLIENT_SERVICEURL_DEFAULTZONE=“http://register-server.choerodon-devops-prod:8000/eureka/” 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_BROKERS=“kafka-0.kafka-headless.choerodon-devops-prod.svc.cluster.local:2181,kafka-1.kafka-headless.choerodon-devops-prod.svc.cluster.local:2181,kafka-2.kafka-headless.choerodon-devops-prod.svc.cluster.local:2181” 
–set env.open.SPRING_CLOUD_STREAM_KAFKA_BINDER_ZK_NODES=“zookeeper-0.zookeeper-headless.choerodon-devops-prod.svc.cluster.local:2181,zookeeper-1.zookeeper-headless.choerodon-devops-prod.svc.cluster.local:2181,zookeeper-2.zookeeper-headless.choerodon-devops-prod.svc.cluster.local:2181” 
–name config-server 
–version=0.5.0 --namespace=choerodon-devops-prod命令看起来没有问题，能不能kubectl logs 把config-server的日志发一下找到问题了 是文档helm命令中 kafka的端口给错了  更正后 OK了执行 helm install paas/gitlab 后 pod 未能正常运行
创建主题时选择对应分类会更容易得到专业的回复。已添加分类，请帮忙解答kubectl logs -f POD -n NAMESPASE 看日志贴一下看日志是你把数据库用户名或者密码配置错了，链接时认证出错@Waxion
Gitlab 部署的文档中写道：GRANT ALL PRIVILEGES ON gitlabhq_production.* TO choerodon@’%’;
应该改成
GRANT ALL PRIVILEGES ON gitlabhq_production.* TO gitlab@’%’;
吧@rivers 好的，谢谢指正choerodon-front-iam 通过git下载 之后进入iam目录，pm i 之后 成功，再执行npm run dev  提示missing scripts: dev├── src
│   └── app
│       └── iam
│           ├── assets
│           │   ├── css
│           │   └── images
│           ├── components
│           │   ├── loadingBar
│           │   ├── memberRole
│           │   └── menuType
│           ├── config
│           │   ├── Menu.yml
│           │   └── language
│           ├── containers
│           │   ├── Home.js
│           │   ├── IAMIndex.js
│           │   ├── Masters.js
│           │   ├── global
│           │   ├── master.css
│           │   ├── organization
│           │   ├── project
│           │   └── user
│           ├── locale
│           │   ├── en.js
│           │   └── zh.js
│           ├── stores
│           │   ├── globalStores
│           │   ├── organization
│           │   ├── project
│           │   └── user
│           └── test
│               └── util
├── package-lock.json
├── package.json
├── tsconfig.json
└── yarn.lock你好，可以参考前端代码源码方式运行谢谢已经解决，继续学习harbor-mysql起不来，upgrade mysql报错，有遇到的吗logs看到的内容是：/usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-updatedb.d/upgrade.sh
Maria DB flag not found, the DB was created in mysql image, running upgrade …
Waiting for MySQL start …
Waiting for MySQL start …
…N次…
Waiting for MySQL start …
MySQL failed to start我们这边没遇到过这个问题，可以参考下这篇：


github.com/vmware/harbor





Issue: Installation problem with 1.3.0 and 1.4.0-rc1 -- MariaDB 10.2.10 issue on ext3


	opened by bergtwvd
	on 2018-01-31


	closed by reasonerjt
	on 2018-05-02


I follow the install/config instructions from the book. When I start it all up with install.sh I notice a problem in...

1.4.0-defer
community
kind/note
product/standalone
target/pks-1.0






也看到这篇了，我的文件系统是ext4，是不是nfs挂载有问题，暂时绕过去了，用外部harbor在部署Chartmuseum脚本中,helm install paas/create-pv 里面有个
–set.nfs.server=nfs.exmple.choerodon.io \  ,这个nfs.exmple.choerodon.io我应该设置什么？应该配置成你的nfs的地址请问nfs.server是自己指定的吗？还是需要去哪里配置，是否可以说明一下。是你自己搭建的用于存储的nfs服务器，如果是阿里云的话就是NAS存储。如果不是阿里云的环境需要自己搭建一个nfs服务器用于存储。现在已经搭建了自己的nfs服务器，nfs.server是配置它的IP还是域名（文档中写的是域名），这个域名是如何配置的呢？@rivers 如果没配域名写IP就可以。在自己的笔记本上使用 vagrant 创建虚拟机3台 ，并部署了K8S集群.成功创建应用并部署。
请问怎么才能访问到部署上去的应用
部署之后步骤请参考 用户手册，使用域名访问前请确定正确进行了域名解析。我是在本地的虚拟机中部署了 kubernetes环境，而且在虚拟机内部是可以通过IP地址访问的。请问如何在我电脑上直接访问应用呢？请在集群内执行以下命令尝试然后自己主机访问你执行上面命令的那台虚拟机的  IP:[虚拟机端口]   就可以访问了在node1机器上访问 localhost:8888无法访问吗？@Waxion  注意格式pod/[pod名称] pod名称前需指明类型为pod。核心数量：4核4线程及以上
内存信息：16G及以上
节点数量：4教程里说4个节点，是指MASTER一个，NODE3个吗？我MASTER和NODE1一个节点，NODE2,和NODE3一共三个。cpu和内存已经大于上面的要求。但是安装到KAFKA的时候，就说内存不够了。@yifanyou  您好，MASTER节点建议3个，另外一个节点为普通节点。 注意是每个节点16G。按照demo来弄，master是一个，node是3个吧…看了下ansible脚本确实是3个master…RUNNING HANDLER [etcd : wait for etcd up] ************************************************************
Friday 25 May 2018  10:53:13 +0800 (0:00:00.314)       0:01:56.995 ************
FAILED - RETRYING: wait for etcd up (10 retries left).
FAILED - RETRYING: wait for etcd up (10 retries left).
FAILED - RETRYING: wait for etcd up (10 retries left).
FAILED - RETRYING: wait for etcd up (10 retries left).
FAILED - RETRYING: wait for etcd up (9 retries left).
FAILED - RETRYING: wait for etcd up (9 retries left).
FAILED - RETRYING: wait for etcd up (9 retries left).
FAILED - RETRYING: wait for etcd up (9 retries left).
FAILED - RETRYING: wait for etcd up (8 retries left).
FAILED - RETRYING: wait for etcd up (8 retries left).
FAILED - RETRYING: wait for etcd up (8 retries left).
FAILED - RETRYING: wait for etcd up (8 retries left).
FAILED - RETRYING: wait for etcd up (7 retries left).
FAILED - RETRYING: wait for etcd up (7 retries left).
FAILED - RETRYING: wait for etcd up (7 retries left).
FAILED - RETRYING: wait for etcd up (7 retries left).
FAILED - RETRYING: wait for etcd up (6 retries left).
FAILED - RETRYING: wait for etcd up (6 retries left).
FAILED - RETRYING: wait for etcd up (6 retries left).
FAILED - RETRYING: wait for etcd up (6 retries left).请检查本地网络，出现这种情况是因为你本地docker创建的容器网络不通导致的找到原因了；
https://10.0.1.9:2379/health
需要放开2379端口啊！！
正式环境千差万别，强烈建议官方完善文档啊！
另外，现在有卡在这里了：
TASK [master : kubeadm | Initialize first master] ****************************************************
Friday 25 May 2018  11:05:03 +0800 (0:00:00.980)       0:02:28.151 ************好的，谢谢你的建议抱歉我的不熟练，这又是什么鬼？curl http://localhost:8080是通的啊!
TASK [master : kubeadm | delete old kube-dns service] ****************************************************
Friday 25 May 2018  12:00:54 +0800 (0:00:00.553)       0:00:56.688 ************
fatal: [node1]: FAILED! => {“changed”: true, “cmd”: [“kubectl”, “delete”, “svc”, “kube-dns”, “-n”, “kube-system”], “delta”: “0:00:00.122529”, “end”: “2018-05-25 12:00:54.975559”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-05-25 12:00:54.853030”, “stderr”: “The connection to the server localhost:8080 was refused - did you specify the right host or port?”, “stderr_lines”: [“The connection to the server localhost:8080 was refused - did you specify the right host or port?”], “stdout”: “”, “stdout_lines”: []}能否粘贴一下您的运行环境信息，参考这篇帖子，可能是您的运行内存不足了。




Kubernetes 安装后运行不稳定 Installation management


    在执行完Kubernetes的集群部署完成后，执行 查询K8S nodes 命令  kubectl get nodes ，可以获得信息，但一段时间后就报错了：The connection to the server localhost:8080 was refused - did you specify the right host or port? 
[image]
  

centos7.2 4核16G抱歉我的不熟练，这又是什么鬼？curl http://localhost:8080是通的啊!
TASK [master : kubeadm | delete old kube-dns service] ****************************************************
Friday 25 May 2018 12:00:54 +0800 (0:00:00.553) 0:00:56.688 ************
fatal: [node1]: FAILED! => {“changed”: true, “cmd”: [“kubectl”, “delete”, “svc”, “kube-dns”, “-n”, “kube-system”], “delta”: “0:00:00.122529”, “end”: “2018-05-25 12:00:54.975559”, “msg”: “non-zero return code”, “rc”: 1, “start”: “2018-05-25 12:00:54.853030”, “stderr”: “The connection to the server localhost:8080 was refused - did you specify the right host or port?”, “stderr_lines”: [“The connection to the server localhost:8080 was refused - did you specify the right host or port?”], “stdout”: “”, “stdout_lines”: []}你好，你在创建主题，提交问题的时候，可否选择一个分类，这样相关人员能够快速的回复。谢谢~~下次一定注意，抱歉！ps：我对k8s不是很熟查不到pods…@yifanyou ingress.hosts[0] 即为你要访问的gitlab地址，注意先配置域名解析。问题已经解决，用了虚拟机，资源不太够如下图所示
已经解决，用了虚拟机，资源不够的原因部署好集群后 kube-dns 一直处于 Pending状态
请执行以下命令查看Pending原因找到一种解决方案，在执行集群部署脚本之前，,请将k8s_interface变量从kubeadm-ansible/inventory/vars文件删掉，并在inventory/host文件中给每个机器加上ip地址，比如：
node1 ansible_host=192.168.56.11 ip=192.168.56.11 ansible_user=root ansible_ssh_pass=change_it ansible_become=true，如下图， 在 agent以及 猪齿鱼界面上，都是 “context deadline exceed”错误agent日志：
猪齿鱼界面：
请在服务器用以下命令试试，贴一下返回结果实际观察下来， 对应的nginxdemo-6b658并没有创建成功， 所以， 两个命令执行下来，结果就是not found了
能试试直接手工安装会是什么错误吗问题已得到解决， 是k8s集群的dns问题感谢 @李佳桐 同学的帮忙:grinning:感谢您的支持.以及对我们平台的支持.报错信息，如下：
install release springboot-2b71e: rpc error: code = Unknown desc = Job failed: BackoffLimitExceeded您好,从报错信息中看出是,这个release中所包含的hook,也就是其中的job执行失败了.错误信息如下。但是我的几台主机直接是通的，也能够ssh。
请教为什么？PLAY [kube-master:kube-node:etcd] ****************************************************************************************TASK [Gathering Facts] ***************************************************************************************************
Thursday 24 May 2018  11:49:20 +0800 (0:00:00.083)       0:00:00.083 **********
fatal: [node1]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.6”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [node3]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.8”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [node4]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.9”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [node2]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.7”. Make sure this host can be reached over ssh”, “unreachable”: true}@laoliang 注意修改 inventory/hosts中的主机信息，里面包含用户和密码。这样配置对吗？对ansible不熟。
hosts文件内容如下：
[all]
node1 ansible_host=10.0.1.6 ansible_user=root ansible_ssh_pass=mypass ansible_become=true
node2 ansible_host=10.0.1.7 ansible_user=root ansible_ssh_pass=mypass  ansible_become=true
node3 ansible_host=10.0.1.8 ansible_user=root ansible_ssh_pass=mypass  ansible_become=true
node4 ansible_host=10.0.1.9 ansible_user=root ansible_ssh_pass=mypass  ansible_become=true[kube-master]
node1
node2
node3
node4[etcd]
node1
node2
node3
node4[kube-node]
node1
node2
node3
node4ansible_user 为远程主机的用户名
ansible_ssh_pass 为远程主机的密码是的，是按照实际设置的。看错误提示是连不上节点的ssh，但网络是通的，用cli连ssh没有问题。[root@node1 kubeadm-ansible]# ansible-playbook -i inventory/hosts -e @inventory/vars cluster.ymlPLAY [kube-master:kube-node:etcd] **********************************************TASK [Gathering Facts] *********************************************************
Thursday 24 May 2018  14:38:13 +0800 (0:00:00.086)       0:00:00.086 **********
fatal: [node1]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.6”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [node3]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.8”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [node2]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.7”. Make sure this host can be reached over ssh”, “unreachable”: true}
fatal: [node4]: UNREACHABLE! => {“changed”: false, “msg”: “SSH Error: data could not be sent to remote host “10.0.1.9”. Make sure this host can be reached over ssh”, “unreachable”: true}NO MORE HOSTS LEFT *************************************************************
to retry, use: --limit @/root/kubeadm-ansible/cluster.retryPLAY RECAP *********************************************************************
node1                      : ok=0    changed=0    unreachable=1    failed=0
node2                      : ok=0    changed=0    unreachable=1    failed=0
node3                      : ok=0    changed=0    unreachable=1    failed=0
node4                      : ok=0    changed=0    unreachable=1    failed=0Gathering Facts --------------------------------------------------------- 2.77s你是在哪台机器上执行ansible的，尝试在那一台机器上ssh到其他主机，是否能连上。在1.6上执行的ansible，在cli界面，所有机器之间ssh连接都没问题这个可能是ansible的一个bug，尝试执行export ANSIBLE_SCP_IF_SSH=y再执行命令。原因找到了，经过不懈查找，解决方案如下：
问题原因：没有在ansible管理节点（即安装ansible的节点）上添加目标节点（即需要管理的节点）的ssh认证信息解决步骤：
1：管理节点生成SSH-KEY
#ssh-keygen
成功后在~/.ssh/路径下将生成ssh密钥文件：id_rsa及id_rsa.pub2：添加目标节点的SSH认证信息
#ssh-copy-id root@目标节点IP
这里root是在目标节点上登录的用户，@符号后面接目标节点IP即可，之后会提示输入目标节点root用户密码，输入即可。添加认证信息后，目标节点主机的~/.ssh/目录下将会出现一个authorized_keys文件，里面包含了ansible管理节点的公钥信息，可以检查一下是否存在。切记，管理节点本机也要做以上同样的操作至此以上问题应该能解决。另外，特别感谢@vinkdong热心的回答。下一步有问题还需多多指教！运行激活环境脚本，结果如下：
贴个env-agent的日志截图看看|paas     |http://openchart.choerodon.com.cn/choerodon/paas/
|choerodon|http://openchart.choerodon.com.cn/choerodon/devops/|@shizy123 仓库暂不支持浏览器直接访问，请使用helm命令添加和使用。在执行完Kubernetes的集群部署完成后，执行 查询K8S nodes 命令  kubectl get nodes ，可以获得信息，但一段时间后就报错了：The connection to the server localhost:8080 was refused - did you specify the right host or port?
@Waxion  您的硬件配置是什么，可能是内存不够了。用下边这两条命令查看下是否有报错信息journalctl -u docker
journalctl -u kubelet我的硬件配置是 CPU i5    内存 16G，
运行你给的两条指令都没有报错信息，具体如下
[vagrant@node1 vagrant]$ kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the ri
[vagrant@node1 vagrant]$ journalctl -u docker
No journal files were found.
– No entries –
[vagrant@node1 vagrant]$ journalctl -u kubelet
No journal files were found.
– No entries –
[vagrant@node1 vagrant]$@Waxion 运行报错之后查看kube-apiserver是否还在运行中docker ps | grep kube-apiserverkube-apiserver没有在运行  ，请问需要怎么解决查看下历史kube-apiserver的日志信息docker ps -a | grep kube-apiserver | awk '{print $1}' | head -n 1 | xargs -I -- docker logs --发现是虚拟机的内存太低，将Vagrantfile中 v.memory = 2048改成 4096后 K8S运行稳定激活环境时，将choerodon生成的脚本在 k8s中运行 报错，如下：
Error: could not find tiller你好,根据你的描述,想必你应该时在安装环境的时候出了问题,你在安装环境的时候需确保你的k8s环境安装了tiller server.
版本信息如下 Helm：2.8.2及以上(tiller版本请一定与helm版本一致)
文档中也有相应描述部署与配置未对 inventory/hosts ， inventory/vars 和 cluster.yml进行修改，默认开启了3台虚拟机，
在执行ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml后 控制台有报错。如下：
fatal: [node1 -> 192.168.56.11]: FAILED! => {“changed”: true, “cmd”: “/etc/ssl/etcd/gen_cert.sh”, “delta”: “0:00:00.003638”, “end”: “2018-05-23 10:11:38.699149”, “msg”: “non-zero return code”, “rc”: 126, “start”: “2018-05-23 10:11:38.695511”, “stderr”: “/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory”, “stderr_lines”: ["/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory"], “stdout”: “”, “stdout_lines”: []}，
有时会报不同的错误。再次发现时，我在提交仓库克隆下来后除了inventory/hosts ，inventory/vars这两文件，其他文件有编辑过或打开过吗？在使用ansible-playbook -i inventory/hosts reset.yml重置集群脚本后，继续执行依然报这个错误用 vs code 打开了kubeadm-ansible整个目录请在linux集群内重新克隆代码，在liunx环境中编辑inventory/hosts，inventory/vars这两文件，后执行ansible-playbook -i inventory/hosts reset.yml重置集群，然后再进行安装。请不要在windows环境中打开本项目任何文件，这将导致打开的文件编码格式变化。我重新将之前克隆下来的目录全部删除后再克隆，按照步骤执行，没有打开过任何文件，依然报错了，如下：fatal: [node1 -> 192.168.56.11]: FAILED! => {“changed”: true, “cmd”: “/etc/ssl/etcd/gen_cert.sh”, “delta”: “0:00:00.003686”, “end”: “2018-05-23 10:47:52.440891”, “msg”: “non-zero return code”, “rc”: 126, “start”: “2018-05-23 10:47:52.437205”, “stderr”: “/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory”, “stderr_lines”: ["/bin/sh: /etc/ssl/etcd/gen_cert.sh: /bin/bash^M: bad interpreter: No such file or directory"], “stdout”: “”, “stdout_lines”: []}我发现时 vagrant这个账号没有进入 /etc/ssl/etcd 这个目录的权限，-bash: cd: etcd: Permission denied我切换成root权限执行部署集群的脚本 ，报错了 如下：
An exception occurred during task execution. To see the full traceback, use -vvv. The error was: timeout: timed out
fatal: [node2]: FAILED! => {“changed”: false, “msg”: “failed to create temporary content file: timed out”}我是在服务器上装的。。然后我刚刚检查了下，我的服务器cpu并不支持虚拟化
找到问题所在了，在windows环境下 ，使用 git bash clone时会自动切换换行符，导致.sh文件在linux环境下不可执行，需要设置git配置，把自动切换换行符功能关闭。命令如下：
git config --global core.autocrlf false
为了保证文件的换行符是以安全的方法，避免windows与unix的换行符混用的情况，最好也加上这么一句
git config --global core.safecrlf true。部署集群（在node1），执行ansible-playbook -i inventory/hosts -e @inventory/vars cluster.yml后
@ahthzxs 麻烦贴一下红色的报错信息，谢谢。@wuyizhh 需要卸载你机器上原有dockerdocker-compose 也要删除吗找到原因了，。docker-common也要删除@wuyizhh docker没有卸载干净，卸载之后参考安装失败步骤重试卸载docker或者删除虚拟机，重新安装虚拟机后再部署可以解决根据Kubernetes集群部署 文档步骤进行操作，在进行到 克隆kubeadm-ansible搭建脚本时 ，发现克隆不成功，报错如下：
repository ‘https://github.com/choerodon/kubeadm-ansible.git/’ not found不好意思，目前这个库还没有上传到Github。现在正在上传，好了，我会告诉你。请问上传好了吗？https://github.com/choerodon/kubeadm-ansible.git这个仓库可以克隆了，但是里面没有找到vagrantfile,导致vagrant up无法执行。是否是自己写vagrantfile 来生成多个节点？@Waxion Vagrantfile 已经上传，现在拉取新的代码就可以看到了。能提供一下您的运行环境信息吗，比如操作系统，虚拟机版本，Vagrant版本等。
2.0.1
centos7 64位
16G
四核@wuyizhhvirtualbox --help | grep Virtual 查看一下 virtualbox的版本5.1.38
你的主机是否开启了虚拟化呢我是在服务器上装的
然后刚刚百度了一下，发现我的服务器cpu不支持虚拟化
curl -o choerodon-install.sh 
http://http://file.choerodon.com.cn/choerodon-install/install.sh && 
sh choerodon-install.sh “$PWD/values.sh”
这段脚本里面有 两个http@Waxion 感谢您的反馈，现已修正。正确的脚本为：后端springboot，还要用封装的模块？
前端用react？您好:
后端项目是基于spring boot的，choerodon开发框架提供了一些实用的依赖包。框架的其他组件，比如api-gateway的请求路由定向，gateway-helper的鉴权，限流等，manager-service的配置管理，swagger管理rest api等，oauth-server提供了统一的认证和授权管理。所以使用choerodon开发框架，能够极大的提高了微服务的开发效率。详细步骤，可参考choerodon开发框架的开发文档。前端使用的是React。是要使用choerodon 管理还是使用choerodon framework开发？如果是使用Choerodon的微服务开发框架，那么就按照Choerodon的标准来，这样可以省去不必要的麻烦；如果是自己使用原生的技术开发，例如spring MVC+Mybatis，自己写前端，也是可以的，这样就没有必要按照Choerodon的开发标准来。使用Choerodon作为项目管理、开发、部署工具，仅需要按照猪齿鱼的代码库标准，添加相关的配置文件即可，例如.gitlab-ci.yml等。可以理解为需要自己二次开发？整个不是二次开发。和EBS等不一样，choerodon是一个开发的管理平台，是用来支持开发和支撑系统部署，与二次开发没有关系。官方文档中给出的后端开发demo，
需要引入下面依赖

io.choerodon
choerodon-framework-parent
0.5.0.RELEASE
如果是现有的项目，而不是按照这个demo来开发的，可以吗？可以的，这个库是Choerodon的微服开开发框架的库。每个项目还是有自己的差异， 希望增加 自定义应用模版好吧，没看到， 有自定义功能在组织层是有应用模板的定义，您可以使用Choerodon预置的模板，也可以选择自定义模板。有交流安装群吗？目前还没有创建QQ群或者微信群的计划，请以choerodon.io/zh/docs的文档为准。
如果有问题请在论坛上向Choerodon社区提问，谢谢~~要求我5小时以后才能发帖，急死我了！@shayulei你再试试可以了吗?centos7.3 的三个节点的默认登录账户密码是多少？你是指的使用ansible在本地用vagrant模拟安装吗？是的话，默认登录名和密码为: root/vagrantvagrant是本地用vagrant模拟安装环境的，可以登录了，谢了   !报404错误
http://choerodon.io/zh/docs/installation-configuration/parts/chartmuseum这是官网的一个缺陷。我们在做目录结构的更改，所以之前的一些链接失效。我们正在修复，多谢！什么时候能恢复？失效的链接已经好了。新链接如下：http://choerodon.io/zh/docs/installation-configuration/parts/base/chartmuseum/。另外，你可以通过整个网址路径进入，http://choerodon.io/zh/docs/installation-configuration/parts/ 。还是404错误！http://choerodon.io/zh/docs/installation-configuration/parts/chartmuseum可以了！！！！！！！谢谢！要过5小时才能再提问，这样真的好吗？我更改一下设置~~~回复论坛时报“内容似乎不清楚，这是一个完整的句子？”怎么回事？回帖的语句尽量写的完整，不要过于省略或者“太不按照语法”写语句。否则，系统会去判断回复的语句太不合法，就不通过。我已经描述的很清楚了。Choerodon猪齿鱼致力于打造一个多元化的开放社区，大家可以通过以下社区途径了解猪齿鱼的最新动态、产品特性，以及参与社区贡献：官网：http://choerodon.io
论坛：http://forum.choerodon.io
Github：https://github.com/choerodon/
微信公众号：Choerodon猪齿鱼
微博：Choerodon猪齿鱼欢迎加入Choerodon猪齿鱼社区，共同为企业数字化服务打造一个开放的生态社区。众人拾柴火焰高，愿choerodon社区越来越活跃！开发人员正在大批入驻中…赶紧整个微信群吧，迫切需要交流目前，鼓励大家在论坛上提交问题，沟通交流。我想测试一下！不知道入口你好：目前没有提供统一的外部测试链接，你可以参考文档在本地搭建测试如果平台里面加了某些服务的话，我要怎么去调用他人的服务为我所用框架基于spring cloud，服务启动注册在go-register-server后，可通过feign或ribbon的方式调用给微服务大佬点赞2018年5月20日，Choerodon猪齿鱼正式发布 0.5.0 版本，同时汉得公司宣布Choerodon猪齿鱼平台开源，公司希望通过开源社区的力量不断完善和提升产品的体验，并为企业提供数字化转型的企业级应用容器PaaS平台支持。Choerodon猪齿鱼是一个开源企业服务平台，是基于Kubernetes的容器编排和管理能力，整合DevOps工具链、微服务和移动应用框架，来帮助企业实现敏捷化的应用交付和自动化的运营管理，并提供IoT、支付、数据、智能洞察、企业应用市场等业务组件，来帮助企业聚焦于业务，加速数字化转型。